[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Study Group",
    "section": "",
    "text": "CS224w를 공부하며 정리한 글들을 모아둔 블로그입니다.\n\nContributors\n\nalphabetical order\n\n\nEunsung Shin\niDeal\nJooHo Kim\nJung Yeon Lee\nmhkim9714\nsunLeee"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS224W-KOR",
    "section": "",
    "text": "Lecture 4\n\n\n\n\n\n\n\nLEC04\n\n\n\n\nGraph as Matrix\n\n\n\n\n\n\nJul 20, 2022\n\n\nJung Yeon Lee\n\n\n\n\n\n\n  \n\n\n\n\nLecture 2\n\n\n\n\n\n\n\nLEC02\n\n\n\n\nTraditional Methods for Machine Learning in Graphs\n\n\n\n\n\n\nJul 13, 2022\n\n\nJooHo Kim\n\n\n\n\n\n\n  \n\n\n\n\nLecture 1\n\n\n\n\n\n\n\nLEC01\n\n\n\n\nIntroduction to Graph\n\n\n\n\n\n\nJul 6, 2022\n\n\nEunsung Shin\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/lec04.html",
    "href": "posts/lec04.html",
    "title": "Lecture 4",
    "section": "",
    "text": "4강에서는 Graph를 매트릭스(선형대수) 관점으로 바라보는 것에 대해 이야기 합니다.\n\n다음 3가지 키워드, Random walk(Node Importance), Matrix Factorization, Node embedding를 중심으로 공부합니다. 강의는 총 4파트로 나누어져 진행됩니다.\n\n\n\n웹을 거시적인 관점으로 보게되면, 하나의 웹 페이지 → Node로 하이퍼링크 → Edge로 생각하여 하나의 거대한 Graph로 볼 수 있습니다.\n\nSide issue - 다이나믹하게 새로 페이지들이 생길 수 있습니다. - 다크웹과 같은 접근할 수 없는 페이지들도 있을 수 있습니다.\n\n잠시 Side issue는 내려놓고, 새로 페이지들이 생기지도 않고 기존의 페이지들이 사라지지도 않는 Static pages 상황을 가정해봅시다. 아래의 그림에서처럼 페이지들은 하이퍼링크들로 서로 연결되어 있고, 유저는 페이지들에 달려있는 하이퍼 링크들로 이루어진 연결망을 기반으로 항해하듯이 Navigational 하게 page to page 이동을 하게 됩니다. (오늘날에는 post, comment, like 등의 기반의 transactional한 웹에서의 상호작용이 일어나지만 이는 우선 논외로 하겠습니다.)\n \n위의 그림처럼 웹 그래프는 방향성이 있는 유향 그래프(Directed graph)임을 알 수 있습니다. 위키피디아와 같은 웹 사전 페이지들 간의 관계성이나 논문의 인용 관계 그래프 등에서 예시를 쉽게 찾아볼 수 있습니다.\n\n\n\n\n웹을 하나의 거대한 유향 그래프로 생각할 때 한가지 중요한 insight가 있습니다.\n\n💡 모든 웹 페이지들이 똑같이 중요하지는 않다\n\n바로 각 페이지의 중요성이 똑같지 않다는 이야기는 그래프에서 각 노드의 중요성(importance)가 다르다는 말로 바꿔 생각할 수 있습니다. 아래 사진을 보면 직관적으로 파란색 노드가 빨간색 노드보다 더 중요할 것 같다라고 생각할 수 있습니다. 왜 그렇게 보일까요? 아직 노드의 중요성에 대해 정의하지 않았지만 그래프에서 각 노드를 중심으로 뻗어있는 edge(link)의 수가 한눈에 비교되기 때문에 직관적으로 파악할 수 있는 것입니다. 이처럼 웹 그래프의 link structure를 가지고 우리는 각 페이지들(node)의 ranking을 매길 수 있습니다.\n\n\n\n\n각 페이지들의 중요성(importance)를 파악하기 위해 Link Analysis가 필요합니다.\n본 수업에서 다룰 Link Analysis 알고리즘들은 아래 총 3개에 대해서 다룰 예정입니다.\n\nPageRank\nPersonalized PageRank (PPR)\nRandom Walk with Restarts\n\n\n\n\n링크가 투표용지라고 생각해봅시다. 여기서 유향 그래프인 웹 그래프에서 링크는 2가지 종류가 있다는 것을 다시한번 생각해봐야 합니다.\n\nin-comming links(in-links): 기준 페이지로 들어오는 방향의 링크\nout-going links(out-links): 기준 페이지에서 나가는 방향의 링크\n\n이렇게 방향까지 고려하여 링크를 투표라고 생각할 때, 엄밀히 말하자면 in-link를 투표라고 생각해야 할 것 입니다. 한 가지 더 생각해볼 문제는 모든 in-link들을 동등하게 생각할 수 있는가?라는 문제입니다. 어떤 링크들은 다른 링크들에 비해 좀 더 중요한 페이지로부터(from) 기준페이지로(to) 온 링크일 수도 있기 때문에 count에 차등을 둬야 하지 않을까라고 생각할 수도 있습니다. 이런 고민들은 결국 페이지들이 서로 연결되어 있어서 recursive한 문제로 볼 수 있습니다.\n\n➕ recursive한 문제란, 물리고 물리는 문제로 생각할 수 있습니다. A→B 링크에서 A가 중요한 페이지라는 사실을 기반으로 B가 중요해지고, 이어지는 B→C 링크에서 이 영향을 이어받아 C까지 중요한 페이지라고 판단하게 되기 때문입니다.\n\n\n\n\nThe “Flow” Model\n위에서 설명한 recursive한 특성을 기반으로 중요성이 흘러가는(flow) 모델을 생각해볼 수 있습니다. 중요성을 \\(r\\)이라는 변수로 두고 기준 노드 j의 importance가 어떻게 flow되는지 살펴보겠습니다.\n\nj로 in-link되어있는 i, k 의 importance \\(r_i\\), \\(r_k\\)를 각 노드의 out-link의 수만큼 나누어서 j로 전달됩니다. i 노드의 out-link는 총 3개 이므로 \\(\\frac{r_i}{3}\\), k노드의 out-link는 총 4개 이므로 \\(\\frac{r_k}{4}\\)로 계산되어 두 값의 합이 \\(r_j\\)가 됩니다.\n\\(r_j\\)는 j노드의 out-link를 통해 flow하게 되는데 out-link의 수, 즉 3으로 나누어져 \\(\\frac{r_j}{3}\\) 값이 각각의 다음 노드들로 \\(r_j\\)값이 전달되게 됩니다.\n\n\n이처럼 importance가 높은 페이지로부터 in-link된 페이지는 영향을 받아 importance가 높아짐을 알 수 있습니다. 노드 \\(j\\)의 rank, \\(r_j\\)를 정의하면 다음과 같이 수식으로 나타낼 수 있습니다. (이때 \\(d_i\\)는 노드 i의 out-degree를 말합니다.)\n\\[\nr_{j}=\\sum_{i \\rightarrow j} \\frac{r_{i}}{d_{i}}\n\\]\n다음과 같은 예시에서 각 기준 노드를 가지고 in-link들을 고려하여 “Flow equation”을 계산해보면 다음과 같다.\n\n\n\n\n\n\n\n\n\n노드 y\n노드 a\n노드 m\n\n\n\n\ny에서 오는 링크 + a에서 오는 링크\ny에서 오는 링크 + m에서 오는 링크\na에서 오는 링크\n\n\n\\(r_y = \\frac{r_y}{2} + \\frac{r_a}{2}\\)\n\\(r_a = \\frac{r_y}{2} + r_m\\)\n\\(r_m = \\frac{r_a}{2}\\)\n\n\n\n\n➕ 3 Unknowns, 3 Equations 이기 때문에 4번째 constraint로 \\(r_y + r_a + r_m =1\\)로 scale관련 constraint를 추가하여 Gaussian elimination을 사용하여 선형방정식으로 풀려고 하는 생각은 좋지 않다. 왜냐하면 importance는 이런식으로 scalable하지 않기 때문이다. (It’s not scalable) 좀 더 정교한 설계가 필요하다.\n\n\n\n\nStochastic Adjacency Matrix \\(\\mathbf{M}\\)\n\n\\(\\mathbf{M}\\)은 \\((node의 수)\\times (node의 수)\\)차원의 매트릭스 입니다.\n\\(i\\)→\\(j\\) 링크에서 매트릭스 요소 \\(M_{ji}\\)는 \\(\\frac{1}{d_i}\\)가 됩니다. (\\(d_i\\)를 노드 \\(i\\)의 out-degree라고 정의합니다.)\n\\[\n  M_{ji} = \\frac{1}{d_i}\n  \\]\n오른쪽 예시에서처럼 노드 \\(i\\)를 기준으로 총 3개의 out-link들이 있다면 각각의 값은 \\(1/3\\)이 됩니다.\ncolumn 기준 stochastic : 열 방향의 모든 값들을 더하면 1이 되는 확률값이 됩니다.\n\n\nRank Vector \\(r\\)\n\n\\(\\mathbf{r}\\)은 각 페이지의 entry 값을 가지는 \\((node의 수)\\times 1\\) 차원의 벡터입니다.\n각 페이지의 importance score를 \\(r_i\\)로 정의합니다.\n모든 노드의 importance score의 합은 1입니다. 따라서 이 또한 확률값으로 생각할 수 있습니다.\n\n\\[\n\\sum_ir_i = 1\n\\]\n\n\n\n이전에 정의했던 노드의 rank 수식을 새롭게 정의한 매트릭스 \\(M\\)과 벡터 \\(r\\)로 다시 써보면 Flow Equation을 완성할 수 있습니다.\n\\[\n\\mathbf{r}=\\mathbf{M} \\cdot \\mathbf{r}\n\\]\n앞서 살펴본 간단한 그래프 예시를 가져와서 flow equation을 매트릭스 연산으로 표현해보면 아래와 같습니다. (flow equation은 앞내용을 참고)\n\n\n\n\n\n다음과 같은 조건을 만족하며 랜덤하게 웹페이지들을 돌아다니고 있는 유저를 생각해보겠습니다.\n\n시점 \\(t\\)에 페이지 \\(i\\)에 있습니다.\n다음 시점 \\(t+1\\)에 페이지 \\(i\\)로부터 나가는 방향의 out-link들 중에 uniform하게 선택하여 서핑을 합니다.\n앞서 선택된 out-link를 통해 \\(i\\)와 연결된 \\(j\\) 페이지에 도달합니다.\n이 과정(1~3)을 무한으로 반복합니다.\n\n\n여기에서 우리는 시점 의 개념을 고려하여 새로운 개념 정의를 하나 할 수 있습니다.\n\\[\n\\mathbf{p(t)}\n\\]\n\\(\\mathbf{p(t)}\\)는 확률 벡터(probability distribution)로, 이 벡터의 \\(i\\)번째 요소는 앞서 가정한 유저가 시점 \\(t\\)에 페이지 \\(i\\)에 있을 확률을 나타냅니다.\n\n\n\n앞서 정의한 \\(\\mathbf{p(t)}\\)를 가지고 이 유저가 시점 \\(t+1\\)에 있을 확률분포는 다음과 같이 계산합니다.\n\\[\n\\mathbf{p(t+1)}=\\mathbf{M} \\cdot \\mathbf{p(t)}\n\\]\n\n💡 만약에 유저가 웹 서핑을 계속하다가 \\(\\mathbf{p(t+1)} = \\mathbf{p(t)}\\) 같은 상황이 되면 어떨까요?\n\n\\[\n\\mathbf{p(t+1)}=\\mathbf{M} \\cdot \\mathbf{p(t)} = \\mathbf{p(t)}\n\\]\n이러한 상황에서는 더 이상 유저가 특정 페이지에 있을 확률이 변하지 않고 유지되는 경우가 되며, 이를 stationary distribution of a random walk 라고 합니다.\n이러한 형태는 낮설지가 않은데, 앞서 rank vector \\(\\mathbf{r}\\)가 매트릭스 \\(\\mathbf{M}\\)과 flow equation을 구성할 때 이러한 꼴이었으며, 따라서 \\(\\mathbf{r}\\)은 stationary distribution of a random walk 입니다.\n\n\n\n이전 Lecture 2에서 잠시 배웠던 eigenvector와 eignvalue를 생각해보면 다음 수식을 떠올려볼 수 있습니다.\n\\[\n\\lambda \\mathbf{c} = \\mathbf{A} \\mathbf{c}\n\\]\n여기에서 flow equation을 다시 위와 같은 꼴로 작성해보면, 아래와 같이 eigenvalue가 1이고 eigenvector가 \\(\\mathbf{r}\\)인 수식으로 해석될 수 있습니다.\n\\[\n1 \\cdot \\mathbf{r}=\\mathbf{M} \\cdot \\mathbf{r}\n\\]\n따라서 \\(\\mathbf{r}\\)은 매트릭스 \\(\\mathbf{M}\\)의 principle eigenvector(eigenvalue 1)이며, 임의의 벡터 \\(\\mathbf{u}\\)에서 시작해서 계속 매트릭스 \\(\\mathbf{M}\\)을 곱하여 극한 \\(\\mathbf{M}(\\mathbf{M}(...(\\mathbf{M}(\\mathbf{M}\\mathbf{u}))))\\)으로 도달하게되는 long-term distribution이 됩니다. 이러한 방식으로 \\(\\mathbf{r}\\)을 구하는 방법을 Power iteration 이라고 합니다.\n\n\n\n\n웹 구조에서 볼 수 있는 link들을 기반으로 node들의 importance를 측정할 수 있다.\n랜덤하게 웹 서핑하는 유저 모델은 stochastic advacency matrix \\(\\mathbf{M}\\)으로 나타낼 수 있다.\nPageRank 수식은 \\(\\mathbf{r} = \\mathbf{M}\\mathbf{r}\\) 이며, \\(\\mathbf{r}\\)은 (1) 매트릭스 \\(\\mathbf{M}\\)의 principle eigenvector,\n\n\nstationary distribution of a random walk 2가지로 해석될 수 있다.\n\n\n\nOriginal Lecture Video : CS224W: Machine Learning with Graphs 2021 Lecture 4.1 - PageRank"
  },
  {
    "objectID": "posts/lec04.html#power-iteration-method",
    "href": "posts/lec04.html#power-iteration-method",
    "title": "Lecture 4",
    "section": "Power Iteration Method",
    "text": "Power Iteration Method\npower iteration은 2가지 표현식이 있는데 하나는 벡터의 요소 관점에서의 업데이트 식(왼쪽)과 다른 하나는 매트릭스 관점의 업데이트 식(오른쪽)으로 나타낼 수 있습니다.\n\n과정을 살펴보면 다음과 같습니다.\n\n처음 초기화로 모든 노드의 importance score를 똑같은 값으로 만들어 줍니다.(반복적인 연산으로 수렴을 보장하므로 사실 어떤 값으로 초기화하든 상관없습니다.) \\(\\boldsymbol{r}^{(0)}=[1 / N, \\ldots ., 1 / N]^{T}\\)\n반복적인 연산을 하면서 \\(\\mathbf{r}\\) 값을 업데이트합니다. \\(\\boldsymbol{r}^{(t+1)}=\\boldsymbol{M} \\cdot \\boldsymbol{r}^{(t)}\\)\n수렴조건 \\(\\left\\|\\boldsymbol{r}^{(\\boldsymbol{t}+\\mathbf{1})}-\\boldsymbol{r}^{(t)}\\right\\|_{1}<\\varepsilon\\) 을 만족할 때까지 2번 과정의 연산을 진행합니다.\n\n예시 그래프에서의 power iteration 과정은 다음과 같습니다."
  },
  {
    "objectID": "posts/lec04.html#three-questions",
    "href": "posts/lec04.html#three-questions",
    "title": "Lecture 4",
    "section": "Three Questions",
    "text": "Three Questions\n\nDoes this converge? 반복적인 연산과정을 통해 값이 수렴하는가?\nDoes it converge to what we want? 수렴한 값이 우리가 원하는 값인가?\nAre results reasonable? 연산 결과가 합당한가?(말이 되는가?)\n\n(어색한 한국어 번역보다 영어로된 질문에서 얻어가는 insight가 좋을 것 같습니다.)"
  },
  {
    "objectID": "posts/lec04.html#problems",
    "href": "posts/lec04.html#problems",
    "title": "Lecture 4",
    "section": "Problems",
    "text": "Problems\nPageRank에는 2가지의 문제가 있습니다.\n\nDead Ends\n\nout-link를 가지지 않는 일부 페이지(노드)들에서 생기는 문제로 이런 페이지들에서 importance가 leak out 됩니다. leak out의 세어나가다 라는 뜻 그대로 importance flow의 흐름에서 값이 세어나가는 문제를 말합니다.\n아래의 예시에서 페이지 b에서 나가는 out-link가 없다보니 importance update를 한 결과가 \\(r_a = 0, r_b=0\\)이 됨을 확인할 수 있습니다. 이는 앞서 page rank \\(\\mathbf{r}\\) vector의 정의에서 약속한 모든 노드의 importance의 합이 1이 된다는 column stochastic 수학적 전제에서 벗어난 결과 입니다.\n\n\nSpider traps\n\n특정 페이지의 모든 out-link들이 다른페이지로 나가지 않아 결국 spider trap 페이지가 모든 importance 값을 독차지하게 됩니다.\n아래의 예시에서 a에서 walk를 시작하더라고 b로 이동한 후 b에서 빠져나올 수 없습니다. 이런 경우 importance update 결과 모든 importance를 페이지 b가 가지게 되어 \\(r_a = 0, r_b=1\\)이 됩니다. 이런 경우 페이지 a에 아무리 큰 웹 그래프가 연결되어 있다고 하더라도 이동할 수 없습니다. 사실 spider trap은 column stochastic을 만족하기 때문에 수학적으로 문제되진 않습니다. 하지만 우리가 원하지 않는 값에 수렴하는 문제로 볼 수 있습니다."
  },
  {
    "objectID": "posts/lec04.html#solutions",
    "href": "posts/lec04.html#solutions",
    "title": "Lecture 4",
    "section": "Solutions",
    "text": "Solutions\n위의 2가지 문제들 모두 Teleports로 해결할 수 있습니다.\n\n\nDead Ends를 Teleports로 해결하기\n\nDead Ends인 m 페이지에서 column stochastic을 만족하지 않고 모든 값이 0이 되지 않도록 자신을 포함한 그래프의 모든 노드들로 uniform random 하게 teleport 이동을 하도록 합니다. 이때 그래프의 노드가 총 3개이므로 m열의 행렬값을 \\(1/3\\)으로 채워 \\(\\mathbf{M}\\)을 완성합니다.\n\n\nSpider Traps를 Teleports로 해결하기\n\nSpier Trap인 m 페이지에서 다른 노드로 빠져나갈 수 있도록 일정 확률 \\(1-\\beta\\)만큼 random 페이지로 점핑(teleport)할 수 있도록 합니다. 즉, 확률 \\(\\beta\\)만큼은 원래 그래프의 out-links 중에 골라서(random) 이동하고 나머지 확률(\\(1-\\beta\\))로는 out-link와 상관없이 그래프의 모든 페이지들 중에 골라서 이동하여 거미줄, Spider trap에서 벗어나게 되는 것 입니다. 보통 \\(\\beta\\)값으로는 0.8~0.9값을 사용하는 것이 일반적입니다."
  },
  {
    "objectID": "posts/lec04.html#the-google-matrix",
    "href": "posts/lec04.html#the-google-matrix",
    "title": "Lecture 4",
    "section": "The Google Matrix",
    "text": "The Google Matrix\nPageRank에서 생길 수 있는 2가지 문제를 Teleport로 해결한다면 PageRank Equation은 다음과 같이 바꿀 수 있습니다. 첫번째 항은 기존의 수식에 있던 부분으로 페이지 \\(i\\)의 out-link를 random하게 골라서 이동하는 것에다 확률 \\(\\beta\\)값을 곱해 보통 0.8~0.9의 확률로 out-link를 통해 이동하게 합니다.\n두번째 항은 Teleport를로 out-link와 상관없이 그래프의 모든 페이지들중 하나로 랜덤하게 순간이동하는 것을 수식적으로 표현한 부분입니다. 그래프에 존재하는 모든 페이지의 수를 \\(N\\)이라고 할 때, 추가적으로 \\(1/N\\)의 확률로 페이지 \\(j\\)로 갈 수 있고 이는 앞서 확률 \\(\\beta\\)를 제외한 나머지 확률, 약 0.2~0.1의 확률로 이동하는 것이므로 \\(1-\\beta\\)를 곱해줍니다.\n\\[\nr_{j}=\\sum_{i \\rightarrow j} \\beta \\frac{r_{i}}{d_{i}}+(1-\\beta) \\frac{1}{N}\n\\]\n(단, 위의 수식은 \\(\\mathbf{M}\\)에 dead ends가 없다고 가정하며, 실제로 모든 dead ends를 없애거나 dead ends인 부분들에는 random teleport를 확률1로 따르게 하여 계속 다른 노드로 이동하게 할 수 있습니다.)\n구글 매트릭스는 이와 크게 다르지 않습니다. 단지 위의 PageRank equation을 행렬식으로 바꿔쓰면 구글 매트릭스가 됩니다. 각각의 항들이 의미하는 바는 위에서 설명된 것과 동일하며, 두번째 항의 \\(\\left[\\frac{1}{N}\\right]_{N \\times N}\\)는 행렬의 모든 원소가 \\(\\frac{1}{N}\\)으로 채워진 \\(N \\times N\\)차원의 행렬을 말합니다.\n\\[\nG=\\beta M+(1-\\beta)\\left[\\frac{1}{N}\\right]_{N \\times N}\n\\]"
  },
  {
    "objectID": "posts/lec04.html#random-teleports-beta0.8",
    "href": "posts/lec04.html#random-teleports-beta0.8",
    "title": "Lecture 4",
    "section": "Random Teleports (\\(\\beta=0.8\\))",
    "text": "Random Teleports (\\(\\beta=0.8\\))\n아래의 \\(\\beta=0.8\\)일 때 Random Teleports 예시에서 검은색 선들은 teleports를 적용하지 않았을 때의 그래프의 directed links를 표현하며 초록색 선들은 0.2확률의 teleports가 추가된 부분을 나타냅니다. Power iteration을 통해 계산되면 페이지 \\(y, a, m\\)이 각각 \\(7/33, 5/33, 21/33\\)으로 수렴하는 것을 알 수 있고 spider trap인 페이지 m이 모든 importance를 흡수하지 않는 것을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/lec04.html#solving-pagerank-정리",
    "href": "posts/lec04.html#solving-pagerank-정리",
    "title": "Lecture 4",
    "section": "Solving PageRank 정리",
    "text": "Solving PageRank 정리\n\nPageRank \\(\\mathbf{r} = \\mathbf{G} \\mathbf{r}\\)을 power iteration method로 풀 수 있다.\nPageRank에서 생길 수 있는 문제들인 Dead Ends와 Spider Traps를 Random Uniform Teleportation으로 해결할 수 있다.\n\n\n\nOriginal Lecture Video : CS224W: Machine Learning with Graphs 2021 Lecture 4.2 - PageRank: How to Solve?"
  },
  {
    "objectID": "posts/lec04.html#recommendation",
    "href": "posts/lec04.html#recommendation",
    "title": "Lecture 4",
    "section": "Recommendation",
    "text": "Recommendation\n추천 시스템에서 이분그래프(Bipartite graph)로 user와 item의 (구매)관계를 나타낸 Bipartite User-Item Graph는 다음 그림과 같습니다. 여기에서 특정 item Q를 구매한 user에게 어떤 item을 추천해주는 것이 좋을지를 고민한다면, 직관적으로 item Q가 item P와 비슷하게 user들과 관계를 가지고 있을 때 item P를 이 유저에게 추천하는 것이 좋을 것이라고 생각할 수 있습니다. 즉, item Q와 item P가 얼마나 가까운 관계인지 판단하는 것이 중요합니다."
  },
  {
    "objectID": "posts/lec04.html#node-proximity-measurements",
    "href": "posts/lec04.html#node-proximity-measurements",
    "title": "Lecture 4",
    "section": "Node proximity Measurements",
    "text": "Node proximity Measurements\n노드 근접성(proximity) 측정에 대해 생각해보기 위해 아래의 3가지 케이스를 보겠습니다. A-A’은 B-B’보다 더 가까운 관계를 가지고 있다고 할 수 있습니다. 왜냐하면 A-A’ path에서 user을 한번만 거치는데 반해, B-B’path에서는 B-user-item-user-B’ 로 path의 길이가 더 길기 때문입니다. A-A’와 C-C’를 비교해보면 C-C’이 더 가까운 관계를 가지고 있다고 판단할 수 있는데 그 이유는 C-C’이 A-A’보다 더 많은 공통의 이웃(Common Neighbors)를 가지고 있기 때문입니다. C-C’은 A-A’의 shortest path가 2개있는 것으로도 볼 수 있습니다."
  },
  {
    "objectID": "posts/lec04.html#proximity-on-graphs",
    "href": "posts/lec04.html#proximity-on-graphs",
    "title": "Lecture 4",
    "section": "Proximity on Graphs",
    "text": "Proximity on Graphs\n이전에 PageRank를 다시 떠올려보면, (1) rank는 node의 “importance”를 정의하며 (2) 그래프의 모든 node들에 균일 분포로 teleport 이동을 할 수 있는 알고리즘이었습니다.\n여기에 좀 더 아이디어를 덧붙여서 Personalized PageRank 알고리즘을 생각해 볼 수 있습니다. 그래프의 모든 노드들에 대해 균일 분포로 teleport 이동을 하는 것이 아닌, 그래프 노드들의 부분집합(subset) \\(\\mathbf{S}\\)의 노드들로만 teleport 이동을 하도록 할 수 있습니다. 모든 노드들로 랜덤하게 teleport하지 않고 좀 더 연관성이 높은 노드들로 teleport할 수 있도록 하는 것입니다. item Q와 item P가 더 연관성이 높다는 것(Node proxmity ↑)을 어떻게 알 수 있을까요? 이는 Random Walks로 확인해볼 수 있습니다."
  },
  {
    "objectID": "posts/lec04.html#random-walks",
    "href": "posts/lec04.html#random-walks",
    "title": "Lecture 4",
    "section": "Random Walks",
    "text": "Random Walks\nitem Q가 우리가 알고싶은 item 노드들의 집합인 QUERY_NODES집합에 속해있다고 해봅시다. Bipartite User-Item Graph 상에서 QUERY_NODES 집합에 속해 있는 어떤 노드(item Q)에서 시작하여 랜덤하게 움직이면서 과정을 기록합니다. 이 과정을 기록한다는 것은 item↔︎user 사이를 계속 랜덤하게 움직이면서 방문(visit)하게 된 item 노드에는 +1 count를 하는 것을 의미합니다. 이렇게 랜덤하게 움직이면서 이동을 결정할 때마다 일정 확률 ALPHA 만큼 재시작을 하게되는데, 재시작시에는 QUERY_NODES집합에 속해 있는 하나의 노드로 이동해서 다시 랜덤하게 움직이기 시작합니다. (아래 pseudo code 참고)\n\n이렇게 계속 Random Walks를 하다보면 item 노드의 visit 수가 높을수록 query item Q와 높은 관계성을 가진것으로 판단할 수 있습니다.\n Benefits\n이와 같은 Random Walks를 통한 시뮬레이션과 visit 수로 노드들간의 근접성(proximity)을 판단하는데 좋은 이유는 다음과 같은 사항들을 고려하여 similarity를 나타낼 수 있는 방법이기 때문입니다.\n\nMultiple connnections\nMultiple paths\nDirect and Indirect connections\nDegree of the node"
  },
  {
    "objectID": "posts/lec04.html#pagerank-varients-정리",
    "href": "posts/lec04.html#pagerank-varients-정리",
    "title": "Lecture 4",
    "section": "PageRank Varients 정리",
    "text": "PageRank Varients 정리\nPageRank와 이를 변형한 총 3가지 알고리즘들을 정리하면 다음과 같습니다.\n\n\n\n\n\n\n\n\nPageRank\nPersonalized PR\nRandom Walk w/ Restarts\n\n\n\n\n모든 노드들에 같은 확률로 teleport 이동\n특정 노드들로 특정 확률을 가지고 teleport 이동\n항상 똑같은 1개의 노드로 이동\n\n\n\n\n\n\nOriginal Lecture Video : CS224W: Machine Learning with Graphs 2021 Lecture 4.3 - Random Walk with Restarts"
  },
  {
    "objectID": "posts/lec04.html#recall-node-embeddings-embedding-matrix",
    "href": "posts/lec04.html#recall-node-embeddings-embedding-matrix",
    "title": "Lecture 4",
    "section": "Recall: Node Embeddings & Embedding matrix",
    "text": "Recall: Node Embeddings & Embedding matrix\n이전 강의에서 배웠던 embedding matrix \\(\\mathbf{Z}\\)에 대해 다시 떠올려봅시다. 이 매트릭스는 그래프의 각 노드들을 잠재변수 공간(embedding space)으로 encoding하는 행렬로 열의 차원은 embedding하는 크기, 행의 차원은 그래프에 있는 노드의 수가 됩니다. 이 매트릭스의 한 열은 특정 노드 \\(u\\)의 embedding vector \\(\\mathbf{z}_u\\)를 나타내게 됩니다.\n\n\n이러한 Node embedding에서 objective는 그래프상에서 실제로 유사한 노드들의 simliarity가 embedding vector들의 내적(inner product)값도 높도록 만드는 것입니다.\n\n📌 Objective: Maximize \\(\\mathbf{z}_{v}^{\\mathrm{T}} \\mathbf{z}_{u}\\) for node pairs \\((u, v)\\) that are similar"
  },
  {
    "objectID": "posts/lec04.html#matrix-factorization",
    "href": "posts/lec04.html#matrix-factorization",
    "title": "Lecture 4",
    "section": "Matrix Factorization",
    "text": "Matrix Factorization\nEmbedding matrix를 Matriz Factorization 관점에서 다시 생각해봅시다. 그래프를 노드들간의 연결이 되어 있으면 1, 아니면 0으로 나타낸 인접행렬 \\(\\mathbf{A}\\)을 embedding matrix \\(\\mathbf{Z}\\)로 factorization 한다고 생각해볼 수 있습니다. 즉 \\(\\mathbf{Z}^{\\mathrm{T}}\\)와 \\(\\mathbf{Z}\\)의 내적으로 인접행렬 \\(\\mathbf{A}\\)를 만드는 것입니다.\n\\[\n\\mathbf{Z}^{\\mathrm{T}} \\mathbf{Z} = \\mathbf{A}\n\\]\n\n하지만 embedding matrix \\(\\mathbf{Z}\\)의 행의 수, 즉 embedding dimension \\(d\\)는 노드의 수 \\(n\\)보다 작으므로 완벽한 factorization을 할 수 없고 대신에 이를 최적화 기법을 사용하여 근접(approzimate)시킬 수 있습니다. 이 최적화를 목적함수는 다음과 같습니다.\n\\[\n\\min _{\\mathbf{Z}}\\left\\|A-\\boldsymbol{Z}^{T} \\boldsymbol{Z}\\right\\|_{2}\n\\]\n결론은 edge connectivity로 정의된 node similarity을 나타내는 decoder(\\(\\mathbf{Z}\\))의 내적은 \\(\\mathbf{A}\\)의 matrix factorization과 동일하다는 것입니다."
  },
  {
    "objectID": "posts/lec04.html#randomwalk-based-similarity",
    "href": "posts/lec04.html#randomwalk-based-similarity",
    "title": "Lecture 4",
    "section": "RandomWalk-based Similarity",
    "text": "RandomWalk-based Similarity\nDeepWalk와 node2vec 알고리즘에서는 random walks를 기반으로한 좀 더 복잡한 node similarity를 사용합니다. 2개의 알고리즘 모두에서 matrix factorization을 사용하고 있습니다. DeepWalk에서 사용하는 node simliarity는 다음과 같이 정의됩니다. (node2vec은 이보다 조금 더 복잡합니다. 자세한 내용을 확인하고 싶으면 Network Embedding as Matrix Factorization paper를 참고)"
  },
  {
    "objectID": "posts/lec04.html#limitations",
    "href": "posts/lec04.html#limitations",
    "title": "Lecture 4",
    "section": "Limitations",
    "text": "Limitations\nMatrix factorization과 random walk로 node embedding을 할 경우 몇가지 제약(단점)이 있습니다.\n\n그래프에 새로운 노드가 생겼을 때 대응하지 못합니다. training과정에서 보지 못한 노드가 생겼을 때 scratch부터 다시 계산해야 합니다.\n\n\n\n구조적인 유사성을 파악하지 못합니다. 아래의 그림에서 1-2-3과 11-12-13은 그래프에서 비슷한 구조를 가지고 있지만 각 노드마다 unique한 embedding 값으로 인해 구조적인 유사성을 파악하지 못합니다.\n\n\n\n노드, 엣지, 그래프의 feature 정보를 활용할 수 없습니다. DeepWalk나 node2vec에 쓰인 embedding은 노드에 있는 feature 정보를 활용할 수 없습니다. 이는 추후에 배울 Deep Representation Learning으로 해결할 수 있습니다."
  },
  {
    "objectID": "posts/lec04.html#algorithms-정리",
    "href": "posts/lec04.html#algorithms-정리",
    "title": "Lecture 4",
    "section": "Algorithms 정리",
    "text": "Algorithms 정리\n\nPageRank: 그래프에서 노드의 importance를 측정하는 알고리즘이며 인접행렬의 power iteration으로 계산할 수 있다.\n\n총 3가지 관점에서 해석할 수 있다.\n\nFlow formulation\n\n\nRandom walk & Stationary distribution\n\n\nLinear Algebra - eigenvector\n\n\nPersonalized PageRank(PPR): PageRank에서 좀 더 발전시킨 알고리즘으로 random walk로 구한 특정 노드의 중요성을 더 고려하여 teleport를 하는 알고리즘\nRandom walks 기반 Node Embeddings은 Matrix factorization으로 표현될 수 있다.\n\n\n그래프를 행렬로 이해하는 것은 위의 알고리즘들을 이해하는데 매우 중요하다는 것을 알 수 있습니다.\n\n\n\nOriginal Lecture Video : CS224W: Machine Learning with Graphs 2021 Lecture 4.4 - Matrix Factorization and Node Embeddings"
  },
  {
    "objectID": "posts/lec01.html",
    "href": "posts/lec01.html",
    "title": "Lecture 1",
    "section": "",
    "text": "그래프란?\n⇒ a general language for describing and analyzing entities with relations/interactions\n⇒ 서로 관계/상호작용하는 entity들을 설명하고 분석하기 위한 언어라고 할 수 있음 (여기서 entity란 정보의 세계에서 의미있는 하나의 정보 단위)\n\n그래프 예시\n\nevent graphs\ncomputer networks\ndisease pathways\nfood webs\nparticle networks\nunderground networks\nsocial networks\neconomic networks\ncommunication networks\ncitation networks\ninternet\nnetworks of neurons\nknowledge graphs\nregulatory networks\nscene graphs\ncode graphs\nmolecules\n3D shapes\n\n\n⇒ 그래프로 세상에 일어나는 모든 현상과 구조들을 설명할 수 있다(broadly applicable).\n(관심있는 분야의 현상을 그래프로 표현하여 딥러닝 모델 구조로 변환할 수 있는 능력이 있다면…)\ne.g) 분자 구조, 3D 이미지 모형(voxel), 먹이사슬, 소셜 네트워크 등\n(graph와 network의 차이?)\n그래프에서 얻을 수 있는 정보 유형\n\n데이터 포인트 간의 구성(organization)과 연결\n유사한 데이터 포인트 간의 밀접성(similarity)\n데이터 포인트 간의 연결들이 이루는 그래프 구조\n\n그래프가 갖는 구조를 어떻게 활용해 나은 예측을 할 수 있을까?\n⇒ 현상을 명시적(explicitly)으로 잘 반영한 그래프 모델링이 중요하다!\n그래프 ML이 더 어려운 이유?\n⇒ arbitrary size and complex topology\n⇒ spatial locality(공간 지역성)가 없다\n⇒ 이미지나 텍스트 인풋의 경우 어느 한 데이터포인트로부터 다른 데이터 포인트 간 상대적 위치가 정해져있다(e.g 상하좌우). 하지만, 그래프의 경우 축이 되는 데이터 포인트가 존재하지 않는다.\n그래프를 사용한 딥러닝\n⇒ 인풋으로는 그래프를 받고, 아웃풋으로는 아래와 같은 형식(ground truth도 동일한 형식)이 가능하다. >\n\nnode-level\nedge-level\ngraph/subgraph generation\ngraph/subgraph classification\n\n그래프 딥러닝 모델에서 우리가 바라는 플로우\n\n\n위 플로우를 거친 좋은 성능의 모델을 만들기 위해서는, 인풋이 현상을 잘 반영한 embedding vector로 변환될 수 있도록 학습하는 것이 중요하다.(Representation Learning)\n\n코스 과정동안 배울 그래프 방법들\n\ntraditional methods : graphlets, graph kernels\nnode embeddings : DeepWalk, Node2Vec\nGraph Neural Networks : GCN, GraphSAGE, GAT, Theory of GNNs\nKnowledge graphs and reasoning : TransE, BetaE\nDeep Generative Models for graphs\nApplications\n\n\n\n\n\n그래프 ML은 다양한 태스크 커버가 가능하다\n\nNode Level(node classification)\n\n\nexample1) protein folding (구글의 알파폴드)\n\n배경 : 단백질은 아미노산으로 이루어져있는데, 복잡한 3D 입체 구조의 아미노산 연결 때문에 단백질 구조를 파악하는 태스크는 많게는 1-2년까지 걸린다고 함.\nkey idea : spatial graph\n\ngraph : 단백질\nnodes : 아미노산\nedges : 사슬구조\n\n\n\nEdge Level\n\n\nexample1) Recommender system (PinSage)\n\nnodes : users and items\nedges : user-item interactions\n\n\nexample2) drugs and side effects\n\nnodes : drugs & proteins\nedges : interaction\n\nusing 2 heterogeneous graphs(drugs & proteins)\n\n\n\n스크린샷 2022-07-05 오후 11.15.49.png\n\n\n⇒ drug A와 B를 함께 썼을 때, 생길 수 있는 interaction(edge)는 무엇인가?\n\nCommunity(subgraph) level\n\n\nexample1) Traffic Prediction\n\nnodes : Road Segments(도로 구간)\nedges : 도로 구간 교차점\n\n\n⇒ 아웃풋 : 도착 예정 시간\n\nGraph-level prediction\n\n\n4-1) graph classification\nexample1) drug discovery\n\nnodes : atoms\nedges : chemical bonds\ngraph : molecules\n\n⇒ 노드와 에지 정보를 통해 그래프(분자) 예측\n4-2 ) Graph-level generation\nexample1) drug generation\n\nnodes : atoms\nedges : chemical bonds\ngraph : moelcules\n\n⇒ 새로운 그래프(분자) 생성\nexample2) physics simulation (graph evolution)\n\nnodes : particles\nedges : interaction between particles\n\n⇒ t시점 전의 정보로 t시점 이후의 그래프 생성\n\n\n\n\n그래프 구성요소\n\nobjects : nodes, vertices\nInteractions : links, edges\nsystem : network, graph\n**object와 interactions로 이루어진 데이터 구조 ⇒ graph\n그래프로 세상에 일어나는 모든 현상과 구조들을 설명할 수 있다(broadly applicable).\n\n\n표현법에 따라 그래프의 활용방법은 무궁무진하다. 중요한 것은 적절한 표현을 선택하는 것.\n설명하고자하는 현상을 그래프로 정의하고 싶다면, 먼저 아래 두가지 질문을 하자.\n\nwhat are nodes?\nwhat are edges?\n\n\nDirected vs Undirected Graphs\n\n’페이스북의 친구와 인스타그램의 팔로우’는 directed와 undirected graph를 이해하기에 좋은 예이다.\nNode Degrees (차수)\n\nundirected graph\n\nundirected graph의 경우, node degree는 해당 노드에 연결된 에지의 총 갯수\naverage degree는 연결된 에지의 총 갯수 곱하기 2(쌍방향 연결이기 때문)를 그래프를 이루는 전체 노드 수로 나눈 값\n\ndirected graph\n\ndirected graph의 경우, 해당 노드로 향하는 in-degree와 해당 노드로부터 뻗어나가는 out-degree로 나눌 수 있다. node degree는 이 in-degree와 out-degree의 합.\n\n\nBipartite Graph\n\n\n자주 등장하는 또다른 그래프의 종류는 bipartite graph(이분 그래프)이다.\n이분 그래프는 2개의 집합 U와 V의 interaction을 나타낸 그래프이다. U와 V는 서로 독립적인 집합이며, 같은 집합의 원소끼리는 연관되지 않는다. e.g) A와 B 간 연결X\n이분 그래프의 예로는 구매자-구매 아이템 관계 등이 있다.\n\nFolded/Projected Bipartite Graph\n\nBipartite 그래프에서 집합 간의 요소들 간의 상관관계가 명시되어있다면 Folded 또는 Projected Bipartite Graph라고 부른다.\n\n(정보에 depth가 생긴다는 의미에서 folded라고 붙인듯하다.)\n\nAdjacency Matrix\n\n\n그래프의 노드 간 연결관계(edge)를 나타낸 매트릭스이다.\n각 행과 열은 노드의 번수를 의미하고, 0은 연결되지 않음, 1은 연결됨을 의미한다. 만약 3번째 행에 4번째 열이 1이라면, 3번 노드와 4번 노드는 연결되어있음을 의미한다.\nundirect graph라면, 주대각선을 기준으로 adj matrix는 대칭이고, directed graph라면, 대칭이 아닐 수도 있다.\nadjacency matrix는 컴퓨터가 그래프를 이해할 수 있는 형태이지만, 문제는 노드의 수가 수백개에서 수십만개로 늘어나고, 많은 노드들의 연결이 몇 개 없을 때, 메모리 사이즈에 비해 0인 값이 너무 많게되는 문제(sparse)가 발생한다.\n\nEdge List\n\n\nEdge List는 그래프를 엣지들의 리스트로 나타낸 값이다.\n서로 연결된 노드를 짝지어 리스트에 배열한다.\n그래프가 크고 sparse할 때 유용하다.\n\nNode and Edge Attributes\n⇒ 노드와 엣지로 나타낼 수 있는 값들은 어떤 것들이 있을까?\n⇒ 그래프에서 어떤 정보들을 얻을 수 있을까?\n\nweight (e.g., frequency of communication)\n\n엣지가 0과 1 이외의 값을 갖는다면?\n\n\nranking (best friend, second best friend, …)\ntype (friend, relative, co-worker)\nsign (+ / - )\nproperties depending on the structure of the rest of the graph : Number of common friends\n\nMore Types of Graphs\n\nConnected(undirected) graph\n\n\n**연결이 부분적으로만 되어 있어도, 그래프는 adjacency matrix에 표현 가능하다.\n\nConnectivity of Driected Graphs\n\nStrongly connected directed graph\n\n모든 노드들이 다른 모든 노드들로 방향 상관없이 다다르는 path가 항상 있다면, strongly connected directed graph이다.\n\nWeakly connected directed graph\n\n에지 방향을 무시했을 때, 노드 간 전부 연결되어있다면, weakly connected directed graph이다.\n\nStrongly connected components(SCC)\n\n\n그래프에 속한 다른 노드들 전부는 아니지만, 해당 그룹 간의 연결이 한 노드에서 다른 노드로 항상 도달할 수 있다면(strong connection)한다면, 그 그룹을 strongly connected components라고 지칭한다."
  },
  {
    "objectID": "posts/lec02.html",
    "href": "posts/lec02.html",
    "title": "Lecture 2",
    "section": "",
    "text": "2강에선 Graph를 이용한 Traditional ML 방법론들에 대해 설명한다.\n\n\nTraditional ML Pipeline은 크게 2단계로 이루어져 있다.\n\nData Point, Node, Link, Graph(이하 입력)를 Feature Vector로 변환해 ML모델을 학습시킨다.\n새로운 입력이 들어오면 Feature Vector를 얻고 모델을 통해 예측한다.\n\n\n\n\n\n\nGoal : 입력 Set이 주어졌을때 예측값을 만들어내는것 / 모델은 ML Model 사용\n\nDesign Choice\n\nFeature : \\(d\\)차원의 벡터\n입력 : Nodes, Links, Sets of Nodes, Entire Graph\nObjective Function : 무슨 Task를 풀려고 하는가?\n\n\n\n\nTraditional ML Pipeline은 수작업으로 만들어진(Hand-Designed) Feature를 사용한다.\nHand Designed Feature를 Graph의 세 레벨 (Node, Link, Graph)로 나누어 설명한다.\nUndirected Graph를 중점으로 설명한다.\n\n\n\n\n\n\nGoal : Network에서 Node의 구조와 위치를 특정할 수 있는 Feature를 만드는 것\n\n\n\nNode \\(v\\) 의 Degree를 \\(k_v\\) 라고 정의하자. 이때 \\(k_v\\) 는 \\(v\\) 가 갖고있는 Edge(Link)의 수와 같다.\n\n\n\n\nNode Degree는 단순히 이웃한 Node의 갯수를 세므로, 그것들의 중요도를 Capture할 수 없다.\nNode Centrality(\\(c_v\\))는 Graph에서 해당 Node( \\(v\\))의 중요도를 포함시킨 개념이다.\n\n2-1. Engienvector centrality\n\nImportant : \\(v\\)가 Important 이웃노드 \\(u\\)에 둘러싸여 있을 때 \\(v\\)는 Important하다고 한다.\nFormula\n\n\\(c_v = {1 \\over \\lambda} \\sum\\limits_{u\\in N(v)}c_u\\) (\\(\\lambda\\)는 Normalization 상수) ⇒ 이렇게 하면 Recursive함\n\\(\\lambda c= Ac\\) (\\(A\\)는 Adjacency Matrix)\n\n고유값과 고유벡터 형태로 재설정\n\\(c\\)는 \\(A\\)의 고유벡터, \\(\\lambda\\)는 고유값이며 \\(\\lambda_{max}\\)는 항상 양수에 Unique함\n\n\n\n2-2. Betweenness centrality\n\nImportant : \\(v\\)가 다른 노드들을 연결하는 최단 경로에 있을때 Important하다고 한다.(경유)\nFormula : \\(c_v = \\sum\\limits_{s \\neq v \\neq t}{v를\\ 포함하는 s와\\ t사이 \\ 최단\\ 경로 \\over s와\\ t사이\\ 최단\\ 경로}\\)\n\n\n2-3. Closeness Centrality\n\nImportant : \\(v\\)가 다른 모든 노드에 대한 최단 경로의 길이가 짧을때 Important하다고 한다.\nFormula : \\(c_v = 1 \\div \\sum\\limits_{u \\neq v} (u와\\ v사이의\\ 최단경로의\\ 길이)\\)\n\n\n\n\n\n\n\nClustering Coefficient는 Node \\(v\\)의 이웃들이 얼마나 연결되어 있는지를 측정하는 개념이다.\n\\(v\\)의 이웃간 연결된 경우의 수를 이웃 Node들이 서로 연결될 수 있는 전체 경우의 수로 나누어준다.\n\n\n\n\n\n\n\nObservation : Clustering Coefficient는 Ego-Network의 #(Triangle)을 센다)\n\nEgo-Network : Node가 주어졌을때 자기자신과 1차-이웃만 포함한 Network\n#(Triangle) : 3개의 노드가 연결되어 있는 것\n이런 Triangle Counting을 다양한 구조에 대해 일반화 하는것 ⇒ Graphlets의 개념\n\n\n\n\nGraphlet의 목적 : Node \\(u\\)의 이웃 구조를 기술하는 것\n\nGraphlets : \\(u\\)의 이웃 구조를 기술하기 위한 작은 Subgraph(Template?)\n\n\n\n\n\nGraphlet Degree Vector(GDV) : Node의 Graphelt-Based Feature\n\nDegree of Graphlet : 특정 Node가 포함된 Graphlet의 갯수 벡터이다. 어떻게 세는지는 아래의 예시를 통해 설명한다.\n\n\n\n\n아래와 같이 생긴 Graph \\(G\\)에서 Node \\(u\\)에 관심있다고 가정해 보자.\n\nGraph 구조를 보았을때, 최대 3개의 Node가 참여하는 Graphlet을 만들 수 있다.\n\n각각의 Graphlet이 \\(G\\)에서 \\(u\\)를 포함한채로 몇번 나타나는지 세보자\n\nNode \\(u\\)의 GDV는 [2,1,0,2]가 된다.\n\n\nGraphlet Summary\n\n2~5개의 Node가 참여하는 Graphlet의 갯수는 73개이다. 이를 73차원의 벡터로 표시할 수 있고, 각 Index는 특정한 Neighborhood Topology에 Signature이다. 이 벡터를 이용해 Node의 Local Network Topology를 잘 정제된 Feature로 만든게 GDV이며, 앞에서 소개한 방식보다 자세한 정보를 갖고있다.\n\n\n\nNode Level Feature는 2가지 분류로 나눌수 있다\n\n1. Importance Based (Ex Task : 영향력있는 Node찾기(SNS의 셀럽찾기))\n    1. Node Degree : 단순히 이웃의 숫자를 센다\n    2. Node Centrality : Graph에서의 이웃 노드의 중요도를 모델링한다.\n2. Structure Based (Ex Task : Node의 역할 찾기(단백질 구조에서 특정 단백질의 기능찾기))\n    1. Node Degree : 단순히 이웃의 숫자를 센다\n    2. Clustering Coefficient : 이웃이 어떻게 연결되어있는지 측정한다.\n    3. Graphlet Count Vector : 여러 Graphlet들이 출현하는 빈도를 센다.\n\n\n\n\n\n\n\n\nLink-Level Task는 존재하는 Link를 바탕으로 새로운 Link를 예측하는 것이다. Link를 예측하는 Task는 크게 2가지 Formulation이 있다.\n\n1. 랜덤하게 사라진 Link 찾기 : Static한 Graph에 적절하다.\n\n\n2. 시간이 흐름에 따라 생겨나는 Link 찾기 : SNS, Transaction같이 Dynamic한 Graph에 적절하다.\n\nLink-Level Prediction의 자세한 방법은 다음과 같다.\n\n각 Node쌍 (\\(x,y)\\)에 score \\(c(x,y)\\)를 계산한다.\n\\(c(x,y)\\) 내림차순으로 Node 쌍을 정렬한다\nTop \\(N\\)개의 Pair들을 새로운 Link로 예측한다.\n\n\n\n\n\n\n\n1. 노드간 최단 경로\n\n두 Node간 최단경로의 거리를 사용한다. 이웃의 수나 강도에 대한 어떠한 정보도 캡쳐하지 않는다.\n\n2. Local Neighborhood Overlap\n\n두 Node가 공유하는 이웃을 캡쳐한다.\n\nCoomon Neighbors : 단순히 교집합을 구한다\nJaccard’s Coefficient : 교집합의 크기를 합집합으로 나눈다\nAdamic-Adar Index : (SNS에서 잘 동작한다고 하네요) 두 Node가 공유하는 이웃을 u라고 할 때 \\(\\sum \\limits_u {1\\over log(k_u)}\\)\n\n\n3. Global Neighborhood Overlap\n\nLocal Neighborhood Overlap의 단점은 잠재적 이웃도 직접적인 공통 이웃이 없으면 0이 된다는 점이다.\n\n\nKatz Index : 주어진 Node 쌍을 잇는 모든 길이의 경로를 센다. Matrix를 이용해 깔끔하게 연산할수 있다.\n\n\\(A_{uv}\\)는 직접 이웃일 때 1이고 아니면 0이다.\n\\(P_{uv}^{(K)}\\)는 \\(K\\)길이의 \\(u,v\\)를 잇는 경로이다.\n\\(P^{(K)}\\) = \\(A^k\\)이다.\nFormula\n\n\\(S_{uv} = \\sum \\limits_{l=1}^\\infty  \\beta^l A_{uv}^{l}\\) (\\(l\\) : Path의 길이, \\(\\beta\\): Discount Factor(\\(0<\\beta<1\\))\n\\(S_{uv} = \\sum \\limits_{l=1}^\\infty \\beta^lA^i=(I-\\beta A)^{-1}-I\\) (Closed-Form)\n\n\n\n\n\n\nLink Level Feature는 3가지 분류로 나눌수 있다\n\n1. Distance-Based :두 Node간 최단경로의 거리\n2. Local Neigborhood Overlap : 두 Node가 공유하는 이웃의 수\n3. Global Neighborhood Overlap : 두 Node를 잇는 모든 길이의 경로 가중합\n\n\n\n\n\n\n\nGoal : 전체 Graph 구조를 특정할 수 있는 Feature를 만드는 것\n\n아이디어 : Graph로 Feature를 직접 만드는 대신 Kernel을 만들자.\nKernel \\(K(G,G') \\in \\R\\) 은 두 Graph\\((G)\\) 사이의 유사도를 측정한다.\nKernel Matrix는 항상 양의 고유값을 갖고 대칭행렬 이어야한다.\nFeature Representaiton \\(\\phi(.)\\)이 존재한다.\n이 Kernel을 SVM등에 붙여서 사용한다.\n\n\n\n\n\n\nGoal : Graph Feature Vector \\(\\phi(G)\\)를 설계한다\nIdea : Graph에 대해 Bow를 만든다.\n\nBow : NLP에서 모든 단어가 몇 번 나타나는지 세는 방법\nNaive Solution : Node를 Word로 사용한다. 그러나 너무 Naive해서 써먹기 어렵다.\n\nNode Degrees : Node Degree를 Word로 사용한다.\n\n이런식의 Bag-of-something 방식이 Graphlet Kernel과 WL Kernel에서도 사용된다.\n\n\n\n\n\n\n\nIdea : Graph에 존재하는 서로 다른 Graphlet의 숫자를 세자\nNote : 이때의 Graphlet은 Node-Level과 조금 다른 정의를 갖고있다.\n\nIsolated Node로 Graphlet의 일부로 허용한다.\nRoot Node가 없다.\n\n\nGraph \\(G\\)와 Graphlet list \\(g_k = (g_1,g_2 ...g_{nk})\\)가 주어졌을 때 Graphlet Count Vector \\(f_G \\in \\R^{nk}\\) 는 Graph에서 나타나는 각 Graphlet의 인스턴스 수로 정의된다.\n\n\\((f_G)_i = \\#(g_i \\in G)\\) | (for \\(i = 1,2,...n_k)\\)\n\n\n\n\n\n\n\n2개의 Graph \\(G\\) 와 \\(G'\\)가 주어지면, Graphlet Kernel은 \\(K(G,G') = {f_G}^Tf_{G'}\\)로 표현될 수 있다(내적)\nProblem : \\(G\\) 와 \\(G'\\)가 크기(Scale)이 다르면 값이 크게 왜곡된다.\nSolution: \\(f_G\\) 대신 Sum으로 나눠준 \\(h_G\\)를 사용한다. \\(h_G = {f_G \\over Sum(f_G)}\\)\nLimitation : Graphlet을 세는 연산이 매우 Expensive하다 !\n\n\\(n\\) 크기의 Graph의 \\(k\\) 크기의 Graphlet를 세려면 \\(n^k\\)번 연산해야 한다.\n\n\n\n\n\n\n\nGoal : 효율적인 Graph Feature Descriptor를 만드는 것\n\nWL-Kernel은 강력하고 효율적이어서 인기가 많다.\n\nIdea : Node Degree를 이용해 반복적으로 Node Vocap을 풍부하게 만들어 나가는 것\n\nOne-Hop Neighborhood인 Node Degree 방식을 일반화 한 버전이다.\nColor Refinement 알고리즘을 통해 이루어진다.\n\n각 Step에서의 Time-Complexity가 Edge에 따라 Linear하게 증가한다.\n\n\n\n\n\nGiven : Graph \\(G\\)와 그것들의 Set of Nodes \\(V\\)\n\nInitial Color \\(c^{(0)}(v)\\)를 각 노드 \\(v\\)에 할당한다\nIteratively하게 Node의 Color를 정제해 나간다. \\(c^{(k+1)}(v) = HASH(\\{c^{(k)}(v), \\{c^{(k)}(u) \\}_{u\\in N(v)})\\)\n\nHASH는 다른 입력을 다른 Color로 매핑하는 연산이다.\n\n\\(K\\) Step 동안의 정제가 끝나면 \\(c^{(K)}(v)\\) 값을 Summary한다.\n\n\n\n비슷하지만 조금 다른 Graph 두개 (\\(G_1, G_2\\))가 주어졌을때 Color Refienment 예시이다\n\n동일한 Initial Color를 모든 Node에 할당한다.\n\n이웃하는 색상에 대해 Aggregate한다.\n\nAggregate된 Color를 HASH한다.\n\n이웃하는 색상에 대해 Aggregate한다.\n\nAggregate된 Color를 HASH한다.\n\nColor Refinement가 끝나면 WL Kernel이 각 Color가 등장했던 횟수를 세서 Summary한다.\n\nColor Count Vector를 내적해 WL Kernel의 결과값을 구한다.\n\n\n\n\n\n\n\n\nGraph Level Feature는 Kernel을 이용한다.\n\n1. Graphlet Kernel :Bag-of-Graphlets, Computationally Expensive\n2. WL- Kernel :\n    - Color-Refinement 알고리즘을 이용해 반복적으로 피팅\n    - Bag-of-Colors\n    - Computationally Efficient !\n    - Closely related to Graph Neural Networks"
  }
]