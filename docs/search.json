[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Study Group",
    "section": "",
    "text": "CS224w를 공부하며 정리한 글들을 모아둔 블로그입니다.\n\nContributors\n\nalphabetical order\n\n\nEunsung Shin\niDeal\nJooHo Kim\nJung Yeon Lee\nmhkim9714\nsunLeee\nYoojinKim"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS224W Lecture Korean Summary",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 19\n\n\n\n\n\n\n\nLEC19\n\n\n\n\nDesign Space of Graph Neural Networks\n\n\n\n\n\n\nSep 9, 2022\n\n\nYoojinKim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 18\n\n\n\n\n\n\n\nLEC18\n\n\nGUEST\n\n\n\n\nGuest Lecture\n\n\n\n\n\n\nSep 8, 2022\n\n\nJung Yeon Lee\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 17\n\n\n\n\n\n\n\nLEC17\n\n\n\n\nScaling Up GNNs\n\n\n\n\n\n\nSep 7, 2022\n\n\nmhkim9714\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 16\n\n\n\n\n\n\n\nLEC16\n\n\n\n\nAdvanced Topics on GNNs\n\n\n\n\n\n\nAug 31, 2022\n\n\nJooHo Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 15\n\n\n\n\n\n\n\nLEC15\n\n\n\n\nDeep Generative Models for Graphs\n\n\n\n\n\n\nAug 30, 2022\n\n\nEunsung Shin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 14\n\n\n\n\n\n\n\nLEC14\n\n\n\n\nCommunity Structure in Networks\n\n\n\n\n\n\nAug 25, 2022\n\n\nsunLeee\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 13\n\n\n\n\n\n\n\nLEC13\n\n\n\n\nGNNs for Recommender Systems\n\n\n\n\n\n\nAug 24, 2022\n\n\nmhkim9714\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 12\n\n\n\n\n\n\n\nLEC12\n\n\n\n\nFrequent Subgraph Mining with GNNs\n\n\n\n\n\n\nAug 18, 2022\n\n\niDeal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 11\n\n\n\n\n\n\n\nLEC11\n\n\n\n\nReasoning over Knowledge Graphs\n\n\n\n\n\n\nAug 17, 2022\n\n\nJung Yeon Lee\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 10\n\n\n\n\n\n\n\nLEC10\n\n\n\n\nKnowledge Graph Embeddings\n\n\n\n\n\n\nAug 11, 2022\n\n\nYoojinKim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 9\n\n\n\n\n\n\n\nLEC09\n\n\n\n\nTheory of Graph Neural Networks\n\n\n\n\n\n\nAug 10, 2022\n\n\nJooHo Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 8\n\n\n\n\n\n\n\nLEC08\n\n\n\n\nApplications of Graph Neural Networks\n\n\n\n\n\n\nAug 4, 2022\n\n\nEunsung Shin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 7\n\n\n\n\n\n\n\nLEC07\n\n\n\n\nGraph Neural Networks 2 - Design Space\n\n\n\n\n\n\nAug 3, 2022\n\n\nsunLeee\n\n\n\n\n\n\n  \n\n\n\n\nLecture 6\n\n\n\n\n\n\n\nLEC06\n\n\n\n\nGraph Neural Networks 1 - GNN Model\n\n\n\n\n\n\nJul 27, 2022\n\n\nmhkim9714\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture 5\n\n\n\n\n\n\n\nLEC05\n\n\n\n\nLabel Propagation for Node Classification\n\n\n\n\n\n\nJul 20, 2022\n\n\niDeal\n\n\n\n\n\n\n  \n\n\n\n\nLecture 4\n\n\n\n\n\n\n\nLEC04\n\n\n\n\nLink Analysis - PageRank\n\n\n\n\n\n\nJul 14, 2022\n\n\nJung Yeon Lee\n\n\n\n\n\n\n  \n\n\n\n\nLecture 3\n\n\n\n\n\n\n\nLEC03\n\n\n\n\nNode Embeddings\n\n\n\n\n\n\nJul 13, 2022\n\n\nYoojinKim\n\n\n\n\n\n\n  \n\n\n\n\nLecture 2\n\n\n\n\n\n\n\nLEC02\n\n\n\n\nTraditional Methods for ML on Graphs\n\n\n\n\n\n\nJul 7, 2022\n\n\nJooHo Kim\n\n\n\n\n\n\n  \n\n\n\n\nLecture 1\n\n\n\n\n\n\n\nLEC01\n\n\n\n\nIntroduction - Machine Learning for Graphs\n\n\n\n\n\n\nJul 6, 2022\n\n\nEunsung Shin\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/lec18.html",
    "href": "posts/lec18.html",
    "title": "Lecture 18",
    "section": "",
    "text": "Geometric Deep Learning\n\nGNNs Beyond Permutation Equivariance"
  },
  {
    "objectID": "posts/lec04.html",
    "href": "posts/lec04.html",
    "title": "Lecture 4",
    "section": "",
    "text": "4강에서는 Graph를 매트릭스(선형대수) 관점으로 바라보는 것에 대해 이야기 합니다.\n\n다음 3가지 키워드, Random walk(Node Importance), Matrix Factorization, Node embedding를 중심으로 공부합니다. 강의는 총 4파트로 나누어져 진행됩니다.\n\n\n\n웹을 거시적인 관점으로 보게되면, 하나의 웹 페이지 → Node로 하이퍼링크 → Edge로 생각하여 하나의 거대한 Graph로 볼 수 있습니다.\n\nSide issue - 다이나믹하게 새로 페이지들이 생길 수 있습니다. - 다크웹과 같은 접근할 수 없는 페이지들도 있을 수 있습니다.\n\n잠시 Side issue는 내려놓고, 새로 페이지들이 생기지도 않고 기존의 페이지들이 사라지지도 않는 Static pages 상황을 가정해봅시다. 아래의 그림에서처럼 페이지들은 하이퍼링크들로 서로 연결되어 있고, 유저는 페이지들에 달려있는 하이퍼 링크들로 이루어진 연결망을 기반으로 항해하듯이 Navigational 하게 page to page 이동을 하게 됩니다. (오늘날에는 post, comment, like 등의 기반의 transactional한 웹에서의 상호작용이 일어나지만 이는 우선 논외로 하겠습니다.)\n \n위의 그림처럼 웹 그래프는 방향성이 있는 유향 그래프(Directed graph)임을 알 수 있습니다. 위키피디아와 같은 웹 사전 페이지들 간의 관계성이나 논문의 인용 관계 그래프 등에서 예시를 쉽게 찾아볼 수 있습니다.\n\n\n\n\n웹을 하나의 거대한 유향 그래프로 생각할 때 한가지 중요한 insight가 있습니다.\n\n💡 모든 웹 페이지들이 똑같이 중요하지는 않다\n\n바로 각 페이지의 중요성이 똑같지 않다는 이야기는 그래프에서 각 노드의 중요성(importance)가 다르다는 말로 바꿔 생각할 수 있습니다. 아래 사진을 보면 직관적으로 파란색 노드가 빨간색 노드보다 더 중요할 것 같다라고 생각할 수 있습니다. 왜 그렇게 보일까요? 아직 노드의 중요성에 대해 정의하지 않았지만 그래프에서 각 노드를 중심으로 뻗어있는 edge(link)의 수가 한눈에 비교되기 때문에 직관적으로 파악할 수 있는 것입니다. 이처럼 웹 그래프의 link structure를 가지고 우리는 각 페이지들(node)의 ranking을 매길 수 있습니다.\n\n\n\n\n각 페이지들의 중요성(importance)를 파악하기 위해 Link Analysis가 필요합니다.\n본 수업에서 다룰 Link Analysis 알고리즘들은 아래 총 3개에 대해서 다룰 예정입니다.\n\nPageRank\nPersonalized PageRank (PPR)\nRandom Walk with Restarts\n\n\n\n\n링크가 투표용지라고 생각해봅시다. 여기서 유향 그래프인 웹 그래프에서 링크는 2가지 종류가 있다는 것을 다시한번 생각해봐야 합니다.\n\nin-comming links(in-links): 기준 페이지로 들어오는 방향의 링크\nout-going links(out-links): 기준 페이지에서 나가는 방향의 링크\n\n이렇게 방향까지 고려하여 링크를 투표라고 생각할 때, 엄밀히 말하자면 in-link를 투표라고 생각해야 할 것 입니다. 한 가지 더 생각해볼 문제는 모든 in-link들을 동등하게 생각할 수 있는가?라는 문제입니다. 어떤 링크들은 다른 링크들에 비해 좀 더 중요한 페이지로부터(from) 기준페이지로(to) 온 링크일 수도 있기 때문에 count에 차등을 둬야 하지 않을까라고 생각할 수도 있습니다. 이런 고민들은 결국 페이지들이 서로 연결되어 있어서 recursive한 문제로 볼 수 있습니다.\n\n➕ recursive한 문제란, 물리고 물리는 문제로 생각할 수 있습니다. A→B 링크에서 A가 중요한 페이지라는 사실을 기반으로 B가 중요해지고, 이어지는 B→C 링크에서 이 영향을 이어받아 C까지 중요한 페이지라고 판단하게 되기 때문입니다.\n\n\n\n\nThe “Flow” Model\n위에서 설명한 recursive한 특성을 기반으로 중요성이 흘러가는(flow) 모델을 생각해볼 수 있습니다. 중요성을 \\(r\\)이라는 변수로 두고 기준 노드 j의 importance가 어떻게 flow되는지 살펴보겠습니다.\n\nj로 in-link되어있는 i, k 의 importance \\(r_i\\), \\(r_k\\)를 각 노드의 out-link의 수만큼 나누어서 j로 전달됩니다. i 노드의 out-link는 총 3개 이므로 \\(\\frac{r_i}{3}\\), k노드의 out-link는 총 4개 이므로 \\(\\frac{r_k}{4}\\)로 계산되어 두 값의 합이 \\(r_j\\)가 됩니다.\n\\(r_j\\)는 j노드의 out-link를 통해 flow하게 되는데 out-link의 수, 즉 3으로 나누어져 \\(\\frac{r_j}{3}\\) 값이 각각의 다음 노드들로 \\(r_j\\)값이 전달되게 됩니다.\n\n\n이처럼 importance가 높은 페이지로부터 in-link된 페이지는 영향을 받아 importance가 높아짐을 알 수 있습니다. 노드 \\(j\\)의 rank, \\(r_j\\)를 정의하면 다음과 같이 수식으로 나타낼 수 있습니다. (이때 \\(d_i\\)는 노드 i의 out-degree를 말합니다.)\n\\[\nr_{j}=\\sum_{i \\rightarrow j} \\frac{r_{i}}{d_{i}}\n\\]\n다음과 같은 예시에서 각 기준 노드를 가지고 in-link들을 고려하여 “Flow equation”을 계산해보면 다음과 같다.\n\n\n\n\n\n\n\n\n\n노드 y\n노드 a\n노드 m\n\n\n\n\ny에서 오는 링크 + a에서 오는 링크\ny에서 오는 링크 + m에서 오는 링크\na에서 오는 링크\n\n\n\\(r_y = \\frac{r_y}{2} + \\frac{r_a}{2}\\)\n\\(r_a = \\frac{r_y}{2} + r_m\\)\n\\(r_m = \\frac{r_a}{2}\\)\n\n\n\n\n➕ 3 Unknowns, 3 Equations 이기 때문에 4번째 constraint로 \\(r_y + r_a + r_m =1\\)로 scale관련 constraint를 추가하여 Gaussian elimination을 사용하여 선형방정식으로 풀려고 하는 생각은 좋지 않다. 왜냐하면 importance는 이런식으로 scalable하지 않기 때문이다. (It’s not scalable) 좀 더 정교한 설계가 필요하다.\n\n\n\n\nStochastic Adjacency Matrix \\(\\mathbf{M}\\)\n\n\\(\\mathbf{M}\\)은 \\((node의 수)\\times (node의 수)\\)차원의 매트릭스 입니다.\n\\(i\\)→\\(j\\) 링크에서 매트릭스 요소 \\(M_{ji}\\)는 \\(\\frac{1}{d_i}\\)가 됩니다. (\\(d_i\\)를 노드 \\(i\\)의 out-degree라고 정의합니다.)\n\\[\n  M_{ji} = \\frac{1}{d_i}\n  \\]\n오른쪽 예시에서처럼 노드 \\(i\\)를 기준으로 총 3개의 out-link들이 있다면 각각의 값은 \\(1/3\\)이 됩니다.\ncolumn 기준 stochastic : 열 방향의 모든 값들을 더하면 1이 되는 확률값이 됩니다.\n\n\nRank Vector \\(r\\)\n\n\\(\\mathbf{r}\\)은 각 페이지의 entry 값을 가지는 \\((node의 수)\\times 1\\) 차원의 벡터입니다.\n각 페이지의 importance score를 \\(r_i\\)로 정의합니다.\n모든 노드의 importance score의 합은 1입니다. 따라서 이 또한 확률값으로 생각할 수 있습니다.\n\n\\[\n\\sum_ir_i = 1\n\\]\n\n\n\n이전에 정의했던 노드의 rank 수식을 새롭게 정의한 매트릭스 \\(M\\)과 벡터 \\(r\\)로 다시 써보면 Flow Equation을 완성할 수 있습니다.\n\\[\n\\mathbf{r}=\\mathbf{M} \\cdot \\mathbf{r}\n\\]\n앞서 살펴본 간단한 그래프 예시를 가져와서 flow equation을 매트릭스 연산으로 표현해보면 아래와 같습니다. (flow equation은 앞내용을 참고)\n\n\n\n\n\n다음과 같은 조건을 만족하며 랜덤하게 웹페이지들을 돌아다니고 있는 유저를 생각해보겠습니다.\n\n시점 \\(t\\)에 페이지 \\(i\\)에 있습니다.\n다음 시점 \\(t+1\\)에 페이지 \\(i\\)로부터 나가는 방향의 out-link들 중에 uniform하게 선택하여 서핑을 합니다.\n앞서 선택된 out-link를 통해 \\(i\\)와 연결된 \\(j\\) 페이지에 도달합니다.\n이 과정(1~3)을 무한으로 반복합니다.\n\n\n여기에서 우리는 시점 의 개념을 고려하여 새로운 개념 정의를 하나 할 수 있습니다.\n\\[\n\\mathbf{p(t)}\n\\]\n\\(\\mathbf{p(t)}\\)는 확률 벡터(probability distribution)로, 이 벡터의 \\(i\\)번째 요소는 앞서 가정한 유저가 시점 \\(t\\)에 페이지 \\(i\\)에 있을 확률을 나타냅니다.\n\n\n\n앞서 정의한 \\(\\mathbf{p(t)}\\)를 가지고 이 유저가 시점 \\(t+1\\)에 있을 확률분포는 다음과 같이 계산합니다.\n\\[\n\\mathbf{p(t+1)}=\\mathbf{M} \\cdot \\mathbf{p(t)}\n\\]\n\n💡 만약에 유저가 웹 서핑을 계속하다가 \\(\\mathbf{p(t+1)} = \\mathbf{p(t)}\\) 같은 상황이 되면 어떨까요?\n\n\\[\n\\mathbf{p(t+1)}=\\mathbf{M} \\cdot \\mathbf{p(t)} = \\mathbf{p(t)}\n\\]\n이러한 상황에서는 더 이상 유저가 특정 페이지에 있을 확률이 변하지 않고 유지되는 경우가 되며, 이를 stationary distribution of a random walk 라고 합니다.\n이러한 형태는 낮설지가 않은데, 앞서 rank vector \\(\\mathbf{r}\\)가 매트릭스 \\(\\mathbf{M}\\)과 flow equation을 구성할 때 이러한 꼴이었으며, 따라서 \\(\\mathbf{r}\\)은 stationary distribution of a random walk 입니다.\n\n\n\n이전 Lecture 2에서 잠시 배웠던 eigenvector와 eignvalue를 생각해보면 다음 수식을 떠올려볼 수 있습니다.\n\\[\n\\lambda \\mathbf{c} = \\mathbf{A} \\mathbf{c}\n\\]\n여기에서 flow equation을 다시 위와 같은 꼴로 작성해보면, 아래와 같이 eigenvalue가 1이고 eigenvector가 \\(\\mathbf{r}\\)인 수식으로 해석될 수 있습니다.\n\\[\n1 \\cdot \\mathbf{r}=\\mathbf{M} \\cdot \\mathbf{r}\n\\]\n따라서 \\(\\mathbf{r}\\)은 매트릭스 \\(\\mathbf{M}\\)의 principle eigenvector(eigenvalue 1)이며, 임의의 벡터 \\(\\mathbf{u}\\)에서 시작해서 계속 매트릭스 \\(\\mathbf{M}\\)을 곱하여 극한 \\(\\mathbf{M}(\\mathbf{M}(...(\\mathbf{M}(\\mathbf{M}\\mathbf{u}))))\\)으로 도달하게되는 long-term distribution이 됩니다. 이러한 방식으로 \\(\\mathbf{r}\\)을 구하는 방법을 Power iteration 이라고 합니다.\n\n\n\n\n웹 구조에서 볼 수 있는 link들을 기반으로 node들의 importance를 측정할 수 있다.\n랜덤하게 웹 서핑하는 유저 모델은 stochastic advacency matrix \\(\\mathbf{M}\\)으로 나타낼 수 있다.\nPageRank 수식은 \\(\\mathbf{r} = \\mathbf{M}\\mathbf{r}\\) 이며, \\(\\mathbf{r}\\)은 (1) 매트릭스 \\(\\mathbf{M}\\)의 principle eigenvector,\n\n\nstationary distribution of a random walk 2가지로 해석될 수 있다.\n\n\n\nOriginal Lecture Video : CS224W: Machine Learning with Graphs 2021 Lecture 4.1 - PageRank"
  },
  {
    "objectID": "posts/lec04.html#power-iteration-method",
    "href": "posts/lec04.html#power-iteration-method",
    "title": "Lecture 4",
    "section": "Power Iteration Method",
    "text": "Power Iteration Method\npower iteration은 2가지 표현식이 있는데 하나는 벡터의 요소 관점에서의 업데이트 식(왼쪽)과 다른 하나는 매트릭스 관점의 업데이트 식(오른쪽)으로 나타낼 수 있습니다.\n\n과정을 살펴보면 다음과 같습니다.\n\n처음 초기화로 모든 노드의 importance score를 똑같은 값으로 만들어 줍니다.(반복적인 연산으로 수렴을 보장하므로 사실 어떤 값으로 초기화하든 상관없습니다.) \\(\\boldsymbol{r}^{(0)}=[1 / N, \\ldots ., 1 / N]^{T}\\)\n반복적인 연산을 하면서 \\(\\mathbf{r}\\) 값을 업데이트합니다. \\(\\boldsymbol{r}^{(t+1)}=\\boldsymbol{M} \\cdot \\boldsymbol{r}^{(t)}\\)\n수렴조건 \\(\\left\\|\\boldsymbol{r}^{(\\boldsymbol{t}+\\mathbf{1})}-\\boldsymbol{r}^{(t)}\\right\\|_{1}<\\varepsilon\\) 을 만족할 때까지 2번 과정의 연산을 진행합니다.\n\n예시 그래프에서의 power iteration 과정은 다음과 같습니다."
  },
  {
    "objectID": "posts/lec04.html#three-questions",
    "href": "posts/lec04.html#three-questions",
    "title": "Lecture 4",
    "section": "Three Questions",
    "text": "Three Questions\n\nDoes this converge? 반복적인 연산과정을 통해 값이 수렴하는가?\nDoes it converge to what we want? 수렴한 값이 우리가 원하는 값인가?\nAre results reasonable? 연산 결과가 합당한가?(말이 되는가?)\n\n(어색한 한국어 번역보다 영어로된 질문에서 얻어가는 insight가 좋을 것 같습니다.)"
  },
  {
    "objectID": "posts/lec04.html#problems",
    "href": "posts/lec04.html#problems",
    "title": "Lecture 4",
    "section": "Problems",
    "text": "Problems\nPageRank에는 2가지의 문제가 있습니다.\n\nDead Ends\n\nout-link를 가지지 않는 일부 페이지(노드)들에서 생기는 문제로 이런 페이지들에서 importance가 leak out 됩니다. leak out의 세어나가다 라는 뜻 그대로 importance flow의 흐름에서 값이 세어나가는 문제를 말합니다.\n아래의 예시에서 페이지 b에서 나가는 out-link가 없다보니 importance update를 한 결과가 \\(r_a = 0, r_b=0\\)이 됨을 확인할 수 있습니다. 이는 앞서 page rank \\(\\mathbf{r}\\) vector의 정의에서 약속한 모든 노드의 importance의 합이 1이 된다는 column stochastic 수학적 전제에서 벗어난 결과 입니다.\n\n\nSpider traps\n\n특정 페이지의 모든 out-link들이 다른페이지로 나가지 않아 결국 spider trap 페이지가 모든 importance 값을 독차지하게 됩니다.\n아래의 예시에서 a에서 walk를 시작하더라고 b로 이동한 후 b에서 빠져나올 수 없습니다. 이런 경우 importance update 결과 모든 importance를 페이지 b가 가지게 되어 \\(r_a = 0, r_b=1\\)이 됩니다. 이런 경우 페이지 a에 아무리 큰 웹 그래프가 연결되어 있다고 하더라도 이동할 수 없습니다. 사실 spider trap은 column stochastic을 만족하기 때문에 수학적으로 문제되진 않습니다. 하지만 우리가 원하지 않는 값에 수렴하는 문제로 볼 수 있습니다."
  },
  {
    "objectID": "posts/lec04.html#solutions",
    "href": "posts/lec04.html#solutions",
    "title": "Lecture 4",
    "section": "Solutions",
    "text": "Solutions\n위의 2가지 문제들 모두 Teleports로 해결할 수 있습니다.\n\n\nDead Ends를 Teleports로 해결하기\n\nDead Ends인 m 페이지에서 column stochastic을 만족하지 않고 모든 값이 0이 되지 않도록 자신을 포함한 그래프의 모든 노드들로 uniform random 하게 teleport 이동을 하도록 합니다. 이때 그래프의 노드가 총 3개이므로 m열의 행렬값을 \\(1/3\\)으로 채워 \\(\\mathbf{M}\\)을 완성합니다.\n\n\nSpider Traps를 Teleports로 해결하기\n\nSpier Trap인 m 페이지에서 다른 노드로 빠져나갈 수 있도록 일정 확률 \\(1-\\beta\\)만큼 random 페이지로 점핑(teleport)할 수 있도록 합니다. 즉, 확률 \\(\\beta\\)만큼은 원래 그래프의 out-links 중에 골라서(random) 이동하고 나머지 확률(\\(1-\\beta\\))로는 out-link와 상관없이 그래프의 모든 페이지들 중에 골라서 이동하여 거미줄, Spider trap에서 벗어나게 되는 것 입니다. 보통 \\(\\beta\\)값으로는 0.8~0.9값을 사용하는 것이 일반적입니다."
  },
  {
    "objectID": "posts/lec04.html#the-google-matrix",
    "href": "posts/lec04.html#the-google-matrix",
    "title": "Lecture 4",
    "section": "The Google Matrix",
    "text": "The Google Matrix\nPageRank에서 생길 수 있는 2가지 문제를 Teleport로 해결한다면 PageRank Equation은 다음과 같이 바꿀 수 있습니다. 첫번째 항은 기존의 수식에 있던 부분으로 페이지 \\(i\\)의 out-link를 random하게 골라서 이동하는 것에다 확률 \\(\\beta\\)값을 곱해 보통 0.8~0.9의 확률로 out-link를 통해 이동하게 합니다.\n두번째 항은 Teleport를로 out-link와 상관없이 그래프의 모든 페이지들중 하나로 랜덤하게 순간이동하는 것을 수식적으로 표현한 부분입니다. 그래프에 존재하는 모든 페이지의 수를 \\(N\\)이라고 할 때, 추가적으로 \\(1/N\\)의 확률로 페이지 \\(j\\)로 갈 수 있고 이는 앞서 확률 \\(\\beta\\)를 제외한 나머지 확률, 약 0.2~0.1의 확률로 이동하는 것이므로 \\(1-\\beta\\)를 곱해줍니다.\n\\[\nr_{j}=\\sum_{i \\rightarrow j} \\beta \\frac{r_{i}}{d_{i}}+(1-\\beta) \\frac{1}{N}\n\\]\n(단, 위의 수식은 \\(\\mathbf{M}\\)에 dead ends가 없다고 가정하며, 실제로 모든 dead ends를 없애거나 dead ends인 부분들에는 random teleport를 확률1로 따르게 하여 계속 다른 노드로 이동하게 할 수 있습니다.)\n구글 매트릭스는 이와 크게 다르지 않습니다. 단지 위의 PageRank equation을 행렬식으로 바꿔쓰면 구글 매트릭스가 됩니다. 각각의 항들이 의미하는 바는 위에서 설명된 것과 동일하며, 두번째 항의 \\(\\left[\\frac{1}{N}\\right]_{N \\times N}\\)는 행렬의 모든 원소가 \\(\\frac{1}{N}\\)으로 채워진 \\(N \\times N\\)차원의 행렬을 말합니다.\n\\[\nG=\\beta M+(1-\\beta)\\left[\\frac{1}{N}\\right]_{N \\times N}\n\\]"
  },
  {
    "objectID": "posts/lec04.html#random-teleports-beta0.8",
    "href": "posts/lec04.html#random-teleports-beta0.8",
    "title": "Lecture 4",
    "section": "Random Teleports (\\(\\beta=0.8\\))",
    "text": "Random Teleports (\\(\\beta=0.8\\))\n아래의 \\(\\beta=0.8\\)일 때 Random Teleports 예시에서 검은색 선들은 teleports를 적용하지 않았을 때의 그래프의 directed links를 표현하며 초록색 선들은 0.2확률의 teleports가 추가된 부분을 나타냅니다. Power iteration을 통해 계산되면 페이지 \\(y, a, m\\)이 각각 \\(7/33, 5/33, 21/33\\)으로 수렴하는 것을 알 수 있고 spider trap인 페이지 m이 모든 importance를 흡수하지 않는 것을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/lec04.html#solving-pagerank-정리",
    "href": "posts/lec04.html#solving-pagerank-정리",
    "title": "Lecture 4",
    "section": "Solving PageRank 정리",
    "text": "Solving PageRank 정리\n\nPageRank \\(\\mathbf{r} = \\mathbf{G} \\mathbf{r}\\)을 power iteration method로 풀 수 있다.\nPageRank에서 생길 수 있는 문제들인 Dead Ends와 Spider Traps를 Random Uniform Teleportation으로 해결할 수 있다.\n\n\n\nOriginal Lecture Video : CS224W: Machine Learning with Graphs 2021 Lecture 4.2 - PageRank: How to Solve?"
  },
  {
    "objectID": "posts/lec04.html#recommendation",
    "href": "posts/lec04.html#recommendation",
    "title": "Lecture 4",
    "section": "Recommendation",
    "text": "Recommendation\n추천 시스템에서 이분그래프(Bipartite graph)로 user와 item의 (구매)관계를 나타낸 Bipartite User-Item Graph는 다음 그림과 같습니다. 여기에서 특정 item Q를 구매한 user에게 어떤 item을 추천해주는 것이 좋을지를 고민한다면, 직관적으로 item Q가 item P와 비슷하게 user들과 관계를 가지고 있을 때 item P를 이 유저에게 추천하는 것이 좋을 것이라고 생각할 수 있습니다. 즉, item Q와 item P가 얼마나 가까운 관계인지 판단하는 것이 중요합니다."
  },
  {
    "objectID": "posts/lec04.html#node-proximity-measurements",
    "href": "posts/lec04.html#node-proximity-measurements",
    "title": "Lecture 4",
    "section": "Node proximity Measurements",
    "text": "Node proximity Measurements\n노드 근접성(proximity) 측정에 대해 생각해보기 위해 아래의 3가지 케이스를 보겠습니다. A-A’은 B-B’보다 더 가까운 관계를 가지고 있다고 할 수 있습니다. 왜냐하면 A-A’ path에서 user을 한번만 거치는데 반해, B-B’path에서는 B-user-item-user-B’ 로 path의 길이가 더 길기 때문입니다. A-A’와 C-C’를 비교해보면 C-C’이 더 가까운 관계를 가지고 있다고 판단할 수 있는데 그 이유는 C-C’이 A-A’보다 더 많은 공통의 이웃(Common Neighbors)를 가지고 있기 때문입니다. C-C’은 A-A’의 shortest path가 2개있는 것으로도 볼 수 있습니다."
  },
  {
    "objectID": "posts/lec04.html#proximity-on-graphs",
    "href": "posts/lec04.html#proximity-on-graphs",
    "title": "Lecture 4",
    "section": "Proximity on Graphs",
    "text": "Proximity on Graphs\n이전에 PageRank를 다시 떠올려보면, (1) rank는 node의 “importance”를 정의하며 (2) 그래프의 모든 node들에 균일 분포로 teleport 이동을 할 수 있는 알고리즘이었습니다.\n여기에 좀 더 아이디어를 덧붙여서 Personalized PageRank 알고리즘을 생각해 볼 수 있습니다. 그래프의 모든 노드들에 대해 균일 분포로 teleport 이동을 하는 것이 아닌, 그래프 노드들의 부분집합(subset) \\(\\mathbf{S}\\)의 노드들로만 teleport 이동을 하도록 할 수 있습니다. 모든 노드들로 랜덤하게 teleport하지 않고 좀 더 연관성이 높은 노드들로 teleport할 수 있도록 하는 것입니다. item Q와 item P가 더 연관성이 높다는 것(Node proxmity ↑)을 어떻게 알 수 있을까요? 이는 Random Walks로 확인해볼 수 있습니다."
  },
  {
    "objectID": "posts/lec04.html#random-walks",
    "href": "posts/lec04.html#random-walks",
    "title": "Lecture 4",
    "section": "Random Walks",
    "text": "Random Walks\nitem Q가 우리가 알고싶은 item 노드들의 집합인 QUERY_NODES집합에 속해있다고 해봅시다. Bipartite User-Item Graph 상에서 QUERY_NODES 집합에 속해 있는 어떤 노드(item Q)에서 시작하여 랜덤하게 움직이면서 과정을 기록합니다. 이 과정을 기록한다는 것은 item↔︎user 사이를 계속 랜덤하게 움직이면서 방문(visit)하게 된 item 노드에는 +1 count를 하는 것을 의미합니다. 이렇게 랜덤하게 움직이면서 이동을 결정할 때마다 일정 확률 ALPHA 만큼 재시작을 하게되는데, 재시작시에는 QUERY_NODES집합에 속해 있는 하나의 노드로 이동해서 다시 랜덤하게 움직이기 시작합니다. (아래 pseudo code 참고)\n\n이렇게 계속 Random Walks를 하다보면 item 노드의 visit 수가 높을수록 query item Q와 높은 관계성을 가진것으로 판단할 수 있습니다.\n Benefits\n이와 같은 Random Walks를 통한 시뮬레이션과 visit 수로 노드들간의 근접성(proximity)을 판단하는데 좋은 이유는 다음과 같은 사항들을 고려하여 similarity를 나타낼 수 있는 방법이기 때문입니다.\n\nMultiple connnections\nMultiple paths\nDirect and Indirect connections\nDegree of the node"
  },
  {
    "objectID": "posts/lec04.html#pagerank-varients-정리",
    "href": "posts/lec04.html#pagerank-varients-정리",
    "title": "Lecture 4",
    "section": "PageRank Varients 정리",
    "text": "PageRank Varients 정리\nPageRank와 이를 변형한 총 3가지 알고리즘들을 정리하면 다음과 같습니다.\n\n\n\n\n\n\n\n\nPageRank\nPersonalized PR\nRandom Walk w/ Restarts\n\n\n\n\n모든 노드들에 같은 확률로 teleport 이동\n특정 노드들로 특정 확률을 가지고 teleport 이동\n항상 똑같은 1개의 노드로 이동\n\n\n\n\n\n\nOriginal Lecture Video : CS224W: Machine Learning with Graphs 2021 Lecture 4.3 - Random Walk with Restarts"
  },
  {
    "objectID": "posts/lec04.html#recall-node-embeddings-embedding-matrix",
    "href": "posts/lec04.html#recall-node-embeddings-embedding-matrix",
    "title": "Lecture 4",
    "section": "Recall: Node Embeddings & Embedding matrix",
    "text": "Recall: Node Embeddings & Embedding matrix\n이전 강의에서 배웠던 embedding matrix \\(\\mathbf{Z}\\)에 대해 다시 떠올려봅시다. 이 매트릭스는 그래프의 각 노드들을 잠재변수 공간(embedding space)으로 encoding하는 행렬로 열의 차원은 embedding하는 크기, 행의 차원은 그래프에 있는 노드의 수가 됩니다. 이 매트릭스의 한 열은 특정 노드 \\(u\\)의 embedding vector \\(\\mathbf{z}_u\\)를 나타내게 됩니다.\n\n\n이러한 Node embedding에서 objective는 그래프상에서 실제로 유사한 노드들의 simliarity가 embedding vector들의 내적(inner product)값도 높도록 만드는 것입니다.\n\n📌 Objective: Maximize \\(\\mathbf{z}_{v}^{\\mathrm{T}} \\mathbf{z}_{u}\\) for node pairs \\((u, v)\\) that are similar"
  },
  {
    "objectID": "posts/lec04.html#matrix-factorization",
    "href": "posts/lec04.html#matrix-factorization",
    "title": "Lecture 4",
    "section": "Matrix Factorization",
    "text": "Matrix Factorization\nEmbedding matrix를 Matriz Factorization 관점에서 다시 생각해봅시다. 그래프를 노드들간의 연결이 되어 있으면 1, 아니면 0으로 나타낸 인접행렬 \\(\\mathbf{A}\\)을 embedding matrix \\(\\mathbf{Z}\\)로 factorization 한다고 생각해볼 수 있습니다. 즉 \\(\\mathbf{Z}^{\\mathrm{T}}\\)와 \\(\\mathbf{Z}\\)의 내적으로 인접행렬 \\(\\mathbf{A}\\)를 만드는 것입니다.\n\\[\n\\mathbf{Z}^{\\mathrm{T}} \\mathbf{Z} = \\mathbf{A}\n\\]\n\n하지만 embedding matrix \\(\\mathbf{Z}\\)의 행의 수, 즉 embedding dimension \\(d\\)는 노드의 수 \\(n\\)보다 작으므로 완벽한 factorization을 할 수 없고 대신에 이를 최적화 기법을 사용하여 근접(approzimate)시킬 수 있습니다. 이 최적화를 목적함수는 다음과 같습니다.\n\\[\n\\min _{\\mathbf{Z}}\\left\\|A-\\boldsymbol{Z}^{T} \\boldsymbol{Z}\\right\\|_{2}\n\\]\n결론은 edge connectivity로 정의된 node similarity을 나타내는 decoder(\\(\\mathbf{Z}\\))의 내적은 \\(\\mathbf{A}\\)의 matrix factorization과 동일하다는 것입니다."
  },
  {
    "objectID": "posts/lec04.html#randomwalk-based-similarity",
    "href": "posts/lec04.html#randomwalk-based-similarity",
    "title": "Lecture 4",
    "section": "RandomWalk-based Similarity",
    "text": "RandomWalk-based Similarity\nDeepWalk와 node2vec 알고리즘에서는 random walks를 기반으로한 좀 더 복잡한 node similarity를 사용합니다. 2개의 알고리즘 모두에서 matrix factorization을 사용하고 있습니다. DeepWalk에서 사용하는 node simliarity는 다음과 같이 정의됩니다. (node2vec은 이보다 조금 더 복잡합니다. 자세한 내용을 확인하고 싶으면 Network Embedding as Matrix Factorization paper를 참고)"
  },
  {
    "objectID": "posts/lec04.html#limitations",
    "href": "posts/lec04.html#limitations",
    "title": "Lecture 4",
    "section": "Limitations",
    "text": "Limitations\nMatrix factorization과 random walk로 node embedding을 할 경우 몇가지 제약(단점)이 있습니다.\n\n그래프에 새로운 노드가 생겼을 때 대응하지 못합니다. training과정에서 보지 못한 노드가 생겼을 때 scratch부터 다시 계산해야 합니다.\n\n\n\n구조적인 유사성을 파악하지 못합니다. 아래의 그림에서 1-2-3과 11-12-13은 그래프에서 비슷한 구조를 가지고 있지만 각 노드마다 unique한 embedding 값으로 인해 구조적인 유사성을 파악하지 못합니다.\n\n\n\n노드, 엣지, 그래프의 feature 정보를 활용할 수 없습니다. DeepWalk나 node2vec에 쓰인 embedding은 노드에 있는 feature 정보를 활용할 수 없습니다. 이는 추후에 배울 Deep Representation Learning으로 해결할 수 있습니다."
  },
  {
    "objectID": "posts/lec04.html#algorithms-정리",
    "href": "posts/lec04.html#algorithms-정리",
    "title": "Lecture 4",
    "section": "Algorithms 정리",
    "text": "Algorithms 정리\n\nPageRank: 그래프에서 노드의 importance를 측정하는 알고리즘이며 인접행렬의 power iteration으로 계산할 수 있다.\n\n총 3가지 관점에서 해석할 수 있다.\n\nFlow formulation\n\n\nRandom walk & Stationary distribution\n\n\nLinear Algebra - eigenvector\n\n\nPersonalized PageRank(PPR): PageRank에서 좀 더 발전시킨 알고리즘으로 random walk로 구한 특정 노드의 중요성을 더 고려하여 teleport를 하는 알고리즘\nRandom walks 기반 Node Embeddings은 Matrix factorization으로 표현될 수 있다.\n\n\n그래프를 행렬로 이해하는 것은 위의 알고리즘들을 이해하는데 매우 중요하다는 것을 알 수 있습니다.\n\n\n\nOriginal Lecture Video : CS224W: Machine Learning with Graphs 2021 Lecture 4.4 - Matrix Factorization and Node Embeddings"
  },
  {
    "objectID": "posts/lec05.html",
    "href": "posts/lec05.html",
    "title": "Lecture 5",
    "section": "",
    "text": "Main question today: Given a network with labels on some nodes, how do we assign labels to all other nodes in the network? Example: In a network, some nodes are fraudsters, and some other nodes are fully trusted. How do you find the other fraudsters and trustworthy nodes? We already discussed node embeddings as a method to solve this in Lecture 3\n오늘 주요 질문: 일부 노드에 레이블이 있는 네트워크에서 네트워크의 다른 모든 노드에 레이블을 할당하려면 어떻게 해야 합니까? 예: 네트워크에서 일부 노드는 사기꾼이고 다른 일부 노드는 완전히 신뢰됩니다. 다른 사기꾼과 신뢰할 수 있는 노드를 어떻게 생각하십니까? 우리는 이것을 해결하기 위한 방법으로 노드 임베딩을 이미 강의 3에서 논의하였습니다\n\n💬 이번 수업에서 다룬 내용은 한 그래프에서 특정 노드들에 레이블에 매겨져 있을 때, 다른 노드들에 레이블을 매기는 방법에 대해 다루게 된다.\n\n\n\n\n\n\n\nGiven labels of some nodes Let’s predict labels of unlabeled nodes This is called semi-supervised node classification\n일부 노드의 지정된 레이블 레이블이 없는 노드의 레이블을 예측해 봅시다. 이를 준지도 노드 분류라고 합니다.\n\n💬 위 그림과 같이 그래프 전체에서 일부 노드에 레이블이 있고, 나머지 노드에는 레이블이 없을 때, 이를 예측하는 방법론을 다룬다.\n\n\n\n\n\nMain question today: Given a network with labels on some nodes, how do we assign labels to all other nodes in the network?\nToday we will discuss an alternative framework: Message passing\nIntuition: Correlations (dependencies) exist in networks.\n\nIn other words: Similar nodes are connected.\nKey concept is collective classification: Idea of assigning labels to all nodes in a network together.\n\nWe will look at three techniques today:\n\nRelational classification\nIterative classification\nCorrect & Smooth\n\n오늘 주요 질문: 일부 노드에 레이블이 있는 네트워크에서 네트워크의 다른 모든 노드에 레이블을 할당하려면 어떻게 해야 합니까?\n오늘은 대체 프레임워크에 대해 논의하겠습니다: 메시지 전달\n직관: 상관 관계(의존성) 네트워크에 존재하다\n\n즉, 유사한 노드가 연결되어 있습니다.\n핵심 개념은 집단 분류: 네트워크의 모든 노드에 label을 함께 할당하는 아이디어입니다.\n\n오늘은 세 가지 기법을 살펴보겠습니다.\n\n관계구분\n반복구분\n정확하고 매끄러운\n\n\n\n💬 이를 Semi-supervised node classification이라 한다고 한다.\n이미 레이블이 있는 정보와, 없는 구조를 학습하여 새로이 레이블을 예측하기 때문으로 보인다.\n이를 Message Passing이라는 프레임 워크를 살펴보게 되는데, message passing 의 주요 개념은 상관관계(correlation)이다.\n즉, 비슷한 노드 간에는 상관관계가 있을 것이기 때문에, 노드 간에 상관관계를 파악하여 이를 이용해 레이블을 예측하고자 한다.\n여기서 반복적으로 labeling 작업이 발생하기 때문에, 이전 수업인 Pagerank와 비슷한 점이 있다고 넘어가자.\n\n\n\n\n\nBehaviors of nodes are correlated across the links of the network\nCorrelation: Nearby nodes have the same color (belonging to the same class)\n노드의 동작은 네트워크 링크 전체에서 상관됨\n상관: 근처 노드의 색상은 동일합니다(동일한 클래스에 속함).\n\n\n  \n\n\nTwo explanations for why behaviors of nodes in networks are correlated:\n네트워크에서 노드의 동작이 상관되는 이유에 대한 두 가지 설명은 다음과 같다.\n\n\n\n\n\n💬 위의 그래프에서, 비슷한 노드(같은 레이블을 가지고 있는 노드, 초록색, 빨간색과 같이)는 근처에 위치하는 것을 알 수 있다. 이와 같이, 노드 간의 관계를 파악하여, 서로 근접한 노드를 찾을 수 있다면, 거기에 비슷한 레이블을 매길 수 있을 것이다. 여기서 이제 어떻게 노드 간의 상관관계를 이끌어 낼 수 있을지 살펴보자. 노드 간의 상관관계는 Homophily, Influence 두 개념에 의해 정의되어진다. 두 개념 모두 사회과학 분야에서 social network를 분석하면서 쓰인 개념으로 보이는데, 하나씩 자세히 살펴보도록 하자.\n\n\n\n\n\nHomophily: The tendency of individuals to associate and bond with similar others\n“Birds of a feather flock together”\nIt has been observed in a vast array of network studies, based on a variety of attributes (e.g., age, gender, organizational role, etc.)\nExample: Researchers who focus on the same research area are more likely to establish a connection (meeting at conferences, interacting in academic talks, etc.)\n동성애: 개인이 유사한 타인과 연관되고 유대감을 갖는 경향\n“깃털 같은 새들”\n다양한 속성(예: 연령, 성별, 조직 역할 등)을 기반으로 한 광범위한 네트워크 연구에서 관찰되었다.\n예시: 동일한 연구 분야에 집중하는 연구자는 (학회에서의 회의, 학술 강연에서의 상호작용 등) 연결을 확립할 가능성이 높다.\n\n\n\n\n\n\n\nExample of homophily\n\nOnline social network\n\nNodes = people\nEdges = friendship\nNode color = interests (sports, arts, etc.)\n\nPeople with the same interest are more closely connected due to homophily\n\n동음이의 예\n\n온라인 소셜 네트워크\n\n노드 = 사람\n가장자리 = 우정\n노드 색상 = 관심 사항(스포츠, 예술 등)\n\n동종애로 인해 같은 관심사를 가진 사람들이 더 밀접하게 연결되어 있다.\n\n\n\n\n\n💬 개인의 특징은 나이, 성별, 직업, 취미, 거주지 등 다양한 요소가 있을 것이다. Homophily는 개인들이 비슷한 특징을 가지는 타인들과 서로 연결되고, 함께 행동하려고 한다는 개념이다. 예를 들어, 머신러닝 연구자들은 비슷한 학회를 동시에 참여하고, 비슷한 커뮤니티를 읽고 토론하게 되면서 자연스레 친분을 쌓게 된다. 또한, 가수들은 서로 같이 공연하고, 서로의 앨범을 들으면서 서로 사회적으로 연결되게 된다. 위의 그래프는 한 학교의 학생들을 나타낸 그래프인데, 학생 개인이 노드, 친분이 엣지로 표현되어 있다. 이때 노드의 레이블인 색은 각 학생의 관심사로 운동, 예술 등이 있다. 직관적으로 살펴보아도 알 수 있지만 총 4개의 작은 그룹으로 나누어질 수 있으며, 각 그룹은 비슷한 관심사를 가지는 학생들이 모여있는 것을 알 수 있다. 이를 Homophily라고 할 수 있을 것이다.\n\n\n\n\n\nInfluence: Social connections can influence the individual characteristics of a person.\n\nExample: I recommend my musical preferences to my friends, until one of them grows to like my same favorite genres!\n\n영향: 사회적 관계는 개인의 특성에 영향을 미칠 수 있다.\n\n예시: 친구 중 한 명이 제가 좋아하는 장르를 좋아하게 될 때까지 친구들에게 제 음악적 취향을 추천합니다!\n\n\n\n\n\n\n💬 Influence는 사회적으로 연결된 개인 간에는 서로 영향을 주고 받으면서 비슷한 특징을 가지게 된다는 것을 의미한다. 예를 들어, 내가 1, 2학년 때는 경상계열 친구들과 친하게 지내면서, 경제, 사회, 제도 등에 관심을 가지고 해당 직군을 희망했다면, 3, 4학년이 되면서 통계나 수학, 컴퓨터 공학 친구들과 친하게 지내면서 ML, DL, 코딩 등에 관심을 가지고 해당 직군을 희망하게 된 것이 Influence 때문이라고 할 수 있을 것이다.\n\n\n\n\n\n\n\n\nHow do we leverage this correlation observed in networks to help predict node labels?\n네트워크에서 관찰된 이 상관 관계를 활용하여 노드 레이블을 예측하는 방법은 무엇입니까?\n\n\n\n\nHow do we predict the labels for the nodes in grey?\n노드의 레이블을 회색으로 어떻게 예측합니까?\n\n\n\n\nSimilar nodes are typically close together or directly connected in the network:\n\nGuilt-by-association: If I am connected to a node with label 𝑋, then I am likely to have label 𝑋 as well.\nExample: Malicious/benign web page: Malicious web pages link to one another to increase visibility, look credible, and rank higher in search engines\n\nClassification label of a node 𝑣 in network may depend on:\n\nFeatures of 𝑣\nLabels of the nodes in 𝑣’s neighborhood\nFeatures of the nodes in 𝑣’s neighborhood\n\n유사한 노드는 일반적으로 네트워크에서 서로 가까이 있거나 직접 연결됩니다.\n\n연관별 죄책감: 레이블 𝑋이 있는 노드에 연결되어 있다면 레이블 𝑋도 있을 수 있습니다.\n예: 악성/악성 웹 페이지: 악성 웹 페이지는 가시성을 높이고 신뢰도를 높이며 검색 엔진에서 더 높은 순위를 차지하기 위해 서로 연결됩니다.\n\n네트워크에 있는 노드 \\(v\\)의 분류 라벨은 다음 조건에 따라 달라질 수 있습니다.\n\n\\(v\\)의 기능\n\\(v\\)의 이웃에 있는 노드의 레이블\n\\(v\\)의 이웃에 있는 노드의 기능\n\n\n\n💬 한 그래프 내에서 비슷한 노드는 가까이 위치하거나 직접 연결되어 있을 것이다. 이를 Guilt-by-association이라고 하는데, 노드 b가 아직 레이블이 없는 상태에서, 이웃노드 x가 1로 레이블 되어 있다면, 이웃노드 x와 가깝기 때문에 노드 b 역시 1로 레이블 될 가능성이 높다는 개념이다. 구체적인 예시로는 스팸 사이트들이 안전한 사이트와의 연결고리는 생성할 수 없기 때문에, 노출도와 신뢰도를 높이기 위해 서로 링크를 연결하는 경향이 있는데, 이를 이용해서 스팸 사이트 하나를 잡을 수 있다면, 서로 연결된 다른 스팸 사이트도 색출 할 수 있다고 한다. 이때, 노드 v의 분류에 이용하는 정보들은 다음과 같다. 1. 노드 v의 변수들 2. 노드 v의 이웃 노드들의 레이블 3. 노드 v의 이웃 노드들의 변수들\n\n\n\n\nFormal setting:\nGiven:\n\nGraph\nFew labeled nodes\n\nFind: Class (red/green) of remaining nodes\nMain assumption: There is homophily in the network\n공식 설정:\n제공됨:\n\n그래프\n레이블이 지정된 노드 몇 개\n\n찾기: 나머지 노드의 클래스(빨간색/녹색)\n주요 가정: 네트워크에 동질성이 있습니다.\n\n\n\nExample task:\n\nLet 𝑨 be a 𝑛×𝑛 adjacency matrix over 𝑛 nodes\nLet Y = \\([0,1]^n\\) be a vector of labels:\n\n\\(Y_v\\) = 1 belongs to Class1\n\\(Y_v\\) = 0 belongs to Class0\nThere are unlabeled node needs to be classified\n\nGoal: Predict which unlabeled nodes are likely Class 1, and which are likely Class 0\n\n작업 예:\n\nA를 n개 노드의 nxn 인접 행렬로 설정\nY = \\([0,1]^n\\) 을 레이블 벡터라고 하자:\n\n\\(Y_v\\) = 1이 클래스 1에 속함\n\\(Y_v\\) = 0이 클래스 0에 속함\n라벨이 지정되지 않은 노드가 분류되어야 합니다\n\n목표: 레이블이 없는 노드가 클래스 1일 가능성이 높고 클래스 0일 가능성이 높은 노드를 예측합니다.\n\n\n💬 실제로 이용하게 되는 입력값은 인접행렬: \\(A_{n*n}\\), 레이블 벡터 \\(Y=[0,1]^n\\), \\(Y_v\\) = 1이 클래스 1에 속함, \\(Y_v\\) = 0이 클래스 0에 속함이 있으며, 아직 레이블이 없는 노드에 대해 레이블이 0 혹은 1일 확률을 계산하는 것이 목표이다.\n\n\n\n\nHow to predict the labels \\(Y_v\\) for the unlabeled nodes \\(v\\) (in grey color)? Each node \\(v\\) has a feature vector \\(f_v\\) Labels for some nodes are given (1 for green, 0 for red) Task: Find \\(P(Y_v)\\) given all features and the network 라벨이 없는 노드 \\(v\\)(회색)에 대한 레이블 \\(Y_v\\)를 예측하는 방법 각 노드 \\(v\\)에는 특징 벡터 \\(f_v\\)가 있습니다. 일부 노드의 라벨이 제공됩니다(녹색은 1, 빨간색은 0). 작업: 모든 기능 및 네트워크가 주어진 \\(P(Y_v)\\) 찾기\n\\[\nP(Y_v)=?\n\\]\n\n\n\n\n\n\n\nMany applications under this setting:\n\nDocument classification\nPart of speech tagging\nLink prediction\nOptical character recognition\nImage/3D data segmentation\nEntity resolution in sensor networks\nSpam and fraud detection\n\n이 설정 아래의 많은 응용 프로그램:\n\n문서구분\n음성 태그의 일부\n링크 예측\n광학식 문자 인식\n영상/3D 데이터 분할\n센서 네트워크의 엔티티 해상도\n스팸 및 부정 행위 탐지\n\n\n\n💬 이러한 방법론은 위와 같은 다양한 분야에 적용이 가능하다.\n\n\n\n\ncollective classification을 전반적으로 살펴보면 다음과 같다.\n\n📌 Intuition(직관) : 노드 간 상관관계를 이용해 서로 연결된 노드들을 동시에 분류하기\n\n이때 1차 마르코프 연쇄를 사용한다. 즉, 노드 \\(\\mathrm{v}\\) 의 레이블 \\(Y_{v}\\) 를 예측하기 위해서는 이웃노드 \\(N_{v}\\) 만 필요하다는 것이다. 2차 마르코프 연쇄를 사용할 경우 \\(N_{v}\\) 의 이웃노드 역시 사용할 것이다. 1차 마르포크 연쇄를 사용할 경우 식은 다음과 같아질 것이다.\n\\[\nP\\left(Y_{v}\\right)=P\\left(Y_{v} \\mid N_{v}\\right)\n\\]\ncollective classification은 하나의 모델을 이용하거나 기존의 분류모델처럼 한번의 과정으로 구성되지 않고 총 세가지 과정으로 구성된다.\n\n\n\nLocal Classifier\n최초로 레이블을 할당하기 위해 사용되는 분류기이다. 즉, 그래프에서 레이블이 없는 노드들에 대해 우선 노드를 생성해야 하기 때문에, 기존의 분류문제와 동일하게 구성된다. 이때 예측 과정은 각 노드의 변수만 사용하여 이미 레이블이 있는 노드로 학습하고, 레이블이 없는 노드로 예측하게 된다. 그래프의 구조적 정보가 사용되지 않는다는 점에 유의하자.\nRelational Classifier\n노드 간 상관관계를 파악하기 위해 이웃 노드의 레이블과 변수를 사용하는 분류기이다. 이를 통해 이웃 노드의 레이블과 변수와 현재 노드의 변수를 이용해 현재 노드의 레이블을 예측할 수 있다. 이때, 이웃노드의 정보가 사용되기 때문에, 그래프의 구조적 정보가 사용된다.\nCollective Inference\nCollective Classification은 한번의 예측으로 종료되지 않는 것이 핵심이다. 특정 조건을 만족할 때까지 각 노드에 대해 분류하고 레이블을 업데이트한다. 이때의 조건이란 더이상 레이블이 변하지 않거나, 정해진 횟수를 의미한다. 이때 동일한 변수를 가진 노드라 하더라도 그래프의 구조에 따라 최종 예측이 달라질 수 있다는 점을 유념하자.\n\n\n\n\nWe focus on semi-supervised binary node classification\nWe will introduce three approaches:\n\nRelational classification\nIterative classification\nCorrect & Smooth\n\n우리는 준지도 이진 노드 분류에 초점을 맞춘다.\n다음 세 가지 접근 방식을 소개합니다.\n\n관계구분\n반복구분\n정확하고 매끄러운\n\n\nCS224W: Machine Learning with Graphs 2021 Lecture 5.1 - Message passing and Node Classification"
  },
  {
    "objectID": "posts/lec05.html#relational-classification",
    "href": "posts/lec05.html#relational-classification",
    "title": "Lecture 5",
    "section": "Relational Classification",
    "text": "Relational Classification\n\nProbabilistic Relational Classifier\n\nIdea: Propagate node labels across the network\n\nClass probability \\(Y_v\\) of node \\(v\\) is a weighted average of class probabilities of its neighbors.\n\nFor labeled nodes \\(v\\), initialize label \\(Y_v\\) with ground-truth label \\(Y^*_v\\).\nFor unlabeled nodes, initialize \\(Y_v\\) = 0.5.\nUpdate all nodes in a random order until convergence or until maximum number of iterations is reached.\nUpdate for each node \\(v\\) and label \\(c\\) (e.g. 0 or 1 )\n\\[\n  P\\left(Y_{v}=c\\right)=\\frac{1}{\\sum_{(v, u) \\in E} A_{v, u}} \\sum_{(v, u) \\in E} A_{v, u} P\\left(Y_{u}=c\\right)\n  \\]\n\nIf edges have strength/weight information, \\(A_{v, u}\\) can be the edge weight between \\(v\\) and \\(u\\)\n\\(P\\left(Y_{v}=c\\right)\\) is the probability of node \\(v\\) having label \\(c\\)\n\nChallenges:\n\nConvergence is not guaranteed\nModel cannot use node feature information\n\n아이디어: 노드 레이블을 네트워크에 전파\n\n노드 \\(v\\)의 클래스 확률 \\(Y_v\\)는 이웃의 클래스 확률에 대한 가중 평균이다.\n\n레이블링된 노드 \\(v\\)의 경우 지상 실측 레이블 \\(Y^*_v\\)로 레이블 \\(Y_v\\)를 초기화한다.\n레이블이 없는 노드의 경우 \\(Y_v\\) = 0.5를 초기화합니다.\n수렴할 때까지 또는 최대 반복 횟수에 도달할 때까지 모든 노드를 임의의 순서로 업데이트합니다.\n각 노드 \\(v\\) 및 레이블 \\(c\\)에 대한 업데이트(예: 0 또는 1)\n\\[\n  P\\left(Y_{v}=c\\right)=\\frac{1}{\\sum_{(v, u) \\in E} A_{v, u}} \\sum_{(v, u) \\in E} A_{v, u} P\\left(Y_{u}=c\\right)\n  \\]\n\n가장자리의 강도/무게 정보가 있는 경우 \\(A_{v,u}\\)는 \\(v\\)와 \\(u\\) 사이의 에지 가중치일 수 있습니다.\n\\(P\\left(Y_{v}=c\\right)\\)는 노드 \\(v\\)가 \\(c\\) 레이블을 가질 확률이다.\n\n과제:\n\n수렴이 보장되지 않음\n모델은 노드 피쳐 정보를 사용할 수 없습니다.\n\n\n\n💬 기본 아이디어 : 노드 \\(v\\)의 레이블 확률 \\(Y_v\\)는 노드 \\(v\\)의 주변노드의 레이블 확률의 가중평균과 같다. 즉, 레이블이 없는 노드에 대해 이웃 노드들의 레이블 확률을 가중평균하여 예측하게 된다. 이진 분류문제라 가정하고, 모든 노드에 레이블 확률이 존재해야 하기 때문에, 레이블이 없는 노드는 0.5의 확률로 초기화하여 시작하게 된다. 업데이트는 반복적으로 진행되며, 모든 노드에 대해 수렴하거나 반복횟수에 도달할 경우 멈추게 된다. 노드 \\(*v*\\)에 대한 확률을 계산하는 법은 위 수식과 같다. 이때 행렬 A는 인접행렬에 해당한다. 즉, 주변의 이웃노드의 확률 \\(*P\\left(Y_{v}=c\\right)*\\)를 평균하여 사용하되, 해당 이웃노드와 연결된 degree만큼 가중치, \\(*A_{v,u}*\\)를 주게 된다. 이때 두가지 문제점이 있다. 1. 위 수식은 수렴이 보장되지 않는다. 2. 모델이 노드의 변수를 활용하지 않는다. 이에 대해선 이후 모델을 통해 개선될 것이라 기대해보자.\n\n\n\nExample: Initialization\nInitialization:\n\nAll labeled nodes with their labels\nAll unlabeled nodes 0.5 (belonging to class 1 with probability 0.5)\n\n초기화:\n\n라벨이 표시된 모든 노드\n표시되지 않은 모든 노드 0.5 (확률 0.5의 클래스 1에 속함)\n\n\n\n\n\n💬 위 그래프에서 본래 레이블이 있는 노드는 녹색과 적색으로 표시가 되어 있다. 이에 대해 이진분류 문제이기 때문에, 녹색을 기준으로 확률을 계산하여, 녹색 노드는 1, 적색 노드는 0, 레이블이 없는 회색노드는 0.5로 초기화한다.\n\n\n\nExample: \\(1^{st}\\) Iteration, Update Node 3\n\nUpdate for the \\(1^{st}\\) Iteration:\n\nFor node 3, \\(N_3=[1,2,4]\\)\n\n\\(1^{st}\\) 반복에 대한 업데이트:\n\n노드 3의 경우 \\(N_3=[1,2,4]\\)\n\n\n\n\n\n\n💬 각 노드별로 이웃노드의 확률을 이용해 순차적으로 확률을 업데이트한다. 위 그래프의 경우 undirected이고 엣지가 각 노드 간 최대 하나만 존재하기 때문에 단순 평균을 통해 새로운 확률을 계산하게 된다.\n\n\n\nExample: \\(1^{st}\\) Iteration, Update Node 4\n\nUpdate for the \\(1^{st}\\) Iteration:\n\nFor node 4, \\(N_4=[1,3,5,6]\\)\n\n\\(1^{st}\\) 반복에 대한 업데이트:\n\n노드 4의 경우 \\(N_4=[1,3,5,6]\\)\n\n\n\n\n\n\n💬 위에서 업데이트된 3번 노드의 확률을 이용해 4번 노드 역시 업데이트하게 된다. 즉, 노드를 업데이트하는 순서에 따라 계산이 조금씩 달라지게 된다.\n\n\n\nExample: \\(1^{st}\\) Iteration, Update Node 5\n\nUpdate for the \\(1^{st}\\) Iteration:\n\nFor node 5, \\(N_5=[4,6,7,8]\\)\n\n\\(1^{st}\\) 반복에 대한 업데이트:\n\n노드 5의 경우 \\(N_5=[4,6,7,8]\\)\n\n\n\n\n\n\n💬 3, 4번 노드를 업데이트하고 나서 5번 노드를 업데이트한다. 이때 역시 업데이트된 4번 노드의 확률을 이용하게 된다.\n\n\n\nExample: After \\(1^{st}\\) Iteration\nAfter Iteration 1 (a round of updates for all unlabeled nodes) 반복 후 1 (라벨이 지정되지 않은 모든 노드에 대한 업데이트 라운딩)\n\n\n\n\n💬 첫번째 이터가 종료된 후의 모습니다. 9번 노드의 경우 녹색 노드만 연결되어 있기 때문에 확률이 1로 고정된 모습을 보이고 있다. 이외에 8번 노드 역시 주변에 녹색 노드 2개, 확률이 높은 노드(5번) 한개가 이웃노드이기 때문에 확률이 높은 것을 볼 수 있다. 하지만 4번 노드의 경우 녹색 노드와 적색 노드 각각 하나씩 연결되어 있고, 녹색과 가까운 노드와 적색과 가까운 노드 하나씩 연결되어 있어 0.5에 가까운 확률을 보이고 있다.\n\n\n\nExample: After \\(2^{nd}\\) Iteration\nAfter Iteration 2 (반복 후 2)\n\n\n\n\n\nExample: After \\(3^{rd}\\) Iteration\nAfter Iteration 3 (반복 후 3)\n\n\n\n\n\nExample: After \\(4^{th}\\) Iteration\nAfter Iteration 4 (반복 후 4)\n\n\n\n\n💬 이후 0.5보다 확률이 큰 노드들은 class 1이라 분류하고, 0.5보다 작은 노드들은 class 0으로 분류한다. class 0 : 1, 2, 3 class 1 : 4, 5, 6, 7, 8, 9\n\n\n\nExample: Convergence\n\nAll scores stabilize after 4 iterations.\nWe therefore predict:\n\nNodes 4,5,8,9 belong to class 1 (\\(𝑃_{Y_v}\\) > 0.5)\nNodes 3 belong to class 0 (\\(𝑃_{Y_v}\\) < 0.5)\n\n4회 반복 후 모든 점수가 안정됩니다.\n따라서 다음과 같이 예측한다.\n\n노드 4,5,8,9가 클래스 1(\\(𝑃_{Y_v}\\) > 0.5)에 속함\n노드 3은 클래스 0(\\(𝑃_{Y_v}\\) < 0.5)에 속합니다.\n\n\n\n\n\n\n💬 몇 번의 iterations을 지나자 모든 확률값이 수렴하고 있는 모습을 보여 종료되었으나 이 모델의 수렴이 보장되지 않는 단점이 존재한다. 이를 이전에 배웠던 개념과 연결지어 생각하자면, Influence가 녹아있는 모델이라고 할 수 있겠다. 가까운 노드의 영향을 받아 이웃 노드와 비슷한 레이블 분포를 가지도록 업데이트하고 있기 때문이다. 그 결과 8번 노드는 이웃노드가 녹색일 확률이 높으니 해당 분포와 비슷해지고, 4번 노드는 이웃노드로 녹색과 적색에 가까운 노드들이 모두 있어 0.5에 가까운 확률을 가지게 되었다. Relational Classification은 그래프의 구조적 정보를 일부 활용하고, 노드 레이블은 활용하지만, 노드의 변수 = node featured information(attributes)를 활용하지 못한다는 단점이 크게 작용한다. 단지 노드의 라벨과 이웃들의 엣지를 이용하는 것인 네트워크 정보만 사용하게 된다. 결국 주어진 정보를 최대한 활용하지 못하는 머신러닝 모델은 부족한 점이 많은 모델일 뿐이다."
  },
  {
    "objectID": "posts/lec05.html#iterative-classification",
    "href": "posts/lec05.html#iterative-classification",
    "title": "Lecture 5",
    "section": "Iterative Classification",
    "text": "Iterative Classification\n\nIterative Classification\n\nRelational classifier does not use node attributes.\nHow can one leverage them?\nMain idea of iterative classification:\nClassify node \\(v\\) based on its attributes \\(f_v\\) as well as labels \\(z_v\\) of neighbor set \\(N_v\\).\nInput: Graph\n\n\\(f_{v}\\) : feature vector for node \\(v\\)\nSome nodes \\(v\\) are labeled with \\(Y_{v}\\)\n\nTask: Predict label of unlabeled nodes\nApproach: Train two classifiers: \\(\\phi_{1}\\left(f_{v}\\right)=\\) Predict node label based on node feature vector \\(f_{v}\\). This is called base classifier.\n\\(\\phi_{2}\\left(f_{v}, z_{v}\\right)=\\) Predict label based on node feature vector \\(f_{v}\\) and summary \\(z_{v}\\) of labels of \\(v'{\\text {s }}\\) neighbors.\nThis is called relational classifier.\n관계 분류자가 노드 특성을 사용하지 않습니다.\n어떻게 그들을 활용할 수 있을까?\n반복 분류의 주요 개념:\n노드 \\(v\\)는 속성 \\(f_v\\)와 인접 세트 \\(N_v\\)의 레이블 \\(z_v\\)를 기준으로 분류한다.\n입력: 그래프\n\n\\(f_{v}\\) : 노드 \\(v\\)의 피쳐 벡터\n일부 노드 \\(v\\)는 \\(Y_{v}\\)로 레이블 지정됨\n\n작업: 레이블이 없는 노드의 레이블 예측\n접근법: 두 가지 분류기 훈련: \\(\\phi_{1}\\left(f_{v}\\right)=\\) 노드 특징 벡터 \\(f_{v}\\)를 기반으로 노드 레이블을 예측한다. 이를 기본 분류기라고 합니다.\n\\(\\phi_{2}\\left(f_{v}, z_{v}\\right)=\\) 노드 특징 벡터 \\(f_{v}\\)와 \\(v'{\\text {s}}\\) 이웃 레이블의 요약 \\(z_{v}\\)를 기반으로 레이블을 예측한다.\n이를 관계 분류기라고 합니다.\n\n\n💬 Relational Classifier는 노드의 변수를 활용하지 않는 것이 단점이라고 했다. Iterative Classifier는 노드의 변수를 활용하여 이를 개선했다. 핵심 아이디어는 다음과 같다. > 노드 \\(`v`\\)를 분류할 때, 노드의 변수 \\(`f_v`\\)를 이웃노드 집합 \\(`N_v`\\)의 레이블 \\(`z_u`\\)와 함께 사용하자. 입력은 그래프를 사용하며, \\(f_v\\)는 노드 \\(v\\)의 변수 벡터를 의미하며, 여기서 일부 노드는 레이블 \\(Y_v\\)를 갖는다. 목표는 레이블이 없는 노드에 대해 레이블을 예측하는 것이다. 이를 위해 활용하는 것은 두 개의 분류기를 활용한다. 1. \\(\\phi_{1}\\left(f_{v}\\right)\\) : 노드 \\(v\\)의 변수 \\(f_v\\) 만을 이용해 레이블을 예측하는 모델 2. \\(\\phi_{2}\\left(f_{v}, z_{v}\\right)\\): 노드 \\(v\\)의 변수 \\(*f_v*\\)와 이웃 노드의 레이블에 대한 기술 통계벡터 \\(*z_v*\\)를 이용하여 레이블을 예측하는 모델 \\(z_u\\): 이웃 노드의 라벨을 표현하는 벡터 (라벨의 비율, 개수 등으로 표현된다.)\n\n\n\nComputing the Summary \\(z_v\\)\nHow do we compute the summary \\(z_v\\) of labels of \\(v's\\) neighbors \\(N_v\\)?\n\n\\(z_v\\) = vector that captures labels around node \\(v\\)\n\nHistogram of the number (or fraction) of each label in \\(N_v\\)\nMost common label in \\(N_v\\)\nNumber of different labels in \\(**N_v**\\)\n\n\n\\(v\\)의 인접 \\(N_v\\) 레이블의 요약 \\(z_v\\)는 어떻게 계산합니까?\n\n\\(z_v\\) = 노드 \\(v\\) 주변의 레이블을 캡처하는 벡터\n\n\\(N_v\\) 단위의 각 레이블의 숫자(또는 부분) 히스토그램\n\\(N_v\\)의 가장 일반적인 레이블\n\\(N_v\\)의 서로 다른 레이블 수\n\n\n\n\n\n\n💬 위와 같은 그래프에서 청색 노드에 대한 \\(z_{v}\\) 는 이웃 노드의 색의 count 분포나 존재 유무 분포, 비율 분포 등을 사용해 만들 수 있다. - count 분포 : [녹색 노드의 수, 적색 노드의 수] = \\([2,1]\\) - 존재 유무 분포 : [녹색 노드 존재 여부, 적색 노드 존재 여부]= \\([1,1]\\) - 비율 분포 : [녹색 노드 비율, 적색 노드 비율] =\\(\\left[\\frac{2}{3}, \\frac{1}{3}\\right]\\) 두 분류기를 이용하여 학습과 예측 과정이 조금 복잡한데 크게 두 단계로 나눌 수 있다.\n\n\n\nArchitecture of Iterative Classifiers\n\nPhase 1: Classify based on node attributes alone\n\nOn the labeled training set, train two classifiers:\n\nBase classifier: \\(\\phi_{1}\\left(f_{v}\\right)\\) to predict \\(Y_{v}\\) based on \\(f_{v}\\)\nRelational classifier: \\(\\phi_{2}\\left(f_{v}, z_{v}\\right)\\) to predict \\(Y_{v}\\) based on \\(f_{v}\\) and summary \\(z_{v}\\) of labels of \\(v^{\\prime}\\)s neighbors\n\n\nPhase 2: Iterate till convergence\n\nOn test set, set labels \\(Y_{v}\\) based on the classifier \\(\\phi_{1}\\), compute \\(z_{v}\\) and predict the labels with \\(\\phi_{2}\\)\nRepeat for each node \\(v\\) :\n\nUpdate \\(z_{v}\\) based on \\(Y_{u}\\) for all \\(u \\in N_{v}\\)\nUpdate \\(Y_{v}\\) based on the new \\(z_{v}\\left(\\phi_{2}\\right)\\)\n\nIterate until class labels stabilize or max number of iterations is reached\nNote: Convergence is not guaranteed\n\n1단계: 노드 속성만을 기준으로 분류\n\n라벨이 부착된 교육 세트에서 두 가지 분류기를 교육합니다.\n\n기본 분류자: \\(f_{v}\\)를 기반으로 \\(Y_{v}\\)를 예측하기 위한 \\(\\phi_{1}\\left(f_{v}\\right)\\)\n관계 분류자: \\(f_{v}\\)와 \\(v^{\\prime}\\)s 인접 레이블의 요약 \\(z_{v}\\)를 기반으로 \\(Y_{v}\\)를 예측하기 위한 \\(\\phi_{2}\\left(f_{v}, z_{v}\\right)\\)\n\n\n2단계: 수렴될 때까지 반복\n\n테스트 세트에서 \\(\\phi_{1}\\) 분류기를 기반으로 레이블 \\(Y_{v}\\)을 설정하고 \\(z_{v}\\)를 계산하고 \\(\\phi_{2}\\)로 레이블을 예측한다.\n각 노드 \\(v\\)에 대해 반복:\n\n모든 \\(u \\in N_{v}\\)에 대해 \\(Y_{u}\\)를 기준으로 \\(z_{v}\\) 업데이트\n새 \\(z_{v}\\left(\\phi_{2}\\right)\\)를 기준으로 \\(Y_{v}\\) 업데이트\n\n클래스 레이블이 안정되거나 최대 반복 횟수에 도달할 때까지 반복\n참고: 수렴이 보장되지 않음\n\n\n\n💬 Phase 1 : Train 학습 데이터의 경우에 모든 노드에 레이블이 달려있다고 간주하고 두 분류기를 학습한다. 1. \\(\\phi_{1}\\left(f_{v}\\right): f_{v}\\) 를 이용해 \\(Y_{v}\\) 를 예측한다. : 노드 피쳐 정보들만을 이용하여 노드라벨을 예측하는 모델\n\n\n\\(\\phi_{2}\\left(f_{v}, z_{v}\\right): f_{v}\\) 와 \\(z_{v}\\) 를 이용해 \\(Y_{v}\\) 를 예측한다. 이때, \\(z_{v}\\) 는 실제 레이블을 이용해 구성한다.\n\n노드 피쳐 정보 + 이웃들의 라벨정보를 이용하여 노드 라벨을 예측하는 모델\n\n\n\nPhase 2 : Inference 테스트 데이터의 경우엔 일부 노드에만 레이블이 달려있다고 간주한다. 혹은 레이블이 아예 없을 수도 있다고 간주한다. 이때 \\(\\phi_{1}\\left(f_{v}\\right)\\) 는 \\(f_{v}\\) 가 변하지 않기 때문에 초기에 한번만 계산하여 라벨 \\(Y_{v}\\) 를 예측한다. 이를 통해 모든 노드에 \\(Y_{v}\\) 가 할당된다. 모든 노드에 \\(Y_{v}\\) 가 할당된 후 다음과 같은 과정을 수렴하거나 최대반복횟수에 도달할 때까지 반복한다. 1. 새로운 \\(Y_{v}\\) 에 맞추어 \\(z_{u}\\) 를 업데이트한다. \\(\\left(u \\in N_{v}\\right)\\) 2. 새로운 \\(z_{u}\\) 에 맞추어 \\(Y_{z}=\\phi_{2}\\left(f_{v}, z_{v}\\right)\\) 를 업데이트한다. 다만, 해당 모델 또한 수렴을 보장하지 않기에, 최대 반복 횟수를 지정한다.\n\n\n\nExample: Web Page Classification\n\nInput: Graph of web pages\nNode: Web page\nEdge: Hyper-link between web pages\n\nDirected edge: a page points to another page\n\nNode features: Webpage description\n\nFor simplicity, we only consider two binary features\n\nTask: Predict the topic of the webpage\nBaseline: Train a classifier (e.g., linear classifier) to classify pages based on node attributes.\n입력: 웹페이지 그래프\n노드: 웹 페이지\n가장자리: 웹 페이지 간의 하이퍼링크 (Directed Edge)\n\n방향 가장자리: 한 페이지가 다른 페이지를 가리키다\n\n노드 기능: 웹 페이지 설명 (TF-IDF 등의 토큰 정보, 여기선 2차원 벡터로 표현)\n\n단순성을 위해 두 개의 이진 기능만 고려한다.\n\n작업: 웹 페이지의 주제 예측\n기준: 노드 속성을 기준으로 페이지를 분류하도록 분류기(예: 선형 분류기)를 훈련합니다.\n\n\n\n\n\n💬 Web Page의 주제를 예측하여 분류하기 위해서 아래의 데이터가 사용된다.\n\nInput : 웹페이지 그래프\nNode : 웹페이지\nEdge : 웹페이지 간 하이퍼링크(Directed Edge)\nNode Features : 웹페이지 정보(TF-IDF 등의 토큰 정보, 여기선 2차원 벡터로 표현)\nTask : 각 웹페이지의 주제 예측\n\n\n\nEach node maintains vectors \\(z_v\\) of neighborhood labels:\n\n\\(I\\) = Incoming neighbor label information vector.\n\\(O\\) = Outgoing neighbor label information vector.\n\n\\(I_0\\) = 1 if at least one of the incoming pages is labelled 0.\nSimilar definitions for \\(I_0\\), \\(O_0\\), and \\(O_1\\)\n\n\n각 노드는 이웃 레이블의 벡터 \\(z_v\\)를 유지한다.\n\n\\(I\\) = 들어오는 인접 레이블 정보 벡터.\n\\(O\\) = 나가는 인접 레이블 정보 벡터.\n\n\\(I_0\\) = 1 (수신 페이지 중 하나 이상이 0으로 표시된 경우)\n\\(I_0\\), \\(O_0\\) 및 \\(O_1\\)에 대한 유사한 정의\n\n\n\n\n\n\n\n💬 \\(f_{v}\\) : 변수 벡터 (TF_IDF등) \\(I\\) : incoming neighbor 레이블에 대한 기술 통계치 벡터([0인 이웃노드 유무, 1인 이웃노드 유무 \\(O\\) : outgoing neighbor 레이블에 대한 기술 통계치 벡터([0인 이웃노드 유무, 1인 이웃노드 유무 \\(*z_v*\\)를 여러 방법들중 들어오고 나오는 엣지들의 라벨개수로 설정하기로 한다. 따라서 회색 노드에서는 1번인 초록 노드에서만 엣지가 들어오고 0번 클래스인 노드에서 들어오는것이 없으므로 \\(I=[0,1]\\)이 된다. 또한 0번, 1번 클래스 노드로 모두 나가므로 \\(O=[1,1]\\)으로 구성한다.\n\n\n\nIterative Classifier - Step 1\n\nOn training labels, train two classifiers:\n\nNode attribute vector only: \\(\\phi_1(f_v)\\)\nNode attribute and link vectors \\(z_v\\): \\(\\phi_2(f_v,z_v)\\)\n\n\n\nTrain classifiers\nApply classifier to unlab. set\nIterate\nUpdate relational features \\(z_v\\)\nUpdate label \\(Y_v\\)\n\n\n교육 라벨에서 두 가지 분류기를 교육합니다.\n\n노드 속성 벡터만: \\(\\phi_1(f_v)\\)\n노드 속성 및 링크 벡터 \\(z_v\\): \\(\\phi_2(f_v,z_v)\\)\n\n\n\n분류기 학습\n레이블 해제 세트에 분류자 적용\n반복\n관계 기능 업데이트 \\(z_v\\)\n레이블 업데이트 \\(Y_v\\)\n\n\n\n\n\n💬 \\(\\phi_1(f_v)\\)에서는 피쳐 정보(\\(f_v\\))만을 사용하며, \\(\\phi_2(f_v,z_v)\\)는 피쳐 정보(\\(f_v\\))와 이웃라벨 정보(\\(I,O)\\)를 이용하여 학습한다.\n\n\n\nIterative Classifier - Step 2\n\nOn the unlabeled set:\n\nUse trained node feature ****vector classifier \\(**\\phi_1**\\) to set \\(**Y_v**\\)\n\n\n\nTrain classifiers\nApply classifier to unlab. set\nIterate\nUpdate relational features \\(z_v\\)\nUpdate label \\(Y_v\\)\n\n\n라벨이 없는 세트에서:\n\n훈련된 노드 특징 벡터 분류기 \\(\\phi_1\\)를 사용하여 \\(Y_v\\) 설정\n\n\n\n분류기 학습\n레이블 해제 세트에 분류자 적용\n반복\n관계 기능 업데이트 \\(z_v\\)\n레이블 업데이트 \\(Y_v\\)\n\n\n\n\n\n💬 \\(\\phi_1\\)이 부여한 라벨을 이용해 \\(z_v\\)를 업데이트한다.\n\n\n\nIterative Classifier - Step 3.1\n\nUpdate \\(z_v\\) for all nodes:\n\n\nTrain classifiers\nApply classifier to unlab. set\nIterate\nUpdate relational features \\(z_v\\)\nUpdate label \\(Y_v\\)\n\n\n모든 노드에 대해 \\(z_v\\) 업데이트:\n\n\n분류기 학습\n레이블 해제 세트에 분류자 적용\n반복\n관계 기능 업데이트 \\(z_v\\)\n레이블 업데이트 \\(Y_v\\)\n\n\n\n\n\n💬 \\(\\phi_2\\)가 업데이트 된 \\(z_v\\)와 \\(I\\), \\(O\\) 정보를 사용하여 라벨을 예측한다.\n\n\n\nIterative Classifier - Step 3.2\n\n****Re-classify all nodes with \\(\\phi_2\\):**\n\n\nTrain classifiers\nApply classifier to unlab. set\nIterate\nUpdate relational features \\(z_v\\)\nUpdate label \\(Y_v\\)\n\n\n\\(\\phi_2\\)로 모든 노드를 다시 분류합니다.\n\n\n분류기 학습\n레이블 해제 세트에 분류자 적용\n반복\n관계 기능 업데이트 \\(z_v\\)\n레이블 업데이트 \\(Y_v\\)\n\n\n\n\n\n💬 \\(\\phi_2\\)에 의해 라벨이 변화했으므로, 다시 \\(z_v\\)를 업데이트한다.\n\n\n\nIterative Classifier - Iterate\n\nContinue until convergence\n\nUpdate \\(z_v\\) based on \\(Y_v\\)\nUpdate \\(Y_v\\) = \\(\\phi_2(f_v,z_v)\\)\n\n\n\nTrain classifiers\nApply classifier to unlab. set\nIterate\nUpdate relational features \\(z_v\\)\nUpdate label \\(Y_v\\)\n\n\n정합될 때까지 계속합니다\n\n\\(Y_v\\)를 기준으로 \\(z_v\\) 업데이트\n업데이트 \\(Y_v\\)= \\(\\phi_2(f_v,z_v)\\)\n\n\n\n분류기 학습\n레이블 해제 세트에 분류자 적용\n반복\n관계 기능 업데이트 \\(z_v\\)\n레이블 업데이트 \\(Y_v\\)\n\n\n\n\n\n💬 \\(z_v\\)가 업데이트 되었으므로 \\(\\phi_2\\)가 레이블을 예측한다.\n\n\n\nIterative Classifier - Final Prediction\n\nStop iteration\n\nAfter convergence or when maximum iterations are reached\n\n반복 중지\n\n수렴 후 또는 최대 반복 횟수에 도달한 경우\n\n\n\n\n\n\n💬 \\(\\phi_2\\)를 통한 예측과 \\(z_v\\)에 대한 업데이트를 종료 고전에 도달할 때까지 반복한다.\n\n\n\nSummary\n\nWe talked about 2 approaches to collective classification\nRelational classification\n\nIteratively update probabilities of node belonging to a label class based on its neighbors\n\nIterative classification\n\nImprove over collective classification to handle attribute/feature information\nClassify node 𝑣 based on its features as well as labels of neighbors\n\n집단 분류에 대한 2가지 접근법에 대해 이야기했습니다\n관계구분\n\n인접 관계에 따라 레이블 클래스에 속하는 노드의 확률을 반복적으로 업데이트합니다.\n\n반복구분\n\n특성/특징 정보를 처리하기 위해 집합 분류보다 개선\n특징과 이웃의 label을 기준으로 노드 based 분류\n\n\nCS224W: Machine Learning with Graphs 2021 Lecture 5.2 - Relational and Iterative Classification"
  },
  {
    "objectID": "posts/lec05.html#collective-classification-belief-propagation-1",
    "href": "posts/lec05.html#collective-classification-belief-propagation-1",
    "title": "Lecture 5",
    "section": "Collective Classification : Belief Propagation",
    "text": "Collective Classification : Belief Propagation\n\nCollective Classification Models\n\nRelational classifiers\nIterative classification\nLoopy belief propagation\n관계 분류자\n반복구분\n루피 신앙 전파\n\n\n💬 먼저 지금까지 배운 내용으로는 각 노드는 이웃 노드의 확률의 가중평균을 자신의 새로운 확률로 삼고 있다. 혹은 각 노드는 이웃 노드의 레이블을 활용해 자신의 새로운 확률을 계산하게 된다. 즉, 이웃노드의 정보가 각각의 노드에서 사용되고 있는 것이다. 이것을 각 노드는 이웃노드에게 Belief를 전달받는다고 할 수 있다. 즉 이웃 노드의 belief를 받아 자신의 belief를 생성한다. 다르게 말하면, 모델은 각 노드에 대해 데이터마다 belief를 가지고 있고, 이웃 노드의 belief를 이용해 각 노드의 belief를 업데이트하고 있다. 그렇다면 왜 굳이 바로 이웃노드에서만 belief를 받아야 할까. 좀 더 먼 노드의 belief도 중요하게 작동하지 않을까? 왜냐하면 결국 이터레이션을 반복하여 이웃노드의 belief를 받게 된다면, 해당 belief는 이웃노드의 이웃노드의 belief가 섞여있는 상태기 때문에, 이터레이션을 반복한다는 것은 자신의 이웃노드의 이웃노드의 이웃노드의 …. belief를 받고 있는 것이기 때문이다. 이를 역으로 생각하여 belief가 그래프에 직접 흐르도록 알고리즘을 구성한 것이 loopy belief propagation이 된다.\n\n\n\nLoopy Belief Propagation\n\nBelief Propagation is a dynamic programming approach to answering probability queries in a graph (e.g. probability of node \\(v\\) belonging to class 1 )\nIterative process in which neighbor nodes “talk” to each other, passing messages\n\n📌 “I (node v) believe you (node u) belong to class 1 with likelihood …”\n\nWhen consensus is reached, calculate final belief\n믿음 전파는 그래프에서 확률 쿼리에 응답하는 동적 프로그래밍 접근법이다(예: 클래스 1에 속하는 노드 \\(v\\)의 확률).\n인접 노드가 메시지를 전달하면서 서로 “대화”하는 반복 프로세스\n\n📌 “나(노드 v)는 (노드 u)가 클래스 1에 속한다고 믿고 있습니다.”\n\n합의가 이루어지면 최종 믿음을 계산합니다.\n\n\n💬 Belief Propagation 확률 쿼리에 응답하기 위한 동적 프로그래밍 접근법이다. 또한 반복적으로 이웃노드와 ’talk’하면서 메시지를 전달하는 방법이다. 따라서 주된 아이디어는 각 노드는 메시지를 이웃으로부터 얻는다. 그리고 업데이트하고 앞으로 전달한다.\n\n\n\nMessage Passing: Basics\n\nTask: Count the number of nodes in a graph*\nCondition: Each node can only interact (pass message) with its neighbors\nExample: path graph\n과제: 그래프의 노드 수 계산*\n조건: 각 노드는 인접 노드와만 상호 작용(메시지 전달)할 수 있습니다.\n예: 경로 그래프\n\n\n\n\nPotential issues when the graph contains cycles.\nWe’ll get back to it later!\n그래프에 주기가 포함되어 있을 때 발생할 수 있는 문제. 나중에 다시 얘기하자!\n\n💬 위와 같이 가장 단순한 그래프의 형태를 생각해보자. 우리가 원하는 것은 그래프의 노드 수를 계산하고자 한다. 이때 belief는 각 이터레이션마다 이웃 노드로만 전달될 수 있다.\n\n\n\nMessage Passing: Algorithm\n\nTask: Count the number of nodes in a graph\nAlgorithm:\n\nDefine an ordering of nodes (that results in a path)\nEdge directions are according to order of nodes\n\nEdge direction defines the order of message passing\n\nFor node \\(i\\) from 1 to 6\n\nCompute the message from node \\(i\\) to \\(i+1\\) (number of nodes counted so far)\nPass the message from node \\(i\\) to \\(i+1\\)\n\n\n과제: 그래프의 노드 수 계산\n알고리즘:\n\n노드 순서 정의(경로 생성)\n에지 방향은 노드 순서에 따릅니다.\n\n에지 방향은 메시지 전달 순서를 정의합니다.\n\n노드 \\(i\\)의 경우 1 ~ 6까지\n\n노드 \\(i\\)에서 \\(i+1\\)로 메시지 계산 (지금까지 카운트된 노드 수)\n노드 \\(i\\)에서 \\(i+1\\)로 메시지 전달\n\n\n\n\n\n\n\n💬 알고리즘은 다음과 같을 것이다.\n\n노드의 순서를 정한다.\n1에서 정한 순서에 따라 엣지의 방향을 정한다.\n\\(i\\)번째 노드에 대해 다음을 시행한다. \\(i-1\\) 노드에서 belief(이전까지 지나온 노드의 수)를 받는다. belief에 \\(1\\)(자신에 대한 count)을 더한다. \\(i+1\\) 노드로 belief를 전달한다.\n\n\n\n\nMessage Passing: Basics\nTask: Count the number of nodes in a graph Condition: Each node can only interact (pass message) with its neighbors Solution: Each node listens to the message from its neighbor, updates it, and passes it forward \\(m\\) : the message\n작업: 그래프의 노드 수 계산 조건: 각 노드는 인접 노드와만 상호 작용(메시지 전달)할 수 있습니다. 솔루션: 각 노드는 인접 노드로부터 메시지를 수신하고 업데이트한 후 전달 \\(m\\) : 메시지\n\n\n\n\n💬 그래프의 노드의 수를 세는 방법을 구하기 위해서 위에서 본 알고리즘을 따라 먼저 노드의 순서와 그에 따른 방향을 정해준다. 그리고 난뒤로 이전 노드로 부터 message를 전달받고(listen) 현재노드에서 메시지를 업데이트하고 앞의 노드로 전달해준다.\n\n\n\nGeneralizing to a Tree\n\nWe can perform message passing not only on a path graph, but also on a tree-structured graph\nDefine order of message passing from leaves to root\n경로 그래프뿐만 아니라 트리 구조 그래프에서도 메시지 전달을 수행할 수 있습니다.\n리프에서 루트로 전달되는 메시지 순서 정의\n\n\n\n\n\n💬 같은 알고리즘을 위와 같은 트리 구조에 적용한다. 트리는 parent와 child로 구성되어 있기 때문에, 전체 노드의 수를 세기 위해서 child에서 parent 방향으로 belief가 흐르면 될 것이다.\n\n\n\nMessage passing in a tree\nUpdate beliefs in tree structure\n트리 구조의 신뢰 업데이트\n\n \n\n\n💬 이때 왼쪽 그림과 같이 parent 노드는 자신의 child 노드의 belief를 받아 종합하는 일종의 계산을 수행한 수 자신의 parent 노드로 belief를 넘겨주게 된다. 이를 통해 최종적으로 root 노드에서 전체 노드의 수를 구할 수 있을 것이다. 하지만 실제로 count를 belief로 간주하고 이웃 노드에 전달하지 않을 것이다. 실제로 알고리즘이 어떻게 되어 있는지 살펴보자.\n\n\n\nLoopy BP Algorithm\nWhat message will \\(i\\) send to \\(j\\) ? - It depends on what \\(i\\) hears from its neighbors - Each neighbor passes a message to \\(i\\) its beliefs of the state of \\(i\\)\n\\(i\\)가 \\(j\\)에 보낼 메시지는 무엇입니까? - \\(i\\)가 이웃으로부터 무엇을 듣느냐에 따라 달라진다. - 각 이웃은 \\(i\\) 상태에 대한 믿음을 \\(i\\)에 전달한다.\n\n📌 I (node \\(i\\) ) believe that you (node \\(j\\) ) belong to class \\(Y_{j}\\) with probability \\(\\cdots\\)\n\n\n📌 I(노드 \\(i\\))는 당신(노드 \\(j\\))이 확률로 클래스 \\(Y_{j}\\)에 속한다고 믿는다\\(\\cdots\\)\n\n\n\n\n\n💬 일반적으로는 여러 노드의 정보를 \\(i\\)노드로 전달하고(hear) \\(i\\)노드에서 \\(j\\)노드로 전달한다.\n\n\n\nNotation\n\nLabel-label potential matrix \\(\\psi\\) : Dependency between a node and its neighbor. \\(\\boldsymbol{\\psi}\\left(Y_{i}, Y_{j}\\right)\\) is proportional to the probability of a node \\(j\\) being in class \\(Y_{j}\\) given that it has neighbor \\(i\\) in class \\(Y_{i}\\).\nPrior belief \\(\\phi: \\phi\\left(Y_{i}\\right)\\) is proportional to the probability of node \\(i\\) being in class \\(Y_{i}\\).\n\\(m_{i \\rightarrow j}\\left(Y_{j}\\right)\\) is \\(i^{\\prime}\\) s message / estimate of \\(j\\) being in class \\(Y_{j}\\).\n\\(\\mathcal{L}\\) is the set of all classes/labels\n레이블 레이블 잠재적 매트릭스 \\(\\psi\\) : 노드와 인접 노드 간의 종속성. \\(\\boldsymbol{\\psi}\\left(Y_{i}, Y_{j}\\right)\\)는 노드 \\(j\\)가 클래스 \\(Y_{j}\\)에 있을 확률에 비례한다.\n\\(\\phi: \\phi\\left(Y_{i}\\right)\\)는 노드 \\(i\\)가 클래스 \\(Y_{i}\\)에 포함될 확률에 비례한다.\n\\(m_{i \\rightarrow j}\\left(Y_{j}\\right)\\)는 클래스 \\(Y_{j}\\)에 속하는 \\(j\\)의 \\(i^{\\prime}\\) 메시지/추정이다.\n\\(\\mathcal{L}\\) 는 모든 클래스/라벨의 집합입니다.\n\n\n💬 Notation\n\n\\(\\psi\\) (Label-Label Potential Matrix) : \\(\\psi\\) 는 각 노드가 이웃노드의 클래스에 대한 영향력(비례)을 행렬로 표현한 것이다.\n\n예를 들어 \\(\\psi\\left(Y_{i}, Y_{j}\\right)\\) 는 이웃 노드 i의 레이블이 \\(Y_{i}\\) 일 때, 노드 \\(\\mathrm{j}\\) 가 \\(Y_{j}\\) 레이블에 속할 확률의 비중이다.\n만약 \\(i\\)와 \\(j\\)가 Homophily가 존재한다면(같은 class를 가진다면) 대각원소들의 크기는 높을것이다.\n또한 이 행렬을 얻기위해서는 학습이 필요하다.\n\n\\(\\phi\\) (Prior Belief) : 노드 \\(i\\)가 \\(Y_{i}\\) 에 속할 확률에 비례한다.\n\\(m_{i \\rightarrow j}\\left(Y_{j}\\right)\\) : \\(i\\)의 메세지가 \\(j\\)로 전달되는 것을 의미하는데, \\(i\\)가 이웃 노드로 부터 받은 belief와 자신의 정보를 종합해 \\(j\\)의 레이블을 believe하는 것을 의미한다.\n\n\\(j\\)의 노드를 예측할 수 있도록 \\(i\\)에서 \\(j\\)로 전달하는 메시지이다.\n\n\\(L:\\) 모든 레이블(클래스)을 포함하는 집합\n\\(b_{i}\\left(Y_{i}\\right)\\) : 노드 i의 클래스가 \\(Y_{i}\\) 일 belief\n\n\n\n\nLoopy BP Algorithm\n\nInitialize all messages to 1\nRepeat for each node:\n모든 메시지를 1로 초기화\n각 노드에 대해 반복합니다.\n\n\n\n\n\n\n\nAfter convergence: \\(b_{i}\\left(Y_{i}\\right)=\\) node \\(i\\) ’s belief of being in class \\(Y_{i}\\)\n수렴 후: \\(b_{i}\\left(Y_{i}\\right)=\\) 노드 \\(i\\)의 클래스 \\(Y_{i}\\)에 대한 믿음\n\n\n\n\n\n가장 처음에는 모든 노드의 메세지를 1로 초기화한다.\n이후 가운데 이미지와 같이 모든 노드에 대해 다음 노드로 메세지를 전달하는 과정을 반복한다.\n\n이때 가운데 이미지의 수식을 설명해보자면, 가장 앞의 분홍색 부분은 현재 노드 \\(i\\)의 모든 레이블의 가능성에 대해 반복하여 더한다는 의미이다. 녹색 부분은 label-label potential로서, \\(i\\)노드의 각 레이블마다 \\(j\\)노드가 \\(Y_{j}\\) 레이블을 가질 확률을 계산하게 된다. 적색 부분은 Prior로서 \\(i\\)노드가 \\(Y_{i}\\) 레이블을 가질 확률을 계산하게 된다. 청색 부분은 \\(i\\) 노드가 메세지를 넘겨받는 이웃 노드에서 \\(i\\) 노드가 \\(Y_{i}\\) 레이블일 belief를 넘겨 받는 부분이다. 만약 위의 과정이 충분히 반복되어 수렴한다면 세번째 이미지에 해당하는 실제 확률 \\([b_{i}\\left(Y_{i}\\right)]\\)이 계산되게 된다. 1. 즉, Prior 확률에 belief를 모두 곱하여 최종적인 belief (\\([b_{i}\\left(Y_{i}\\right)]\\)) 를 결정한다. 💡 Q: 수렴 하는 것이 무엇인지? 무엇이 수렴하는 것인지 상황에 대한 질문\n\n\n\nExample: Loopy Belief Propagation\n\nNow we consider a graph with cycles\nThere is no longer an ordering of nodes\nWe apply the same algorithm as in previous slides:\n\nStart from arbitrary nodes\nFollow the edges to update the neighboring nodes\n\n\nWhat if our graph has cycles? Messages from different subgraphs are no longer independent! But we can still run BP, but it will pass messages in loops.\n\n이제 주기가 있는 그래프를 살펴봅시다.\n더 이상 노드 순서가 없습니다.\n이전 슬라이드와 동일한 알고리즘을 적용합니다.\n\n임의 노드에서 시작\n가장자리를 따라 인접 노드를 업데이트합니다.\n\n\n만약 우리 그래프에 주기가 있다면? 다른 하위 그래프의 메시지는 더 이상 독립적이지 않습니다! 하지만 BP는 여전히 실행할 수 있지만 메시지를 루프 형태로 전달합니다.\n\n\n\n\n💬 지금까지 이야기한 그래프들은 순환하는 구조를 가지고 있지 않아 메세지를 전달할 순서를 정하는데 문제가 없었다. 하지만 순환하는 구조를 가지는 그래프의 경우에는 단순하게 노드의 순서를 정해서 메세지를 전달하도록 만들 수 없다 그에 대해 자세히 살펴보자. 만약 위와 같은 그래프가 있고, 위와 같은 순서로 메세지를 주고 받는다고 생각해보자. \\(u\\) 노드는 \\(k\\)에게 메세지를 받는 것처럼 보이지만, 실제로는 자기 자신의 메세지마저 받고 있는 상황이다. 즉, 더 이상 모든 노드가 독립적이지 않고, 의존성이 생긴다. 순서가 반대로 트리와 같이 \\(j\\)가 \\(i, k\\)로 메세지를 전달하고, \\(i, k\\)가 \\(u\\)로 메세지를 전달한다면, \\(j\\)의 메세지는 \\(u\\)에게 중복되어 두 번 전달되는 문제가 생긴다. 이렇게 되면 알고리즘이 크게 문제가 생기는 것 같지만, 실제 적용해보니 그렇지 않다고 한다. 실제 그래프들은 무척 크고, 거기에 순환하는 cycle 구조는 그렇게 큰 부분을 차지하지 않는데 반면, 전체 구조는 매우 복잡하기 때문에 Loopy BP 알고리즘이 잘 작동한다고 한다.\n\n\n\nWhat Can Go Wrong?\n\nBeliefs may not converge\nMessage \\(m_{u \\rightarrow i}\\left(Y_{i}\\right)\\) is based on initial belief of \\(i\\), not a separate evidence for \\(i\\)\nThe initial belief of \\(i\\) (which could be incorrect) is reinforced by the cycle\n\n\\[\ni \\rightarrow j \\rightarrow k \\rightarrow u \\rightarrow i\n\\]\n\nHowever, in practice, Loopy BP is still a good heuristic for complex graphs which contain many branches.\n신념이 수렴되지 않을 수 있다.\n메시지 \\(m_{u \\rightarrow i}\\left(Y_{i}\\right)\\)는 \\(i\\)에 대한 별도의 증거가 아니라 \\(i\\)의 초기 믿음에 기초한다.\n(잘못될 수 있음) \\(i\\)의 초기 신념은 주기에 의해 강화된다.\n\n\\[\ni \\rightarrow j \\rightarrow k \\rightarrow u \\rightarrow i\n\\]\n\n그러나 실제로 Loopy BP는 많은 분기를 포함하는 복잡한 그래프에 여전히 좋은 휴리스틱이다.\n\n\n\n\n\nMessages loop around and around: \\(2,4,8,16,32, \\ldots\\) More and more convinced that these variables are \\(T\\) !\nBP incorrectly treats this message as separate evidence that the variable is \\(\\mathrm{T}\\)!.\nMultiplies these two messages as if they were independent.\n\nBut they don’t actually come from independent parts of the graph.\nOne influenced the other (via a cycle).\n\n메시지는 돌고 돈다: \\(2,4,8,16,32,\\ldots\\) 이러한 변수가 \\(T\\)임을 점점 더 확신하게 된다!\nBP는 이 메시지를 변수가 \\(\\mathrm{T}\\)라는 별도의 증거로 잘못 취급한다.\n이 두 메시지를 독립한 것처럼 곱합니다.\n\n하지만 그것들은 사실 그래프의 독립적인 부분에서 나온 것이 아닙니다.\n한 사람이 다른 사람에게 영향을 주었다.\n\n\n\n\n\nThis is an extreme example. Often in practice, the cyclic influences are weak. (As cycles are long or include at least one weak correlation.)\n이것은 극단적인 예입니다. 실제로, 주기적인 영향은 약하다. (주기가 길거나 하나 이상의 약한 상관 관계를 포함하기 때문에)\n\n\nAdvantages of Belief Propagation\n\nAdvantages:\n\nEasy to program & parallelize\nGeneral: can apply to any graph model with any form of potentials\n\nPotential can be higher order: e.g. \\(\\boldsymbol{\\psi}\\left(Y_{i}, Y_{j}, Y_{k}, Y_{v} \\ldots\\right)\\)\n\n\nChallenges:\n\nConvergence is not guaranteed (when to stop), especially if many closed loops\n\nPotential functions (parameters)\n\nRequire training to estimate\n\n장점:\n\n프로그래밍 및 병렬화가 용이함\n일반: 모든 형태의 잠재력이 있는 그래프 모델에 적용할 수 있습니다.\n\n잠재력은 고차일 수 있다. 예를 들어 \\(\\boldsymbol{\\psi}\\left(Y_{i}, Y_{j}, Y_{k}, Y_{v} \\ldots\\right)\\)\n\n\n과제:\n\n특히 닫힌 루프가 많은 경우 수렴이 보장되지 않습니다(정지 시기).\n\n잠재적 함수(모수)\n\n평가하려면 교육 필요\n\n\n\n💬 Advantages:\n\n코딩이 쉽고, 병렬화가 가능하다.\n어떠한 그래프 모델이더라도, potential matrix를 구성할 수 있으므로 범용적이다.\n\nChallenges:\n\n수렴이 보장되지 않아 언제 멈춰야 할지 알 수 없으며, 특히 순환구조일 경우 수렴이 보장되지 않아 반복횟수를 지정해주어야 한다.\ncycle 구조로 인해 종종 적은 이터만 돌리기도 한다.\n\n\n\n\nSummary\n\nWe learned how to leverage correlation in graphs to make prediction on nodes\nKey techniques:\n\nRelational classification\nIterative classification\nLoopy belief propagation\n\n우리는 그래프의 상관 관계를 활용하여 노드에 대한 예측을 하는 방법을 배웠다.\n주요 기술:\n\n관계구분\n반복구분\n루피 신앙 전파\n\n\nCS224W: Machine Learning with Graphs 2021 Lecture 5.3 - Collective Classification"
  },
  {
    "objectID": "posts/lec01.html",
    "href": "posts/lec01.html",
    "title": "Lecture 1",
    "section": "",
    "text": "1.1 - Why Graphs\n그래프란?\n⇒ a general language for describing and analyzing entities with relations/interactions\n⇒ 서로 관계/상호작용하는 entity들을 설명하고 분석하기 위한 언어라고 할 수 있음 (여기서 entity란 정보의 세계에서 의미있는 하나의 정보 단위)\n\n그래프 예시\n\nevent graphs\ncomputer networks\ndisease pathways\nfood webs\nparticle networks\nunderground networks\nsocial networks\neconomic networks\ncommunication networks\ncitation networks\ninternet\nnetworks of neurons\nknowledge graphs\nregulatory networks\nscene graphs\ncode graphs\nmolecules\n3D shapes\n\n\n⇒ 그래프로 세상에 일어나는 모든 현상과 구조들을 설명할 수 있다(broadly applicable).\n(관심있는 분야의 현상을 그래프로 표현하여 딥러닝 모델 구조로 변환할 수 있는 능력이 있다면…)\ne.g) 분자 구조, 3D 이미지 모형(voxel), 먹이사슬, 소셜 네트워크 등\n(graph와 network의 차이?)\n그래프에서 얻을 수 있는 정보 유형\n\n데이터 포인트 간의 구성(organization)과 연결\n유사한 데이터 포인트 간의 밀접성(similarity)\n데이터 포인트 간의 연결들이 이루는 그래프 구조\n\n그래프가 갖는 구조를 어떻게 활용해 나은 예측을 할 수 있을까?\n⇒ 현상을 명시적(explicitly)으로 잘 반영한 그래프 모델링이 중요하다!\n그래프 ML이 더 어려운 이유?\n⇒ arbitrary size and complex topology\n⇒ spatial locality(공간 지역성)가 없다\n⇒ 이미지나 텍스트 인풋의 경우 어느 한 데이터포인트로부터 다른 데이터 포인트 간 상대적 위치가 정해져있다(e.g 상하좌우). 하지만, 그래프의 경우 축이 되는 데이터 포인트가 존재하지 않는다.\n그래프를 사용한 딥러닝\n⇒ 인풋으로는 그래프를 받고, 아웃풋으로는 아래와 같은 형식(ground truth도 동일한 형식)이 가능하다. >\n\nnode-level\nedge-level\ngraph/subgraph generation\ngraph/subgraph classification\n\n그래프 딥러닝 모델에서 우리가 바라는 플로우\n\n\n위 플로우를 거친 좋은 성능의 모델을 만들기 위해서는, 인풋이 현상을 잘 반영한 embedding vector로 변환될 수 있도록 학습하는 것이 중요하다.(Representation Learning)\n\n코스 과정동안 배울 그래프 방법들\n\ntraditional methods : graphlets, graph kernels\nnode embeddings : DeepWalk, Node2Vec\nGraph Neural Networks : GCN, GraphSAGE, GAT, Theory of GNNs\nKnowledge graphs and reasoning : TransE, BetaE\nDeep Generative Models for graphs\nApplications\n\n\n\n\n1.2 - Applications of Graph ML\n그래프 ML은 다양한 태스크 커버가 가능하다\n\nNode Level(node classification)\n\n\nexample1) protein folding (구글의 알파폴드)\n\n배경 : 단백질은 아미노산으로 이루어져있는데, 복잡한 3D 입체 구조의 아미노산 연결 때문에 단백질 구조를 파악하는 태스크는 많게는 1-2년까지 걸린다고 함.\nkey idea : spatial graph\n\ngraph : 단백질\nnodes : 아미노산\nedges : 사슬구조\n\n\n\nEdge Level\n\n\nexample1) Recommender system (PinSage)\n\nnodes : users and items\nedges : user-item interactions\n\n\nexample2) drugs and side effects\n\nnodes : drugs & proteins\nedges : interaction\n\nusing 2 heterogeneous graphs(drugs & proteins)\n\n\n\n스크린샷 2022-07-05 오후 11.15.49.png\n\n\n⇒ drug A와 B를 함께 썼을 때, 생길 수 있는 interaction(edge)는 무엇인가?\n\nCommunity(subgraph) level\n\n\nexample1) Traffic Prediction\n\nnodes : Road Segments(도로 구간)\nedges : 도로 구간 교차점\n\n\n⇒ 아웃풋 : 도착 예정 시간\n\nGraph-level prediction\n\n\n4-1) graph classification\nexample1) drug discovery\n\nnodes : atoms\nedges : chemical bonds\ngraph : molecules\n\n⇒ 노드와 에지 정보를 통해 그래프(분자) 예측\n4-2) Graph-level generation\nexample1) drug generation\n\nnodes : atoms\nedges : chemical bonds\ngraph : moelcules\n\n⇒ 새로운 그래프(분자) 생성\nexample2) physics simulation (graph evolution)\n\nnodes : particles\nedges : interaction between particles\n\n⇒ t시점 전의 정보로 t시점 이후의 그래프 생성\n\n\n\n1.3 - Choice of Graph\n그래프 구성요소\n\nobjects : nodes, vertices\nInteractions : links, edges\nsystem : network, graph\n**object와 interactions로 이루어진 데이터 구조 ⇒ graph\n그래프로 세상에 일어나는 모든 현상과 구조들을 설명할 수 있다(broadly applicable).\n\n\n표현법에 따라 그래프의 활용방법은 무궁무진하다. 중요한 것은 적절한 표현을 선택하는 것.\n설명하고자하는 현상을 그래프로 정의하고 싶다면, 먼저 아래 두가지 질문을 하자.\n\nwhat are nodes?\nwhat are edges?\n\n\nDirected vs Undirected Graphs\n\n’페이스북의 친구와 인스타그램의 팔로우’는 directed와 undirected graph를 이해하기에 좋은 예이다.\nNode Degrees (차수)\n\nundirected graph\n\nundirected graph의 경우, node degree는 해당 노드에 연결된 에지의 총 갯수\naverage degree는 연결된 에지의 총 갯수 곱하기 2(쌍방향 연결이기 때문)를 그래프를 이루는 전체 노드 수로 나눈 값\n\ndirected graph\n\ndirected graph의 경우, 해당 노드로 향하는 in-degree와 해당 노드로부터 뻗어나가는 out-degree로 나눌 수 있다. node degree는 이 in-degree와 out-degree의 합.\n\n\nBipartite Graph\n\n\n자주 등장하는 또다른 그래프의 종류는 bipartite graph(이분 그래프)이다.\n이분 그래프는 2개의 집합 U와 V의 interaction을 나타낸 그래프이다. U와 V는 서로 독립적인 집합이며, 같은 집합의 원소끼리는 연관되지 않는다. e.g) A와 B 간 연결X\n이분 그래프의 예로는 구매자-구매 아이템 관계 등이 있다.\n\nFolded/Projected Bipartite Graph\n\nBipartite 그래프에서 집합 간의 요소들 간의 상관관계가 명시되어있다면 Folded 또는 Projected Bipartite Graph라고 부른다.\n\n(정보에 depth가 생긴다는 의미에서 folded라고 붙인듯하다.)\n\nAdjacency Matrix\n\n\n그래프의 노드 간 연결관계(edge)를 나타낸 매트릭스이다.\n각 행과 열은 노드의 번수를 의미하고, 0은 연결되지 않음, 1은 연결됨을 의미한다. 만약 3번째 행에 4번째 열이 1이라면, 3번 노드와 4번 노드는 연결되어있음을 의미한다.\nundirect graph라면, 주대각선을 기준으로 adj matrix는 대칭이고, directed graph라면, 대칭이 아닐 수도 있다.\nadjacency matrix는 컴퓨터가 그래프를 이해할 수 있는 형태이지만, 문제는 노드의 수가 수백개에서 수십만개로 늘어나고, 많은 노드들의 연결이 몇 개 없을 때, 메모리 사이즈에 비해 0인 값이 너무 많게되는 문제(sparse)가 발생한다.\n\nEdge List\n\n\nEdge List는 그래프를 엣지들의 리스트로 나타낸 값이다.\n서로 연결된 노드를 짝지어 리스트에 배열한다.\n그래프가 크고 sparse할 때 유용하다.\n\nNode and Edge Attributes\n⇒ 노드와 엣지로 나타낼 수 있는 값들은 어떤 것들이 있을까?\n⇒ 그래프에서 어떤 정보들을 얻을 수 있을까?\n\nweight (e.g., frequency of communication)\n\n엣지가 0과 1 이외의 값을 갖는다면?\n\n\nranking (best friend, second best friend, …)\ntype (friend, relative, co-worker)\nsign (+ / - )\nproperties depending on the structure of the rest of the graph : Number of common friends\n\nMore Types of Graphs\n\nConnected(undirected) graph\n\n\n**연결이 부분적으로만 되어 있어도, 그래프는 adjacency matrix에 표현 가능하다.\n\nConnectivity of Driected Graphs\n\nStrongly connected directed graph\n\n모든 노드들이 다른 모든 노드들로 방향 상관없이 다다르는 path가 항상 있다면, strongly connected directed graph이다.\n\nWeakly connected directed graph\n\n에지 방향을 무시했을 때, 노드 간 전부 연결되어있다면, weakly connected directed graph이다.\n\nStrongly connected components(SCC)\n\n\n그래프에 속한 다른 노드들 전부는 아니지만, 해당 그룹 간의 연결이 한 노드에서 다른 노드로 항상 도달할 수 있다면(strong connection)한다면, 그 그룹을 strongly connected components라고 지칭한다."
  },
  {
    "objectID": "posts/lec02.html",
    "href": "posts/lec02.html",
    "title": "Lecture 2",
    "section": "",
    "text": "2강에선 Graph를 이용한 Traditional ML 방법론들에 대해 설명한다."
  },
  {
    "objectID": "posts/lec02.html#traditional-feature-based-method-node",
    "href": "posts/lec02.html#traditional-feature-based-method-node",
    "title": "Lecture 2",
    "section": "🔴 Traditional Feature-Based Method : Node",
    "text": "🔴 Traditional Feature-Based Method : Node\n\nGoal : Network에서 Node의 구조와 위치를 특정할 수 있는 Feature를 만드는 것\n\n\n💡 1. Node Degree (\\(k_v\\))\nNode \\(v\\) 의 Degree를 \\(k_v\\) 라고 정의하자. 이때 \\(k_v\\) 는 \\(v\\) 가 갖고있는 Edge(Link)의 수와 같다.\n\n\n\n💡 2. Node Centrality(\\(c_v\\))\nNode Degree는 단순히 이웃한 Node의 갯수를 세므로, 그것들의 중요도를 Capture할 수 없다.\nNode Centrality(\\(c_v\\))는 Graph에서 해당 Node( \\(v\\))의 중요도를 포함시킨 개념이다.\n\n2-1. Engienvector centrality\n\nImportant : \\(v\\)가 Important 이웃노드 \\(u\\)에 둘러싸여 있을 때 \\(v\\)는 Important하다고 한다.\nFormula\n\n\\(c_v = {1 \\over \\lambda} \\sum\\limits_{u\\in N(v)}c_u\\) (\\(\\lambda\\)는 Normalization 상수) ⇒ 이렇게 하면 Recursive함\n\\(\\lambda c= Ac\\) (\\(A\\)는 Adjacency Matrix)\n\n고유값과 고유벡터 형태로 재설정\n\\(c\\)는 \\(A\\)의 고유벡터, \\(\\lambda\\)는 고유값이며 \\(\\lambda_{max}\\)는 항상 양수에 Unique함\n\n\n\n2-2. Betweenness centrality\n\nImportant : \\(v\\)가 다른 노드들을 연결하는 최단 경로에 있을때 Important하다고 한다.(경유)\nFormula : \\(c_v = \\sum\\limits_{s \\neq v \\neq t}{v를\\ 포함하는 s와\\ t사이 \\ 최단\\ 경로 \\over s와\\ t사이\\ 최단\\ 경로}\\)\n\n\n2-3. Closeness Centrality\n\nImportant : \\(v\\)가 다른 모든 노드에 대한 최단 경로의 길이가 짧을때 Important하다고 한다.\nFormula : \\(c_v = 1 \\div \\sum\\limits_{u \\neq v} (u와\\ v사이의\\ 최단경로의\\ 길이)\\)\n\n\n\n\n\n\n💡 3. Clustering Coefficient\nClustering Coefficient는 Node \\(v\\)의 이웃들이 얼마나 연결되어 있는지를 측정하는 개념이다.\n\\(v\\)의 이웃간 연결된 경우의 수를 이웃 Node들이 서로 연결될 수 있는 전체 경우의 수로 나누어준다.\n\n\n\n\n\n🤔 4. Graphlets***\n\nObservation : Clustering Coefficient는 Ego-Network의 #(Triangle)을 센다)\n\nEgo-Network : Node가 주어졌을때 자기자신과 1차-이웃만 포함한 Network\n#(Triangle) : 3개의 노드가 연결되어 있는 것\n이런 Triangle Counting을 다양한 구조에 대해 일반화 하는것 ⇒ Graphlets의 개념\n\n\n\n\nGraphlet의 목적 : Node \\(u\\)의 이웃 구조를 기술하는 것\n\nGraphlets : \\(u\\)의 이웃 구조를 기술하기 위한 작은 Subgraph(Template?)\n\n\n\n\n\nGraphlet Degree Vector(GDV) : Node의 Graphelt-Based Feature\n\nDegree of Graphlet : 특정 Node가 포함된 Graphlet의 갯수 벡터이다. 어떻게 세는지는 아래의 예시를 통해 설명한다.\n\n\n\n\n아래와 같이 생긴 Graph \\(G\\)에서 Node \\(u\\)에 관심있다고 가정해 보자.\n\nGraph 구조를 보았을때, 최대 3개의 Node가 참여하는 Graphlet을 만들 수 있다.\n\n각각의 Graphlet이 \\(G\\)에서 \\(u\\)를 포함한채로 몇번 나타나는지 세보자\n\nNode \\(u\\)의 GDV는 [2,1,0,2]가 된다.\n\n\nGraphlet Summary\n\n2~5개의 Node가 참여하는 Graphlet의 갯수는 73개이다. 이를 73차원의 벡터로 표시할 수 있고, 각 Index는 특정한 Neighborhood Topology에 Signature이다. 이 벡터를 이용해 Node의 Local Network Topology를 잘 정제된 Feature로 만든게 GDV이며, 앞에서 소개한 방식보다 자세한 정보를 갖고있다.\n\n\n💡 Node-Level Feature Summary\nNode Level Feature는 2가지 분류로 나눌수 있다\n\n1. Importance Based (Ex Task : 영향력있는 Node찾기(SNS의 셀럽찾기))\n    1. Node Degree : 단순히 이웃의 숫자를 센다\n    2. Node Centrality : Graph에서의 이웃 노드의 중요도를 모델링한다.\n2. Structure Based (Ex Task : Node의 역할 찾기(단백질 구조에서 특정 단백질의 기능찾기))\n    1. Node Degree : 단순히 이웃의 숫자를 센다\n    2. Clustering Coefficient : 이웃이 어떻게 연결되어있는지 측정한다.\n    3. Graphlet Count Vector : 여러 Graphlet들이 출현하는 빈도를 센다."
  },
  {
    "objectID": "posts/lec02.html#traditional-feature-based-method-link",
    "href": "posts/lec02.html#traditional-feature-based-method-link",
    "title": "Lecture 2",
    "section": "🔗 Traditional Feature-Based Method : Link",
    "text": "🔗 Traditional Feature-Based Method : Link\n\n\n💡 Link-Level Prediction Task\nLink-Level Task는 존재하는 Link를 바탕으로 새로운 Link를 예측하는 것이다. Link를 예측하는 Task는 크게 2가지 Formulation이 있다.\n\n1. 랜덤하게 사라진 Link 찾기 : Static한 Graph에 적절하다.\n\n\n2. 시간이 흐름에 따라 생겨나는 Link 찾기 : SNS, Transaction같이 Dynamic한 Graph에 적절하다.\n\nLink-Level Prediction의 자세한 방법은 다음과 같다.\n\n각 Node쌍 (\\(x,y)\\)에 score \\(c(x,y)\\)를 계산한다.\n\\(c(x,y)\\) 내림차순으로 Node 쌍을 정렬한다\nTop \\(N\\)개의 Pair들을 새로운 Link로 예측한다.\n\n\n\n\n💡 Link - Level Feature : Distance-Based Features\n\n\n1. 노드간 최단 경로\n\n두 Node간 최단경로의 거리를 사용한다. 이웃의 수나 강도에 대한 어떠한 정보도 캡쳐하지 않는다.\n\n2. Local Neighborhood Overlap\n\n두 Node가 공유하는 이웃을 캡쳐한다.\n\nCoomon Neighbors : 단순히 교집합을 구한다\nJaccard’s Coefficient : 교집합의 크기를 합집합으로 나눈다\nAdamic-Adar Index : (SNS에서 잘 동작한다고 하네요) 두 Node가 공유하는 이웃을 u라고 할 때 \\(\\sum \\limits_u {1\\over log(k_u)}\\)\n\n\n3. Global Neighborhood Overlap\n\nLocal Neighborhood Overlap의 단점은 잠재적 이웃도 직접적인 공통 이웃이 없으면 0이 된다는 점이다.\n\n\nKatz Index : 주어진 Node 쌍을 잇는 모든 길이의 경로를 센다. Matrix를 이용해 깔끔하게 연산할수 있다.\n\n\\(A_{uv}\\)는 직접 이웃일 때 1이고 아니면 0이다.\n\\(P_{uv}^{(K)}\\)는 \\(K\\)길이의 \\(u,v\\)를 잇는 경로이다.\n\\(P^{(K)}\\) = \\(A^k\\)이다.\nFormula\n\n\\(S_{uv} = \\sum \\limits_{l=1}^\\infty  \\beta^l A_{uv}^{l}\\) (\\(l\\) : Path의 길이, \\(\\beta\\): Discount Factor(\\(0<\\beta<1\\))\n\\(S_{uv} = \\sum \\limits_{l=1}^\\infty \\beta^lA^i=(I-\\beta A)^{-1}-I\\) (Closed-Form)\n\n\n\n\n\n💡Link-Level Feature Summary\nLink Level Feature는 3가지 분류로 나눌수 있다\n\n1. Distance-Based :두 Node간 최단경로의 거리\n2. Local Neigborhood Overlap : 두 Node가 공유하는 이웃의 수\n3. Global Neighborhood Overlap : 두 Node를 잇는 모든 길이의 경로 가중합"
  },
  {
    "objectID": "posts/lec02.html#traditional-feature-based-method-graph",
    "href": "posts/lec02.html#traditional-feature-based-method-graph",
    "title": "Lecture 2",
    "section": "⛓ Traditional Feature-Based Method : Graph",
    "text": "⛓ Traditional Feature-Based Method : Graph\n\n\nGoal : 전체 Graph 구조를 특정할 수 있는 Feature를 만드는 것\n\n아이디어 : Graph로 Feature를 직접 만드는 대신 Kernel을 만들자.\nKernel \\(K(G,G') \\in \\R\\) 은 두 Graph\\((G)\\) 사이의 유사도를 측정한다.\nKernel Matrix는 항상 양의 고유값을 갖고 대칭행렬 이어야한다.\nFeature Representaiton \\(\\phi(.)\\)이 존재한다.\n이 Kernel을 SVM등에 붙여서 사용한다.\n\n\n\n\n💡 Kernel Method\n\nGoal : Graph Feature Vector \\(\\phi(G)\\)를 설계한다\nIdea : Graph에 대해 Bow를 만든다.\n\nBow : NLP에서 모든 단어가 몇 번 나타나는지 세는 방법\nNaive Solution : Node를 Word로 사용한다. 그러나 너무 Naive해서 써먹기 어렵다.\n\nNode Degrees : Node Degree를 Word로 사용한다.\n\n이런식의 Bag-of-something 방식이 Graphlet Kernel과 WL Kernel에서도 사용된다.\n\n\n\n\n\n💡 Grahplet Features\n\nIdea : Graph에 존재하는 서로 다른 Graphlet의 숫자를 세자\nNote : 이때의 Graphlet은 Node-Level과 조금 다른 정의를 갖고있다.\n\nIsolated Node로 Graphlet의 일부로 허용한다.\nRoot Node가 없다.\n\n\nGraph \\(G\\)와 Graphlet list \\(g_k = (g_1,g_2 ...g_{nk})\\)가 주어졌을 때 Graphlet Count Vector \\(f_G \\in \\R^{nk}\\) 는 Graph에서 나타나는 각 Graphlet의 인스턴스 수로 정의된다.\n\n\\((f_G)_i = \\#(g_i \\in G)\\) | (for \\(i = 1,2,...n_k)\\)\n\n\n\n\n\n💡 Graphlet Kernel\n\n2개의 Graph \\(G\\) 와 \\(G'\\)가 주어지면, Graphlet Kernel은 \\(K(G,G') = {f_G}^Tf_{G'}\\)로 표현될 수 있다(내적)\nProblem : \\(G\\) 와 \\(G'\\)가 크기(Scale)이 다르면 값이 크게 왜곡된다.\nSolution: \\(f_G\\) 대신 Sum으로 나눠준 \\(h_G\\)를 사용한다. \\(h_G = {f_G \\over Sum(f_G)}\\)\nLimitation : Graphlet을 세는 연산이 매우 Expensive하다 !\n\n\\(n\\) 크기의 Graph의 \\(k\\) 크기의 Graphlet를 세려면 \\(n^k\\)번 연산해야 한다.\n\n\n\n\n\n💡 Weisfeiler-Lehman(WL) Kernel\n\nGoal : 효율적인 Graph Feature Descriptor를 만드는 것\n\nWL-Kernel은 강력하고 효율적이어서 인기가 많다.\n\nIdea : Node Degree를 이용해 반복적으로 Node Vocap을 풍부하게 만들어 나가는 것\n\nOne-Hop Neighborhood인 Node Degree 방식을 일반화 한 버전이다.\nColor Refinement 알고리즘을 통해 이루어진다.\n\n각 Step에서의 Time-Complexity가 Edge에 따라 Linear하게 증가한다.\n\n\n\n💡 Color Refinement\n\nGiven : Graph \\(G\\)와 그것들의 Set of Nodes \\(V\\)\n\nInitial Color \\(c^{(0)}(v)\\)를 각 노드 \\(v\\)에 할당한다\nIteratively하게 Node의 Color를 정제해 나간다. \\(c^{(k+1)}(v) = HASH(\\{c^{(k)}(v), \\{c^{(k)}(u) \\}_{u\\in N(v)})\\)\n\nHASH는 다른 입력을 다른 Color로 매핑하는 연산이다.\n\n\\(K\\) Step 동안의 정제가 끝나면 \\(c^{(K)}(v)\\) 값을 Summary한다.\n\n\n\n비슷하지만 조금 다른 Graph 두개 (\\(G_1, G_2\\))가 주어졌을때 Color Refienment 예시이다\n\n동일한 Initial Color를 모든 Node에 할당한다.\n\n이웃하는 색상에 대해 Aggregate한다.\n\nAggregate된 Color를 HASH한다.\n\n이웃하는 색상에 대해 Aggregate한다.\n\nAggregate된 Color를 HASH한다.\n\nColor Refinement가 끝나면 WL Kernel이 각 Color가 등장했던 횟수를 세서 Summary한다.\n\nColor Count Vector를 내적해 WL Kernel의 결과값을 구한다.\n\n\n\n\n\n\n\n💡 Graph-Level Feature Summary\nGraph Level Feature는 Kernel을 이용한다.\n\n1. Graphlet Kernel :Bag-of-Graphlets, Computationally Expensive\n2. WL- Kernel :\n    - Color-Refinement 알고리즘을 이용해 반복적으로 피팅\n    - Bag-of-Colors\n    - Computationally Efficient !\n    - Closely related to Graph Neural Networks"
  },
  {
    "objectID": "posts/lec06.html",
    "href": "posts/lec06.html",
    "title": "Lecture 6",
    "section": "",
    "text": "이전 강의에서 배운 내용을 다시 떠올려 봅시다. 다양한 downstream task를 머신러닝으로 푸는 과정에서 비정형 데이터인 그래프 인풋을 활용하기 위해 그래프를 임베딩하는 방법을 공부했었습니다.\n\nIntuition 그래프 상에서 similar한 노드끼리 임베딩 공간에서도 가깝도록 임베딩 하자!\n\n\n임베딩 공간에서의 노드 간 similarity는 간단하게 코사인 유사도를 통해 구할 수 있지만, 1) 원래 그래프 상에서의 노드 간 similarity와 2) 노드를 임베딩 벡터로 mapping하는 encoder은 우리가 새로 정의해야 합니다.\n\nEncoder: Shallow Encoder\n\n먼저, 인풋 그래프의 노드를 d차원의 벡터로 임베딩하기 위해 가장 간단한 look-up table 방식인 Shallow Encoder를 다뤘습니다. 예를 들어, 노드의 갯수가 \\(\\|V\\|\\)인 경우 임베딩 행렬의 크기는 \\(\\|V\\|\\times d\\) 가 됩니다.\nSimilarity Function: Random Walk (DeepWalk, Node2vec) 또한, Random Walk상에서 co-occur 되는 두 노드는 그래프 상 similar한 노드라고 정의하였습니다. Random Walk의 전략에 따라 서로 다른 DeepWalk와 Node2vec 등의 방법을 배웠던 것 기억 나시나요?\n\n\n\n\n이렇듯 Shallow Encoder을 통해서도 성공적으로 노드와 그래프를 임베딩할 수 있지만, 다음과 같은 한계점 때문에 보다 더 고도화된 Encoder를 재정의 할 필요가 있습니다.\n\n\\(O(\\|V\\|)\\) 파라미터가 필요함 그래프의 크기가 커지면 커질수록, 즉 노드의 갯수 \\(\\|V\\|\\)가 증가함에 따라, 임베딩 행렬의 크기도 선형적으로 증가합니다. 또한 각 노드가 모두 서로 다른 d 차원의 임베딩 벡터를 가지기 때문에 파라미터 공유도 일어나지 않습니다.\nTransductivity\n\nTransductive Learning 그래프 학습 관점에서 Transductive Learning이란 하나의 그래프 상 일부 노드와 엣지의 ground truth를 아는 상태에서 나머지 노드와 엣지의 값을 추정하는 방식입니다. 학습 과정 중, 모델은 ground truth를 알지 못하는 노드를 포함한 모든 노드와 엣지를 사용합니다. \nInductive Learning 그래프 학습 관점에서 Inductive Learning이란 ground truth를 알고 있는 그래프(들)에 대해 모델을 학습 한 후, 전혀 새로운 그래프의 노드와 엣지의 값을 추정하는 방식입니다. 즉, 학습이 완료된 후에는 모델이 새로운 처음 보는 노드의 값을 추정하는 데에도 적용될 수 있다는 의미이죠. \n\nShallow Encoder은 Transductive Learning으로 학습해야 하는 대표적인 케이스입니다. 학습 도중 보지 못한 노드는 look-up table상 존재할 리 없으니 맵핑되는 임베딩 벡터가 없을 것이고, 임베딩 벡터가 없다면 node classification 등의 downstream task에서 예측이 불가능하겠죠? 이런 특성 때문에 시간에 따라 노드가 추가될 수 있는 evolving 그래프와 같은 경우 그래프가 변할 때마다 전체 임베딩을 다시 scratch부터 학습해야 한다는 불편함이 있습니다.\n노드 feature을 활용하지 않음 대부분의 그래프 데이터셋은 우리가 활용할 수 있는 노드 feature이 존재합니다. 예를 들어, 소셜 그래프의 경우, 단순히 철수가 영희가 친구라는 정보 이외에도, 철수는 성균관대학교에 다니는 23세 남학생이라는 정보도 존재합니다. 단순한 노드 간 연결 상태 이외에도 이러한 노드 feature를 고려하여 노드를 임베딩 한다면 정보량이 더 풍부해져 효과적일 것입니다.\n\n\n\n\n이제 지금껏 다뤄왔던 간단한 look-up table로 이루어진 encoder에서 벗어나, 좀 더 복잡한 형태의 Deep Encoder을 공부해봅시다.\n\nDeep Encoder은 인풋 그래프에 수차례의 비선형적인 transformation을 가하여 end-to-end으로 최종 임베딩을 얻는 방식을 말합니다. 수업 슬라이드에 쓰인 말 그대로,\n\nDeep Encoder = multiple layers of non-linear transformations based on graph structure\n\n로 생각할 수 있겠습니다. 잘 와닿지 않으신다고요? 사실, 인풋 데이터가 우리가 익숙치 않은 형태의 그래프라 그렇지, 오늘날의 머신러닝/딥러닝 모델이 이미지나 텍스트와 같은 정형 데이터를 처리하는 방식과 유사합니다.\n\n위의 두 그림이 유사하다는 점이 한눈에 보이실 겁니다. 이해를 돕기 위해 가장 기본적인 CNN 구조를 생각해 볼까요? 원본 이미지가 여러 convolution layer을 거치며 더욱 더 축약된 feature map을 만드는 방식과 유사하게, 인풋으로 들어온 그래프는 여러 graph convolution layer을 거치며 원본 그래프의 의미를 적절히 축약하는 노드 임베딩을 만드는 것입니다.\n또한, 개 고양이 이미지 분류 모델이 지도 학습으로 학습될 때 학습 데이터에 대해 각 이미지가 개인지, 고양이인지 나타내는 클래스 label을 활용하는 것과 같이, 노드 분류 문제의 경우 각 노드에 대한 클래스 label이 있다면 이를 직접 활용하여 encoder를 학습할 수 있습니다.\n\n💡 이 경우 decoder은 노드 클래스 label 입니다.\n\n물론, ground truth label이 존재하지 않는 비지도 학습 상황에서는 기존 Shallow Encoder을 학습하던 방법과 동일하게 Random Walk 등으로 정의되는 인풋 그래프상 노드 similarity를 유지하도록 학습할 수도 있겠죠. 이 부분은 본 강의 말에 다시 다루니까 이해되지 않는대도 걱정 마세요! :)\n\n💡 이 경우 decoder은 임베딩 벡터 간 similarity metric인 벡터 내적 등으로 정의할 수 있습니다. (lecture 4)\n\n이렇게 Deep Encoder을 통해 얻은 노드/그래프 임베딩은 여러 task에서 agnostic하게 활용할 수 있습니다.\n\n학습된 임베딩을 활용할 수 있는 여러 task의 예\n\nNode classification\nLink prediction\nCommunity detection\nNetwork similarity\n\n\n\n\n\n아까 언급했듯이 Deep Encoder을 통해 그래프를 임베딩 한다는 개념은 지금껏 우리가 정형 데이터를 처리했던 방식과 비슷하기 때문에 낯설지 않습니다.\n그렇다면 그냥 널리 사용되고 있는 CNN이나 RNN을 활용해서 그래프를 임베딩 하면 되지 않을까요?\n\n그럴 수 없습니다.\n정형 데이터인 이미지, 텍스트에 비해 비정형 데이터인 그래프는 너무나도 복잡하기 때문이죠. 그래프는 이미지와 같이 (0,0) 등의 기준점을 둘 수 없으며, 텍스트와 같이 명백한 순서가 있지도 않습니다. 그래프는 제각기 다른 사이즈 일 수 있으며 각각의 topological structure 또한 모두 다릅니다. 심지어는 노드 마다 multimodal feature을 가질 수도 있습니다.\n따라서, 비정형 그래프 구조에서 각 노드의 구조적 특징 및 노드 feature을 고려하여 적절하게 임베딩 하는 새로운 방법이 필요합니다.\n\n💡 노드의 Multimodal feature 다시 소셜 그래프를 떠올려 봅시다. 각 노드는 철수, 영희를 포함한 개인을 나타내고, 엣지는 각 개인 사이에 친구 관계가 성립하는지를 나타냅니다. 이 때, 철수라는 노드는 프로필 사진(이미지), 자기 소개 글(텍스트) 등 여러 부가적인 feature을 가질 수 있습니다."
  },
  {
    "objectID": "posts/lec06.html#machine-learning-as-optimization",
    "href": "posts/lec06.html#machine-learning-as-optimization",
    "title": "Lecture 6",
    "section": "Machine Learning as Optimization",
    "text": "Machine Learning as Optimization\n먼저 간단한 지도 학습 문제를 생각해 봅시다. 지도 학습(Supervised Learning)이란 데이터에 대한 ground truth label이 존재하는 경우를 일컫는데, 다시 말해 input \\(x\\)가 주어졌을 때, label \\(y\\)를 예측하는 문제라고 할 수 있습니다. 이러한 task는 아래와 같은 식을 통해 최적화 문제로 바꾸어 해결할 수 있습니다.\n\n위 식을 이해하기 위해 먼저 구성 요소에 대해 하나씩 짚어보겠습니다.\n\n\\(\\theta\\) : 우리가 최적화(학습) 하려는 파라미터들\n\n최종적으로 우리가 학습하고자 하는 파라미터 값입니다. 예를 들어 앞의 Shallow Encoder에서는 학습으로 결정되는 \\(\\|V\\|\\times d\\) 사이즈의 임베딩 look-up table이 \\(\\theta\\)에 해당하겠죠.\n\n\\(\\mathcal{L}\\) : Loss 함수\n\nLoss 함수는 ground truth label \\(y\\)와 머신러닝 모델이 예측한 label \\(f(x)\\)의 차이를 계산하는 metric 입니다. 회귀(Regression) 문제에서 주로 사용되는 L2 loss와 분류(Classification) 문제에서 주로 사용되는 Cross entropy loss 이외에도 L1 loss, Huber loss, Hinge loss 등 다양한 loss 함수가 존재합니다. 대표적인 loss 함수인 L2 loss와 Cross entropy loss의 수식은 각각 다음과 같습니다.\nL2 loss\n\nCross entropy loss\n\n\n결국 우리가 원하는 것은 모델이 예측한 label이 정답 ground truth label에 가까워지는 것이기 때문에, 이 loss 함수 값이 작으면 작을수록 우리의 모델이 더 정확한 예측을 한다는 의미입니다.\n\n그럼 이제 위에서 공부한 각 구성 요소를 바탕으로 다시 이 최적화 식을 해석해 봅시다. 결국 우리가 풀고자 하는 머신러닝 문제는, 정답 label \\(y\\)와 모델이 예측한 label \\(f(x)\\)의 차이를 나타내는 loss 함수를 최소화 하는 방향으로 모델 파라미터 \\(\\theta\\)를 최적화하는 문제로 해석할 수 있습니다."
  },
  {
    "objectID": "posts/lec06.html#gradient-descent",
    "href": "posts/lec06.html#gradient-descent",
    "title": "Lecture 6",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n지금까진 두루뭉술하게만 보였던 머신러닝 문제를 동일한 의미인 최적화 문제로 재정의했습니다. 그렇다면 이 최적화 문제를 어떻게 해결해야 할까요?\n\n(출처: https://ieeexplore.ieee.org/abstract/document/9298092)\n우리는 일반적으로 Loss 함수로 convex function(볼록 함수)를 활용합니다. 이 loss 함수의 값이 작아지는 방향으로 모델 파라미터 \\(\\theta\\) 를 업데이트 하기 위해, \\(\\theta\\)에 대한 \\(\\mathcal{L}\\)의 기울기를 구한 후, 기울기가 작아지는 방향으로 \\(\\theta\\)를 업데이트 해줍니다.\n(위 그림에서 Cost를 \\(\\mathcal{L}\\), Weights를 \\(\\theta\\)로 보시면 됩니다!)\n\n이를 다시 Gradient 벡터라는 개념으로 정리해서 이야기 해보겠습니다. Gradient 벡터란 위의 식과 같이 \\(\\theta\\)에 대한 \\(\\mathcal{L}\\)의 편미분, 즉 기울기 값으로 구성된 벡터로써, 가장 빠르게 \\(\\mathcal{L}\\)이 증가하는 방향을 나타내는 방향 도함수 벡터입니다.\n\n💡 Gradient is the directional derivative in the direction of largest increase\n\n일단 Gradient를 구했으면 이제 할 일은 모델 파라미터 \\(\\theta\\)를 gradient의 반대방향으로, 즉 \\(\\mathcal{L}\\)이 작아지는 방향으로, 반복적으로 업데이트 하는 것입니다.\n\n위 식에서 \\(\\eta\\)는 learning rate로, 한번 파라미터를 업데이트 시 얼마나 변경할 것인지 보폭을 나타내는 값입니다. 이는 학습 과정 동안 동일하게 유지할 수도 있고, 목적에 따라 계속 변하게 할 수도 있습니다. 이론적으로 모델 파라미터의 업데이트는 loss 함수의 local minimum에 도달하여 gradient가 0이 될 때까지 진행하는 것이 맞지만, 실전에서는 검증 데이터셋에서의 성능이 더 이상 증가하지 않는 기점에서 모델 파라미터 업데이트를 중단합니다."
  },
  {
    "objectID": "posts/lec06.html#stochastic-gradient-descent-sgd",
    "href": "posts/lec06.html#stochastic-gradient-descent-sgd",
    "title": "Lecture 6",
    "section": "Stochastic Gradient Descent (SGD)",
    "text": "Stochastic Gradient Descent (SGD)\nGradient Descent의 문제점\nGradient descent 방법으로 최적화 문제를 푸는 것은 이론적으론 무결하지만 현실적으로는 어렵습니다. 앞서 언급했듯이, Gradient 벡터를 계산하기 위해서는 전체 데이터에 대한 loss 값을 구해야 하기 때문에 몇십억개의 데이터를 갖는 오늘날의 데이터셋에 적용하기에는 계산적으로 무리가 있습니다.\n\n(출처: https://www.slideshare.net/w0ong/ss-82372826)\n따라서 전체 데이터셋을 작은 미니 배치로 나누어 모델 파라미터를 업데이트하는 SGD 방법이 등장하게 되었습니다. 미니 배치 마다 gradient를 구하고 모델 파라미터를 업데이트하는 것이 전체 데이터셋을 활용한 모델 업데이트 정도를 근사할 수 있기 때문이죠.\n\n💡 SGD is unbiased estimator of full gradient\n\n오늘날의 최적화 optimizer은 SGD의 여러 발전된 형태로써, Adam, Adagrad, Adadelta, RMSprop 등이 있습니다."
  },
  {
    "objectID": "posts/lec06.html#back-propagation",
    "href": "posts/lec06.html#back-propagation",
    "title": "Lecture 6",
    "section": "Back-propagation",
    "text": "Back-propagation\n지금까지 머신러닝 문제를 최적화 문제로 재정의하고, 최적화 문제를 풀기 위해 gradient를 활용해 모델 파라미터를 업데이트 하는 방법에 대해서 배웠습니다. 잘 따라오고 계신가요?\n이제부터는 실질적으로 머신러닝 모델이 주어졌을 때, gradient를 계산하는 방법에 대해 알아보겠습니다. 오늘날의 머신러닝/딥러닝 모델은 아주 복잡한 형태를 가지지만, 일단 이해의 편의를 돕기 위해 가장 간단한 linear function를 예시로 설명하겠습니다.\nCase 1\n\n첫번째로 우리의 머신러닝 모델이 단순한 선형 변환 함수인 경우를 다뤄보겠습니다.\n\n선형 변환 함수가 벡터 형태이든 행렬 형태이든 관계 없이 gradient는 이렇게 간단히 구할 수 있습니다.\nCase 2\n\n\n두번째로 조금 더 복잡한 형태의 모델로 확장해보겠습니다. (물론 아직 엄청 단순한 형태긴 하지만..) 이 경우에는 \\(W_{1}\\)과 \\(W_{2}\\)에 대해 모두 gradient를 구해야 합니다. 여기서 우리가 고등학교 때 배운 합성함수 미분에서의 연쇄법칙이 사용됩니다.\n\n💡 Chain Rule (연쇄법칙) \n\n\n\n\n연쇄법칙에 따라 gradient가 \\(\\mathcal{L}\\) → \\(f(x)\\) → \\(W_{2}\\) → \\(W_{1}\\) 을 따라 차례로 거꾸로 흐르면서 계산됩니다. 이렇게 말단에서부터 앞쪽까지 gradient가 흘러오기 때문에 역전파(back-propagation)이라는 이름이 붙었다고 합니다."
  },
  {
    "objectID": "posts/lec06.html#non-linearity",
    "href": "posts/lec06.html#non-linearity",
    "title": "Lecture 6",
    "section": "Non-linearity",
    "text": "Non-linearity\n지금까지 예시로써 다뤄본 두 케이스는 사실 모두 선형 모델로써, 비선형적인 데이터를 잘 모델링할 수 없습니다. 따라서 오늘날의 머신러닝 모델은 비선형적인 활성 함수(Activation function)를 도입함으로써 이러한 문제를 해결합니다. 대표적인 비선형 함수로는 ReLU, Sigmoid 등이 있습니다."
  },
  {
    "objectID": "posts/lec06.html#multi-layer-perceptron-mlp",
    "href": "posts/lec06.html#multi-layer-perceptron-mlp",
    "title": "Lecture 6",
    "section": "Multi-layer Perceptron (MLP)",
    "text": "Multi-layer Perceptron (MLP)\n\nMLP란 한 layer마다 선형 변환과 비선형 변환이 합쳐진 가장 기본적인 형태의 머신러닝 모델이라고 볼 수 있습니다. 위 식은 MLP 한 layer을 나타내는데, layer \\(l\\) 의 인풋으로 들어온 \\(x^{(l)}\\)에 \\(W_{l}\\)이 곱해져 선형 변환 된 후 bias 항이 더해집니다. 최종적으로 비선형 함수를 거친 아웃풋이 layer \\(l+1\\)의 인풋으로 전달됩니다. 이를 그림으로 나타내면 다음과 같습니다."
  },
  {
    "objectID": "posts/lec06.html#summary",
    "href": "posts/lec06.html#summary",
    "title": "Lecture 6",
    "section": "Summary",
    "text": "Summary\n지금까지 간단하게 배운 딥러닝의 기본 개념을 정리해보고, 본격적인 오늘 강의의 주제로 넘어가겠습니다.\n\n머신러닝 문제는 최적화 문제로 풀 수 있습니다.\n\n\n\n모델 \\(f\\)는 간단한 선형 함수, MLP, 또는 다른 형태의 신경망일 수 있습니다. (e.g., 추후에 다룰 GNN도 가능합니다)\n먼저 전체 데이터셋을 미니배치로 나누어 인풋 \\(x\\)으로 사용합니다.\n순전파(Forward Propagation): \\(x\\)가 주어졌을 때 loss 함수 값 \\(\\mathcal{L}\\) 구하기\n역전파(Backward Propagation): 연쇄법칙으로 gradient \\(\\nabla_{\\theta}\\mathcal{L}\\) 구하기\nSGD를 활용하여 반복적으로 모델 파라미터 \\(\\theta\\)를 최적화합니다."
  },
  {
    "objectID": "posts/lec06.html#setup",
    "href": "posts/lec06.html#setup",
    "title": "Lecture 6",
    "section": "Setup",
    "text": "Setup\n들어가기에 앞서 반복적으로 활용할 notation에 대해 간략히 설명하고 시작하겠습니다. 앞으로 아래 기호를 쭉 사용하여 설명을 할 예정이니 잘 알아두시기 바랍니다.\n\n\\(V\\): 노드 집합\n\\(A\\): 인접 행렬 (Adjacency matrix)\n\\(X \\in \\mathbb{R}^{m\\times \\|V\\|}\\): 노드 features\n\\(v\\): 노드 집합에 포함된 한 노드, \\(N(v)\\): \\(v\\)의 이웃 노드\n\n그래프 구조는 엣지의 방향성 및 가중 여부에 따라 여러 종류로 분류할 수 있지만, 이해를 위해 여기서는 가장 간단한 undirected & unweighted 그래프로 설명하겠습니다. 따라서 인접 행렬은 0과 1로 이루어진, 대각 방향으로 symmetric한 행렬이라고 생각하실 수 있습니다.\n또한, 이전의 Shallow Encoder과는 달리 이제 노드 feature도 함께 고려하여 노드 임베딩을 학습한다는 점에 유의하시기 바랍니다.\n\n사실 대부분의 그래프 데이터셋은 노드 feature을 포함하고 있지만, 만에 하나 없는 경우라면 다음과 같은 벡터/값을 노드 feature로 사용하기도 합니다.\n\n노드의 one-hot 인코딩 벡터\n상수 벡터 [1, 1, …, 1]\n노드 차수(degree)"
  },
  {
    "objectID": "posts/lec06.html#naive-approach",
    "href": "posts/lec06.html#naive-approach",
    "title": "Lecture 6",
    "section": "Naive Approach",
    "text": "Naive Approach\n딥러닝 모델을 활용하여 그래프 및 노드를 임베딩 하기 위해 가장 쉽게 생각할 수 있는 방법은 단순하게 그래프의 구조적인 특성을 나타내는 인접 행렬과 노드 feature 행렬을 그냥 이어 붙여서 딥러닝 모델에 던져주는 것입니다.\n\n위와 같은 undirected 그래프의 각 노드가 2차원의 feature를 각각 가지고 있다면, 단순히 두 행렬을 이어 붙여서 만든 7차원의 벡터를 뉴럴 네트워크에 전달하면 각 노드를 간단히 임베딩 할 수 있을 것입니다. 하지만 이러한 방법에는 다음과 같은 문제가 존재합니다.\n\n\\(O(\\|V\\|)\\) 파라미터가 필요함 노드 feature이 \\(d\\)차원이라고 가정하면, 각 노드가 뉴럴 네트워크에 입력되는 차원이 \\(\\|V\\|+d\\) 이겠죠? 따라서 그래프의 노드 갯수에 비례하여 모델의 파라미터가 증가합니다.\n다른 사이즈의 그래프에는 적용할 수 없음 위와 같은 그래프에 대해 뉴럴 네트워크를 기껏 학습시켜 놓았는데, 100개의 노드로 구성된 새로운 그래프가 인풋으로 들어온다면, 인풋 차원이 맞지 않아 학습시킨 임베딩 모델을 활용할 수 없을 것입니다.\n노드 순서가 바뀌면 동일 노드의 임베딩이 달라질 수 있음 위 그래프에서 노드 순서를 A→B→C→D→E에서 B→E→A→C→D 등으로 바꾼다면, 이에 따라 인접 행렬도 바뀌게 됩니다. 이렇게 된다면 같은 A 노드를 임베딩 하기 위해 인풋으로 활용되는 7차원의 벡터가 달라지기 때문에 임베딩 결과 값도 달라질 것입니다."
  },
  {
    "objectID": "posts/lec06.html#from-images-to-graphs",
    "href": "posts/lec06.html#from-images-to-graphs",
    "title": "Lecture 6",
    "section": "From Images to Graphs",
    "text": "From Images to Graphs\n그렇다면 기존 CNN 모델에서 아이디어를 차용해보는 건 어떨까요?\n\n\n이미지를 다루는 CNN은 위와 같이 고정된 사이즈의 convolution 필터를 사용하여 이미지를 주욱 훑습니다. 여기서의 convolution 필터는 붉은 원으로 표시된 타겟 픽셀의 주위 픽셀 정보를 축약하는 역할을 합니다. 사실 이미지가 특수한 형태의 그래프로 해석될 수 있음을 생각해보면, 그래프 데이터에서도 타겟 노드의 임베딩을 만들기 위해 주변 노드의 정보를 사용한다는 아이디어는 나쁘지 않아 보입니다.\n하지만 그래프에서는 CNN에서와 같이 고정된 크기의 필터(?)를 사용할 수 없습니다. 어떤 노드는 한두개의 이웃 노드를 가지지만 또 어떤 노드는 수백수천개의 이웃 노드를 가질 수 있기 때문이죠.\n\n그렇다면 그래프를 임베딩 할 때 타겟 노드의 이웃 노드에서 정보를 전달받아 이를 활용하여 타겟 노드의 임베딩을 업데이트 하되, 타겟 노드마다 이웃 노드의 갯수가 다를 수 있는 점을 고려하여 각기 다른 computation graph를 갖도록 하는 것이 좋겠습니다!\n다음과 같은 아이디어를 근간으로 Graph Convolutional Network가 등장하게 되었습니다. 남은 강의에서는 이 GCN을 디테일하게 설명하고 있습니다."
  },
  {
    "objectID": "posts/lec06.html#idea-aggregate-neighbors",
    "href": "posts/lec06.html#idea-aggregate-neighbors",
    "title": "Lecture 6",
    "section": "Idea: Aggregate Neighbors",
    "text": "Idea: Aggregate Neighbors\n\n주요 Idea: 이웃 노드 정보를 가지고 타겟 노드 임베딩을 생성하자!\n\n\n왼쪽과 같은 그래프에서 우리가 임베딩하고 싶은 타겟 노드가 노란색의 A 노드라고 생각해 봅시다. 그렇다면 A 노드의 이웃 노드, 그리고 그 이웃 노드들의 이웃 노드를 가지고 오른쪽과 같은 computation graph가 생깁니다.\n타겟 노드 A는 직속 이웃인 노드 B, C, D로부터 메시지를 전달 받고, 모든 메시지를 합친 후 어떠한 변환을 거쳐 본인의 임베딩으로 활용합니다. 우측 그림에서 회색 박스로 표시된 뉴럴 네트워크가 바로 이러한 1) 메시지 변환, 2) 이웃 노드로부터 온 메시지를 통합하는 두 과정을 수행하게 됩니다. 이 뉴럴 네트워크 내의 모델 파라미터가 최종적인 우리의 학습 대상이 되는 것입니다.\n여기서 또 눈여겨 보아야 할 점이 있습니다. 여지껏 우리는 노란색 A 노드를 타겟 노드로 한 computation graph만 보았는데, 그렇다면 B, C, D 등 다른 타겟 노드에 대해서도 동일한 computation graph를 가질까요?\n\n아닙니다. 노드마다 이웃 노드의 갯수와 종류가 다르기 때문에 당연히 노드마다 서로 다른 computation graph를 가지게 됩니다."
  },
  {
    "objectID": "posts/lec06.html#deep-model-many-layers",
    "href": "posts/lec06.html#deep-model-many-layers",
    "title": "Lecture 6",
    "section": "Deep Model: Many Layers",
    "text": "Deep Model: Many Layers\n\nLayer의 수\nDeep Encoder은 여러 layer로 구성할 수 있는데, 한 layer이 직속 이웃 노드에 대한 정보를 aggregate하는 것이기 때문에 layer을 두 개 쌓는다면 직속 이웃 노드에 대한 이웃 노드, 즉 타겟 노드로부터 2-hop 떨어진 노드의 정보까지 활용하겠다는 의미로 해석할 수 있습니다. 위 그림에서도 잘 나타나 있는데, 2개의 layer로 구성된 모델을 사용하기 때문에 타겟 노드 A로부터 2-hop 떨어진 노드 E, F의 정보도 임베딩 생성에 활용되는 것을 볼 수 있습니다.\n노드 임베딩 초기화\n또한, 보통 Layer-0에서 최초 노드 임베딩으로 노드 feature을 사용합니다. 모든 노드 임베딩은 layer을 거칠수록 이웃 노드의 정보를 변환하고 합친 후 업데이트 됩니다. 결국 모든 layer을 거치고 나면 최종 노드 임베딩이 생성되어 우리가 downstream task를 위해 사용하게 되는 것이죠.\nAggregator 함수\n여기서 타겟 노드 A가 이웃 노드 B, C, D의 메시지를 aggregation 할 때, 노드 B, C, D의 순서와 관계 없이 aggregate된 메시지는 동일해야 합니다. 즉, 메시지를 aggregate하는 함수는 permutation-invariant한 속성을 가져야 한다는 말입니다. 일반적인 GNN은 주로 합, 평균, 맥스 풀링등의 aggregator를 활용합니다."
  },
  {
    "objectID": "posts/lec06.html#the-math-deep-encoder",
    "href": "posts/lec06.html#the-math-deep-encoder",
    "title": "Lecture 6",
    "section": "The Math: Deep Encoder",
    "text": "The Math: Deep Encoder\n\n자, 이제 가장 기본적인 GNN 형태를 정의하고 이를 수식으로 나타내어 알고리즘의 원리를 자세히 들여다보는 시간을 갖겠습니다. 우리의 GNN은 타겟 노드의 이웃 노드 임베딩을 전달받아 이를 평균냄으로써 메시지를 aggregate 합니다. 그 후, 뉴럴 네트워크를 통해 어떠한 변환을 거치고 이를 활용하여 타겟 노드 임베딩을 업데이트 합니다. 이를 수식으로 나타내면 아래와 같습니다.\n\n식이 처음엔 되게 복잡해 보이지만, 하나씩 뜯어보면 사실 아주 간단합니다. 식 전반에 나타나는 \\(h_{v}^{(l)}\\) 은 \\(l\\)번째 layer에서 노드 \\(v\\)의 임베딩을 나타낸다고 보시면 됩니다.\n\n초록색 수식 블럭 : 첫 노드 임베딩을 노드 feature로 초기화합니다.\n파란색 수식 블럭 : \\(l+1\\)번째 layer에서의 노드 임베딩을 만들기 위해, 먼저 타겟 노드 \\(v\\)의 이웃 노드에 대해 \\(l\\) 번째 layer에서의 노드 임베딩 평균을 구합니다. 그 후, 이웃 노드의 평균 임베딩에 어떠한 transformation \\(W_{l}\\) 을 가합니다.\n빨간색 수식 블럭 : 타겟 노드의 임베딩을 업데이트할 때 이웃 노드 뿐 아니라, 이전 layer에서 갖고 있던 자기 자신의 임베딩도 활용합니다. 같은 방법으로 \\(h_{v}^{(l)}\\)에 어떠한 transformation \\(B_{l}\\) 을 가합니다.\n노란색 수식 블럭 : 최종적으로 비선형 함수를 적용해서 \\(l+1\\)번째 layer에서의 타겟 노드 임베딩을 구합니다.\n보라색 수식 블럭 : 노드 임베딩 업데이트 과정을 \\(L\\)번 반복합니다. 이 때 최종적으로 형성된 노드 임베딩은 본인으로부터 L-hop 떨어진 노드의 정보까지 활용하여 만든 것입니다."
  },
  {
    "objectID": "posts/lec06.html#matrix-formulation",
    "href": "posts/lec06.html#matrix-formulation",
    "title": "Lecture 6",
    "section": "Matrix Formulation",
    "text": "Matrix Formulation\n이전에 Random Walk를 행렬 형태로 표현했듯이, 벡터식으로 다뤘던 GNN도 행렬식으로 다시 표현해보겠습니다.\n모든 노드 임베딩 벡터를 한데 모아 노드 임베딩 행렬을 만든다면, 이는 아래와 같이 표현할 수 있습니다.\n\\(H^{L} = {[h_{1}^{(l)} ... h_{\\|V\\|}^{(l)}]}^T\\)\n그렇다면 이웃 노드의 임베딩을 합산하는 과정은 그래프의 인접 행렬을 사용하여 아래와 같이 간단하게 나타낼 수 있습니다.\n\\(\\sum_{u\\in N_{v}} h_{u}^{(l)} = A_{v:}H^{(l)}\\)\n만약 대각 행렬을 이렇게 정의한다면, 이 대각 행렬의 역행렬은 다음과 같기 때문에,\n\\(D_{v,v} = Deg(v) = \\|N(v)\\|\\) , \\(D_{v,v}^{-1} = 1/\\|N(v)\\|\\)\n이를 활용하면 이웃 노드의 임베딩을 평균내는 연산을 행렬식으로 간단하게 표현할 수 있습니다.\n\n따라서 최종적으로 노드 임베딩 업데이트 함수를 행렬식으로 나타내면 아래와 같습니다.\n\n벡터식으로 이해하기도 어려웠는데 왜 굳이 사서 행렬식으로 변환하냐고요? 사실 행렬식이 가지는 구현상의 이점이 있기 때문입니다. 행렬식을 사용한다면 각 노드에 대한 임베딩을 따로 따로 업데이트 하지 않고 하나의 행렬로써 한번에 업데이트 할 수 있으며, 이 과정에서 \\(\\tilde{A}\\)가 희소 행렬이기 때문에 보다 더 효율적인 행렬 연산이 가능하기 때문에 구현할 때는 행렬식이 더 선호됩니다."
  },
  {
    "objectID": "posts/lec06.html#how-to-train-a-gnn",
    "href": "posts/lec06.html#how-to-train-a-gnn",
    "title": "Lecture 6",
    "section": "How to train a GNN",
    "text": "How to train a GNN\n오늘 강의의 마지막 부분으로 이렇게 구성한 GNN을 어떻게 학습시켜야 하는지 알아보겠습니다. 시작하기에 앞서 학습의 대상이 되는 파라미터가 무엇인지 짚고 넘어가보죠.\n\n다음 식에서 학습되는 파라미터는 \\(W_{l}\\)와 \\(B_{l}\\) 입니다. 딸린 subscript를 봐도 알 수 있듯이, 두 파라미터 행렬은 모두 layer마다 따로 존재하며, 한 layer 내에서는 공유됩니다. 이를 간단하게 그림으로 표현하면 아래와 같습니다.\n\nGNN을 학습하는 방법은 여느 딥러닝 학습과 마찬가지로 크게 지도 학습, 비지도 학습 세팅으로 나뉩니다. 이를 하나씩 살펴보도록 합시다.\n\n지도 학습 세팅\n\n노드 label \\(y\\)가 존재하는 상황\n정답 노드 label \\(y\\)를 활용하여 \\(min_{\\theta}\\mathcal{L}(y,f(z_v))\\)를 풂, 이 때 task에 맞게 L2 loss 혹은 Cross entropy loss 등을 loss 함수를 사용함\n예시) 각 노드가 safe한지 혹은 toxic한지 분류하는 node classification → 분류 문제이므로 cross-entropy loss를 사용하고, 노드의 정답 클래스 label을 활용하여 직접 모델 학습 가능\n\n💡 아래 loss 식에서 \\(\\sigma(z_v^T\\theta)\\)는 학습된 노드 임베딩 \\(z_v^T\\)를 가지고 모델이 예측한 노드 \\(v\\)의 클래스 확률 을 나타냅니다.\n\n\n\n\n비지도 학습 세팅\n\n노드 label이 존재하지 않는 상황\n3강에서 공부했던 그래프 상 노드 similarity를 supervision으로 활용\n\n임의의 supervision 시그널을 만들어 비지도 학습 세팅을 지도 학습 세팅으로 바꾸기 위해서 원본 그래프에서의 노드 similarity를 바탕으로 label을 지정해줍니다. 여기서 노드 similarity는 3강에서 다뤘던 Random Walk 등을 활용할 수 있습니다.\n간단하게 DeepWalk로 노드 similarity를 정의하는 경우를 생각해볼까요? 만약 두 노드 \\(u\\)와 \\(v\\)가 랜덤 워크 상에서 co-occur한다면 두 노드는 ’similar’하다고 말할 수 있으며, \\(y_{u,v} = 1\\)로 임의의 label을 붙입니다. 또한 여기서 \\(DEC(z_u,z_v)\\)는 학습된 두 노드의 임베딩을 내적함으로써 임베딩 공간에서의 노드 similarity를 측정합니다. Loss 함수로 cross-entropy를 사용함으로써 그래프에서 노드 similarity를 최대한 잘 보존하도록 노드 임베딩을 학습할 수 있게 됩니다.\n\n💡 원본 그래프에서 similar한 노드는 → similar한 임베딩을 갖도록 합니다"
  },
  {
    "objectID": "posts/lec06.html#model-design-overview",
    "href": "posts/lec06.html#model-design-overview",
    "title": "Lecture 6",
    "section": "Model Design: Overview",
    "text": "Model Design: Overview\n자 그럼, 오늘 배웠던 내용을 한번 쭉 훑어 정리하고 포스트를 마무리하도록 하겠습니다. 그래프를 위한 Deep Encoder, a.k.a. GNN 모델을 만들기 위해서 아래와 같은 단계를 따라가야 합니다.\n\n이웃 노드 임베딩을 aggregate하는 함수를 정함\nTask의 특성에 맞추어 loss 함수를 정의함\n여러 computation graph에 대해 GNN 모델을 학습시킴\n학습된 모델을 갖고 노드에 대한 임베딩을 생성할 수 있음. 이 때, 모든 노드에 대해 뉴럴 네트워크의 파라미터가 공유되기 때문에 학습에 사용되지 않은 노드에 대해서도 임베딩을 생성할 수 있음 (Inductive Capability)\n  \n\n\n💡 Inductive Capability1. 새로운 그래프 : 예를 들어 분자 그래프에서, 화합물 A에 대해 학습된 GNN 모델이 화합물 B 그래프에서 임베딩을 만드는데 활용될 수 있음2. 새로운 노드 : 시간에 따라 evolving 하는 그래프에서 새로운 노드가 추가될 때마다 바로바로 임베딩을 생성할 수 있음"
  },
  {
    "objectID": "posts/lec06.html#summary-1",
    "href": "posts/lec06.html#summary-1",
    "title": "Lecture 6",
    "section": "Summary",
    "text": "Summary\n\nDeep Encoder (GNN)의 핵심 아이디어: 이웃 노드의 정보를 aggregate 함으로써 노드 임베딩을 생성하자!\n모델 내 Aggregator과 Transformation 함수를 각각 어떻게 정의하느냐에 따라 모델 구조가 달라질 수 있습니다.\n다음 강의에선 GNN variant의 하나인 GraphSAGE를 다룰 것입니다."
  },
  {
    "objectID": "posts/lec03.html",
    "href": "posts/lec03.html",
    "title": "Lecture 3",
    "section": "",
    "text": "representation learning 목적은 feature engineering을 통해 직접 node의 feature를 지정하는 대신, feature를 자동으로 학습하는 것입니다.\n그림에서와 같이, representation learning을 거쳐 node u를 d차원 vector로 표현하게 됩니다. 이때, feature를 표현하는 vector를 ‘feature representation’ 혹은 ‘embedding’이라고 부릅니다. 이 vector는 node의 특징을 잘 담아내야 하며, 그래프의 전반적인 구조의 의미도 포함해야 합니다.\n\n\n\n\nembedding space내에 표현된 node들은 유사성(similarity) 가질수록 비슷한 embedding을 가진다는 것이 핵심입니다. 이렇게 표현된 node들은 후속 작업인 예측 task (node classification, link prediction, graph classification 등)에 효과적으로 활용됩니다.\n\n\n\nDeepWalk 의 연구로, Zachary’s Karate Club network를 2차원 vector의 feature로 표현했습니다. 같은 색상을 가진 node끼리 비슷한 vector 값을 갖는 것을 확인할 수 있으며, 이를 통해 embedding 작업이 꽤 성공적으로 이루어졌다고 판단할 수 있습니다.\n\n\n\n\n우선 간단한 그래프에서 시작해봅시다!\nfeature는 없고, 연결 관계만(adjacency matrix)만 존재하는 undirected 그래프가 있다고 합시다. 이때 embedding 목표는 embedding후의 embedding space에서의 각 node끼리의 유사성(similarity)과 그래프상에서의 유사성이 비슷해지는 것입니다. embedding space에서의 유사성을 측정할 때는 vector간의 dot product 연산을 수행하는 것이 일반적입니다. dot product은 \\(a \\cdot b = |a||b|cos\\theta\\) 이므로, 두 vector간의 각도가 작을수록(=가까움, 비슷함), 큰 값을 갖게 됩니다. 자 이제, embedding space에서의 유사성은 측정할 수 있게 되었으므로, 그래프상에서의 유사성을 측정할 수 있는 similarity function을 정의해야 합니다.\n\nGoal : \\(\\red {similarity(u,v)} \\;\\approx\\; \\green{z^T_v z_u}\\)\n\n\n\n\n\n1) Encoder를 통해 node를 embedding값으로 변환합니다. Encoder : \\(ENC(v) = z_v\\), \\(z_v : d\\)-dimensional embedding 이때, embedding 차원으로 보통 64~1000을 채택합니다.\n2) 그래프 상에서의 노드 간 유사성을 측정할 similarity function을 정의합니다.\n3) Decoder는 embedding값들 간의 유사성을 측정합니다. → similarity score Decoder : \\(DEC(z^T_vz_u)\\)\n4) \\(\\red {similarity(u,v)} \\;\\approx\\; \\green{z^T_v z_u}\\)가 될 수 있도록, Encoder의 parameter들을 최적화 합니다.\n\n\n\n가장 간단한 encoder는 단순한 embedding-lookup(→조회)입니다.\n\n\\(ENC(v) = z_v= Z \\cdot v\\) , \\(Z\\in\\R^{d*|v|}\\), \\(v\\in I^{|v|}\\)\n\n여기서 \\(**Z\\)는 모든 node의 embedding을 포함하는 matrix로 각 column이 하나의 node embedding을 의미합니다. \\(v\\)는 indicator vector로, 현재 표현할 node만 1이고 나머지는 0인 vector입니다. 이때의 목표는 matrix \\(Z\\)를 최적화하는 것입니다. 다음 방법을 이용한 대표적인 알고리즘은 DeepWalk와 node2vec**이 있습니다.\n\n하지만 이때의 문제점은 최적화해야 할 embedding matrix \\(Z\\)의 parameter수가 매우 커질 수 있다는 점입니다. 예를 들어, 10억 개 node로 구성된 그래프에 대해서 최적화 해야 할 parameter는 (10억 * embedding dimension)개 입니다. 따라서 해당 방법은 단순 lookup 으로 매우 간단할 수 있지만, 확장 가능성이 작습니다.(= low scalability)\n(vs. deep encoder(GNNs)는 6장에서부터 다룰 예정입니다🙂)\n\n\n\nnode embedding을 찾는 것은 unsupervised/self-supervised 방식으로 구분될 수 있습니다. 위에서 언급했듯이 node label, node feature 등을 이용하지 않고, graph network의 구조를 보존한 채 node embedding을 찾기 때문입니다. 따라서, 해당 방법은 task independent로, 어떤 task든 적용 가능합니다."
  },
  {
    "objectID": "posts/lec03.html#random-walk-approaches-for-node-embeddings",
    "href": "posts/lec03.html#random-walk-approaches-for-node-embeddings",
    "title": "Lecture 3",
    "section": "3.2 Random Walk Approaches for Node embeddings",
    "text": "3.2 Random Walk Approaches for Node embeddings\n본격적인 node embedding 방법 소개에 앞서, 용어부터 살펴봅시다!\n\nVector \\(z_u\\) : node u의 embedding → 우리가 찾고자 하는 것.\nProbability \\(P(v|z_u)\\) : node u에서 시작해서 random walk로 node v에 방문할 확률\nNon-linear function :\n\n\nsoftmax\n\n입력받은 값을 출력으로 0~1 사이의 값으로 모두 정규화하며 출력값들의 총합은 항상 1이 되는 특성을 가진 함수로, 분류하고 싶은 class 수만큼 출력으로 구성합니다. 가장 큰 출력값을 부여받은 class가 확률이 가장 높음을 의미합니다.\n\nsigmoid function\n\n모든 실수 입력 값을 0보다 크고 1보다 작은 미분 가능한 수로 변환하는 특징을 갖습니다.\n\nRandom Walk :\n\nnode 4에서 시작한다면, 이동 가능한 neighbor node중 random으로 하나를 선택하여 이동합니다. 이후, 고정된 수만큼 반복적으로 이동하면 됩니다. random walk를 수행하면 그래프상에서 이동한 일련의 node들이 결과로 도출됩니다. 아래 그림에서는 {4,5,8,9,8,11} 입니다.\n\nrandom walk에서의 \\(**z^T_v z_u\\)는 node u와 node v가 random walk 중 동시에 방문 될 확률**을 의미합니다. random walk 전략 \\(R\\)이 있다고 할 때, node u에서 시작하여 전략 \\(R\\)에 따라 random walk를 수행했을 때, node v를 방문할 확률, \\(P_R(v|u)\\) 을 예측합니다. 이후, 두 vector 사이의 각도 \\(\\theta\\)가 \\(P_R(v|u)\\)에 비례하도록 embedding 을 최적화합니다.\n\n\nWhy Random Walks?\n장점 1) Expressivity\nlocal(본인)뿐만 아니라, higher-order neighborhood(여러 hop 떨어진 이웃)들의 정보를 포함할 수 있게 됩니다.\n장점 2) Efficiency\n모든 node를 한꺼번에 고려하지 않고, random walk를 통해 방문한 node들의 쌍만 고려하여 학습할 수 있습니다.\n\n\nUnsupervised Feature Learning\n다시 unsupervised feature learning에서의 목적으로 돌아와 보면, 유사성을 보존한 채 node embedding을 찾고자 합니다. 설득력 있는 node embedding이 되기 위해서 가까운(nearby) 노드끼리 비슷한 embedding을 갖도록 해야 합니다.\n자 그럼 ’가까운’은 어떻게 정의할 수 있을까요?? 여기서 random walk가 등장합니다!! 바로 ’random walk로 방문하게된 이웃들을 ’가깝다’라고 볼 수 있습니다.\n\n\\(N_R(u)\\) : random walk 전략 \\(R\\)에 따라 node u에서 출발하여 방문하게된 이웃들(neighborhood)\n\n\n\nFeature Learning as Optimization\n그래프 \\(G = (V,E)\\)가 주어졌을 때,\nnode embedding function, \\(f: u \\rightarrow \\R^d : f(u) = z_u\\) 을 최적화해야합니다.\n이를 위해서, log-likelihood objective는 \\(max_f \\sum_{u\\in V}\\log P(N_R(u)|z_u)\\) 으로, 해당 값이 최대화 되는 함수 \\(f\\)를 찾아야합니다.\n\n\nRandom Walk Optimization\nStep 1) random walk strategy \\(R\\)에 따라서 각 node u에서 고정된 크기의 짧은 random walk를 수행합니다.\nStep 2) 각 node 마다 multiset 이웃 집합, \\(N_R(u)\\)을 모읍니다. *multiset : random walk동안 특정 노드를 여러 번 반복할 수 있으므로, 중복된 원소를 가질 수 있습니다.\nStep 3) node u가 주어졌을 때, 이웃들 \\(N_R(u)\\) 을 예측할 수 있도록 embedding을 최적화합니다.\n\\(max_f \\sum_{u\\in V}\\log P(N_R(u)|z_u)\\)\n위 식은 모든 node u, 그리고 그 각각의 이웃 노드 v에 대한 식으로 풀어볼 수 있습니다.\n\n\\(L = \\sum_{u\\in V}\\sum_{v\\in N_R(u)} -\\log(P(v|z_u))\\),\n\n또한, node u에 가까운 node v를 판별(비교)하고자 하는 점에서 softmax를 사용하여, \\(p(v|z_u)\\) 를 표현할 수 있습니다.\n\n\\(p(v|z_u) = \\frac{exp(z^T_uz_v)}{\\sum_{n\\in V}exp(z^T_u z_n)}\\)\n\n\n다음과 같이 정의한 loss function \\(L\\) 이 최소화되도록 \\(z_u\\)를 찾습니다!!\n하지만, 다음 식은 매우 계산이 expensive 합니다. 모든 node에 대한 계산이 2번이나(위 그림에서 파랑색, 노랑색 부분에 해당) 중첩되게 이루어지기 때문에, complexity가 무려 \\(O(|V|^2)\\)입니다.\n이를 실제에서 활용하기 위해선, softmax 항의 분모에 해당하는 부분을 근사해야 합니다.\n→ 방법은, ‘Negative Sampling’입니다!!\n모든 노드에 대한 값을 구해 normalize하는 것이 아니라, k개의 random negative sample에 대해 정규화를 진행합니다. 이때 k개의 sample을 sampling할 때는 degree에 기반하여 bias sampling(\\(n_i \\sim P_v\\))을 수행합니다. higher degree(undirected graph의 경우, 연결된 edge 수)를 가질수록 sample 될 확률이 높아집니다. 샘플의 갯수 k가 커질 수록, 더 정교한 예측이 가능하지만, 동시에 bias가 커지는 단점이 존재합니다. 보통, k는 5~20으로 지정합니다.\n\nloss function을 최적화할 때 보통 stochastic gradient descent를 사용합니다. gradient descent란, random point에서 시작하여 미분값을 계산하고 learning rate에 맞게 방향을 조금씩 변화합니다. 이 작업을 수렴할 때 까지 반복합니다. 특히, stochastic gradient descent는 모든 example에 대해서 해당 계산을 수행하는 것이 아니라, 매번 랜덤으로 선택한 하나의 example에 대해서만 미분값을 계산하고 업데이트 해나갑니다.\n자 여기까지 Random Walk 과정을 요약해보자면,\nStep 1) random walk strategy \\(R\\)에 따라서 각 node u에서 고정된 크기의 짧은 random walk를 수행합니다.\nStep 2) 각 node 마다 multiset 이웃 집합, \\(N_R(u)\\)을 모읍니다.\nStep 3) loss funcion : \\(L = \\sum_{u\\in V}\\sum_{v\\in N_R(u)} -\\log(P(v|z_u))\\) 다음 loss function을 negative sampling을 사용해 softmax 부분을 근사하고, stochastic gradient descent를 통해 최적화여 embedding을 구합니다.\n\n\nHow should we randomly walk?\n그렇다면 효과적인 random walk 전략 \\(R\\)은 무엇일까요?? 고정 크기 만큼의 random walk와 같은 단순한 방법은 유사성을 표현하기에 매우 한정적입니다. 이를 극복하기 위해 등장한 방법이 node2vec 입니다. 표현력이 좋은 (expressive)한 이웃들을 모으기 위해 ‘biased 2nd order random walk \\(R\\)’ 이 사용됩니다.\n\n\nnode2vec : Biased Walks\n핵심 아이디어는 local + global 탐색의 tradeoff를 잘 고려하여 biased walk를 수행하는 것입니다. 이때 local은 BFS(Breadth-First Search) 너비 우선 탐색을 통해, global은 DFS(Depth-First Search) 깊이 우선 탐색을 통해 얻을 수 있습니다.\n\n\n\nInterpolating BFS and DFS\nnode u 에 대한 \\(N_R(u)\\)를 모을 때, 2가지 hyper-parameter를 지정하여 BFS와 DFS 방법을 동시에 적절히 활용할 수 있습니다.\n1) return parameter \\(p\\) : 이전 node로 돌아갈 확률\n2) in-out parameter \\(q\\) : BFS(inwards)와 DFS(outwards)간의 비율(ratio)\n\n\nBiased 2nd-order random walks\n다음 그림은 random walk 중 node \\(s_1\\)에서 node \\(w\\)로 이동한 상황입니다.\n이제 \\(w\\)에서 취할 수 있는 행동 유형은 총 3가지, <(1) 이전 노드 \\(s_1\\)으로 돌아가기, (2) \\(s_1\\)에서 동일하게 1 hop 떨어져 있는 \\(s_2\\)로 이동, (3)더 멀리 \\(s_3, s_4\\)로 이동 >이 있습니다. 앞서 정의했던 parameter들을 적용해보면, \\(1/p\\)가 다시 돌아갈 확률, \\(1/q\\) 를 더 멀리 이동할 확률로 표현할 수 있습니다. 이후, \\(w\\)에서의 4가지 선택지(\\(s_1, s_2, s_3,s_4\\))에 대한 확률을 1로 맞추어 정규화합니다. BFS와 같이 inwards를 돌아다니게 하려면 p값을 낮추면 되고, DFS와 같이 outwards를 돌아다니려면 q값을 낮추면 됩니다.\n\n\n\nnode2vec algorithm\nStep 1) random walk 확률을 계산합니다.\nStep 2) 각 노드 u 에서 길이 \\(l\\) 만큼의 random walk를 \\(r\\)번 시뮬레이션합니다.\nStep 3) Stochastic gradient descent를 사용하여 node2vec objective function을 최적화합니다.\n시간 복잡도는 linear-time이며, 각 step이 병렬적으로 수행될 수 있습니다. 다만, 해당 방법의 단점은 모든 node들이 각각의 node embedding을 학습해야 한다는 것입니다. 따라서 그래프 크기가 커질수록, 더 많은 embedding을 학습해야 합니다.\n\n일반적으로 node2vec은 node classification을 잘 수행한다고 알려져있으며, random walk는 전반적으로 우수한 성능을 보입니다. 하지만 어떤 알고리즘을 사용해야 하는가는 본인의 연구에 제일 잘 맞는 방법을 채택해야 합니다."
  },
  {
    "objectID": "posts/lec03.html#embedding-entire-graphs",
    "href": "posts/lec03.html#embedding-entire-graphs",
    "title": "Lecture 3",
    "section": "3.3 Embedding Entire Graphs",
    "text": "3.3 Embedding Entire Graphs\n3장에서는 node level에서 벗어나 그래프 전체를 embedding 해봅시다.\n\n방법 1) node embedding의 합\nrandom walk 혹은 node2vec에서 구한 node embedding을 이용하여 계산(합/평균) 합니다.\n\n\\(z_G = \\sum_{v\\in G }z_v\\)\n\n\n해당 방법은 molecules를 분류하는 연구에 성공적으로 적용되었습니다.\n\n방법 2) ‘virtual node’\n그래프의 특정 부분을 대표하는 ’virtual node’를 추출하고, 여기에 graph embedding 기술을 적용합니다.\n\n방법 3) Anonymous Walk Embeddings\n첫 번째로 방문한 node부터 순서대로 index를 부여합니다. random walk 로 이동하면서 index sequence를 기록합니다. 방문한 node가 실제 어떤 node인지 상관없이 방문한 순서에 따라 index가 주어집니다.\n\nrandom walk 길이가 3일때, 가능한 anonymous walk의 수는 총 5개 입니다. → \\(w_1=111,w_2=112,w_3=121,w_4=122,w_5=123\\)\n가능한 anonymous walk수는 random walk 길이에 따라 exponential하게 증가합니다.\n\n\nSimple Use of Anonymous Walks\nstep 수 \\(l\\) 을 정한 뒤, 가능한 anonymous walk를 계산합니다. 이후 각 walk에 대한 probability distribution을 계산합니다. 예를 들어, \\(l=3\\)일 때 가능한 walk는 5개였습니다. 따라서 각 5개 walk에 대한 확률을 계산하여, 해당 그래프를 5차원의 vector로 표현할 수 있습니다.\n\n\\(Z_G[i]\\) = probability of anonymous walk \\(w_i\\)\n\n\n\nSampling Anonymous Walks\n그렇다면 적당한 random walk는 몇 개 일까요? \\(\\delta\\)의 확률보다 낮게 \\(\\epsilon\\) 이상의 error를 갖게 하기 위해선 다음 식을 만족해야 합니다.\n\n\n\nNew idea : Learn Walk Embeddings\n새로운 아이디어는 anonymous walk \\(w_i\\)의 embedding \\(z_i\\)를 학습하는 것입니다. 이를 통해, 그래프 embedding이 가능합니다. \\(Z = \\{z_i : i = 1 \\dots \\eta \\},\\;\\eta\\) : sampled anonymous walks 갯수\nanynomous random walk를 샘플링합니다. 이후 \\(\\Delta\\)-size window 내에 어떤 walk 가 함께 발생할지 예측합니다. 예를 들어, \\(\\Delta\\)가 1일 때 경우, 직전과 직후에 어떤 walk가 발생할 지 예측하는 것입니다. 이를 수식으로 풀어보면 objective는 다음과 같습니다. graph embedding \\(Z_G\\)와 time window 내에 walk들(\\(w_{t-\\Delta}, \\dots, w_{t+\\Delta}\\))이 주어졌을 때 \\(w_t\\)에 대한 log probability를 최대화해야 합니다. 모든 node가 출발점이 되어서 각각의 objective를 계산하고, 이를 모두 더합니다.\n\n이제 node u 의 이웃 \\(N_R(u)\\)은 random walk 의 set으로 표현됩니다.\n\\(N_R(u) = \\{w_1^u, w_2^u,\\dots, w_T^u\\}\\)\n\n\n\nUntitled 16\n\n\n다음과 같이 graph embedding \\(Z_G\\)를 구한뒤, inner product 혹은 neural network를 거쳐 graph classification과 같은 예측 작업에 사용할 수 있습니다.\n\n\nSummary of Graph embedding\n지금까지, 총 3가지 그래프 embedding 방법에 대해 다루었습니다.\n방법 1) deep walk 혹은 node2vec으로 구한 node embedding을 합하거나 평균내기\n방법 2) virtual node와 같이 super-node구해서 embedding하기\n방법 3) Anonymous Walk Embedding\n3-1) anonymous walk sample을 구한 뒤, 각각의 walk가 몇번 발생했는지 비율 계산\n3-2) anonymous walk를 embedding하고 이를 합쳐 graph embedding하기\n*차후의 강의에서는 Hierarchical Embedding에 대해 다룹니다(Lecture 8).\ngraph상의 node들을 계층적으로 묶은 뒤, 이들을 합/평균 내어 graph embedding 합니다.\n\n\n\nUntitled 17\n\n\n\n\nHow to Use Embeddings\n열심히 구한 embedding은 아래와 같이 활용 가능합니다🙂🙂\n1) Clustering/ community detection\n2) Node classification\n\n3) Link Prediction\n\n2개의 embedding(\\(z_i,z_j\\))에 대해 concatenate, hadamard, sum/avg, distance 계산\n\n\n4) Graph classification"
  }
]