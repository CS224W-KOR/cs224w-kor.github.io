---
toc: true
title: "Lecture 4"
description: Link Analysis - PageRank
author: "Jung Yeon Lee"
date: "2022-07-14"
categories: [LEC04]
---

# 4.1 - PageRank


4강에서는 Graph를 **매트릭스(선형대수) 관점**으로 바라보는 것에 대해 이야기 합니다. 

![](https://i.imgur.com/aU2bZOQ.png?1)

다음 3가지 키워드, **Random walk(Node Importance), Matrix Factorization, Node embedding**를 중심으로 공부합니다. 강의는 총 4파트로 나누어져 진행됩니다.


## The Web as a Directed Graph

![](https://i.imgur.com/OSK8R0d.jpg?1)

웹을 거시적인 관점으로 보게되면, 
**하나의 웹 페이지 → Node**로 
**하이퍼링크 → Edge**로 생각하여 하나의 거대한 **Graph**로 볼 수 있습니다.

> *Side issue*
- 다이나믹하게 새로 페이지들이 생길 수 있습니다.
- 다크웹과 같은 접근할 수 없는 페이지들도 있을 수 있습니다.
> 

잠시 Side issue는 내려놓고, 새로 페이지들이 생기지도 않고 기존의 페이지들이 사라지지도 않는 **Static pages** 상황을 가정해봅시다. 아래의 그림에서처럼 페이지들은 하이퍼링크들로 서로 연결되어 있고, 유저는 페이지들에 달려있는 **하이퍼 링크들**로 이루어진 연결망을 기반으로 항해하듯이 **Navigational** 하게 page to page 이동을 하게 됩니다. 
(오늘날에는 post, comment, like 등의 기반의 transactional한 웹에서의 상호작용이 일어나지만 이는 우선 논외로 하겠습니다.)

![](https://i.imgur.com/wRTsHJI.png?1)
![](https://i.imgur.com/DAynWgh.png?1)

위의 그림처럼 웹 그래프는 방향성이 있는 **유향 그래프(Directed graph)**임을 알 수 있습니다. 위키피디아와 같은 웹 사전 페이지들 간의 관계성이나 논문의 인용 관계 그래프 등에서 예시를 쉽게 찾아볼 수 있습니다.

![](https://i.imgur.com/idCvZ6r.png?1)

## Ranking Nodes on the Graph

웹을 하나의 거대한 유향 그래프로 생각할 때 한가지 **중요한 insight**가 있습니다.

> 💡 **모든** 웹 페이지들이 **똑같이 중요하지는 않다**

바로 각 페이지의 중요성이 똑같지 않다는 이야기는 그래프에서 **각 노드의 중요성(importance)가 다르다**는 말로 바꿔 생각할 수 있습니다. 아래 사진을 보면 직관적으로 파란색 노드가 빨간색 노드보다 더 중요할 것 같다라고 생각할 수 있습니다. *왜 그렇게 보일까요?* 아직 노드의 중요성에 대해 정의하지 않았지만 그래프에서 각 노드를 중심으로 뻗어있는 edge(link)의 수가 한눈에 비교되기 때문에 직관적으로 파악할 수 있는 것입니다. 이처럼 웹 그래프의 **link structure**를 가지고 우리는 각 페이지들(node)의 **ranking**을 매길 수 있습니다.

![](https://i.imgur.com/FvyCCna.png?1)

## Link Analysis Algorithms

**각 페이지들의 중요성(importance)**를 파악하기 위해 **Link Analysis**가 필요합니다.

본 수업에서 다룰 Link Analysis 알고리즘들은 아래 총 3개에 대해서 다룰 예정입니다. 

- PageRank
- Personalized PageRank (PPR)
- Random Walk with Restarts

## Links as Votes

**링크가 투표용지**라고 생각해봅시다. 여기서 유향 그래프인 웹 그래프에서 링크는 2가지 종류가 있다는 것을 다시한번 생각해봐야 합니다. 

- `in-comming links(in-links)`: 기준 페이지로 들어오는 방향의 링크
- `out-going links(out-links)`: 기준 페이지에서 나가는 방향의 링크

이렇게 방향까지 고려하여 링크를 투표라고 생각할 때, 엄밀히 말하자면 in-link를 투표라고 생각해야 할 것 입니다. 한 가지 더 생각해볼 문제는 ***모든 in-link들을 동등하게 생각할 수 있는가?***라는 문제입니다. **어떤 링크들**은 다른 링크들에 비해 **좀 더 중요한 페이지로부터(from) 기준페이지로(to)** 온 링크일 수도 있기 때문에 count에 차등을 둬야 하지 않을까라고 생각할 수도 있습니다. 이런 고민들은 결국 페이지들이 서로 연결되어 있어서 recursive한 문제로 볼 수 있습니다.

<aside>
➕ recursive한 문제란, 물리고 물리는 문제로 생각할 수 있습니다. A→B 링크에서 A가 중요한 페이지라는 사실을 기반으로 B가 중요해지고, 이어지는 B→C 링크에서 이 영향을 이어받아 C까지 중요한 페이지라고 판단하게 되기 때문입니다.

</aside>

## PageRank

**The “Flow” Model**

위에서 설명한 recursive한 특성을 기반으로 중요성이 흘러가는(flow) 모델을 생각해볼 수 있습니다. 중요성을 $r$이라는 변수로 두고 기준 노드 **j**의 importance가 어떻게 flow되는지 살펴보겠습니다.

1. **j**로 in-link되어있는 **i, k** 의 importance **$r_i$, $r_k$**를 각 노드의 out-link의 수만큼 나누어서 **j**로 전달됩니다. i 노드의 out-link는 총 3개 이므로 **$\frac{r_i}{3}$**, k노드의 out-link는 총 4개 이므로 **$\frac{r_k}{4}$**로 계산되어 두 값의 합이 **$r_j$**가 됩니다.
2. **$r_j$**는 **j**노드의 out-link를 통해 flow하게 되는데 out-link의 수, 즉 3으로 나누어져 **$\frac{r_j}{3}$** 값이 각각의 **다음 노드들**로 **$r_j$**값이 전달되게 됩니다.

![](https://i.imgur.com/mKnAS4F.png?1)

이처럼 importance가 높은 페이지로부터 in-link된 페이지는 영향을 받아 importance가 높아짐을 알 수 있습니다. **노드 $j$의 rank**, $r_j$를 정의하면 다음과 같이 수식으로 나타낼 수 있습니다. (이때 $d_i$는 노드 i의 out-degree를 말합니다.)

$$
r_{j}=\sum_{i \rightarrow j} \frac{r_{i}}{d_{i}}
$$

다음과 같은 예시에서 각 `기준 노드`를 가지고 `in-link`들을 고려하여 “Flow equation”을 계산해보면 다음과 같다.

![](https://i.imgur.com/PMaaDM4.png?1)

|노드 y|노드 a|노드 m|
|-|-|-|
|y에서 오는 링크 + a에서 오는 링크|y에서 오는 링크 + m에서 오는 링크|a에서 오는 링크|
|$r_y = \frac{r_y}{2} + \frac{r_a}{2}$ | $r_a = \frac{r_y}{2} + r_m$ | $r_m = \frac{r_a}{2}$|

<aside>
➕ 3 Unknowns, 3 Equations 이기 때문에 4번째 constraint로 $r_y + r_a + r_m =1$로 scale관련 constraint를 추가하여 Gaussian elimination을 사용하여 선형방정식으로 풀려고 하는 생각은 좋지 않다. 왜냐하면 importance는 이런식으로 scalable하지 않기 때문이다. (It’s not scalable) 좀 더 정교한 설계가 필요하다.

</aside>

## Matrix Formulation

**Stochastic Adjacency Matrix** $\mathbf{M}$

- $\mathbf{M}$은 $(node의 수)\times (node의 수)$차원의 매트릭스 입니다.
- $i$→$j$ 링크에서 매트릭스 요소 $M_{ji}$는 $\frac{1}{d_i}$가 됩니다. ($d_i$를 노드 $i$의 out-degree라고 정의합니다.)
    
    $$
    M_{ji} = \frac{1}{d_i}
    $$
    
    오른쪽 예시에서처럼 노드 $i$를 기준으로 총 3개의 out-link들이 있다면 각각의 값은 $1/3$이 됩니다.
    
- **column 기준 stochastic** : 열 방향의 모든 값들을 더하면 1이 되는 확률값이 됩니다.

![](https://i.imgur.com/MwU2yRF.png?1)

**Rank Vector** $r$

- $\mathbf{r}$은 각 페이지의 entry 값을 가지는 $(node의 수)\times 1$ 차원의 벡터입니다.
- 각 페이지의 importance score를 $r_i$로 정의합니다.
- 모든 노드의 importance score의 합은 1입니다. 따라서 이 또한 확률값으로 생각할 수 있습니다.

$$
\sum_ir_i = 1
$$

## Flow Equations

이전에 정의했던 노드의 rank 수식을 새롭게 정의한 매트릭스 $M$과 벡터 $r$로 다시 써보면 Flow Equation을 완성할 수 있습니다.

$$
\mathbf{r}=\mathbf{M} \cdot \mathbf{r}
$$

앞서 살펴본 간단한 그래프 예시를 가져와서 flow equation을 매트릭스 연산으로 표현해보면 아래와 같습니다. (flow equation은 앞내용을 참고) 

![](https://i.imgur.com/jCDl5an.png?1)

![](https://i.imgur.com/j0z5Qm7.png?1)

## Connection to Random Walk

다음과 같은 조건을 만족하며 랜덤하게 웹페이지들을 돌아다니고 있는 유저를 생각해보겠습니다. 

1. 시점 $t$에 페이지 $i$에 있습니다.
2. 다음 시점 $t+1$에 페이지 $i$로부터 나가는 방향의 out-link들 중에 uniform하게 선택하여 서핑을 합니다.
3. 앞서 선택된 out-link를 통해 $i$와 연결된 $j$ 페이지에 도달합니다.
4. 이 과정(1~3)을 무한으로 반복합니다.

![](https://i.imgur.com/mFB2J6y.png?1)

여기에서 우리는 `시점` 의 개념을 고려하여 새로운 개념 정의를 하나 할 수 있습니다.

$$
\mathbf{p(t)}
$$

$\mathbf{p(t)}$는 확률 벡터(probability distribution)로, 
이 벡터의 $i$번째 요소는 앞서 가정한 유저가 **시점 $t$에 페이지 $i$에 있을 확률**을 나타냅니다. 

## The Stationary Distribution

앞서 정의한 $\mathbf{p(t)}$를 가지고 이 유저가 시점 **$t+1$에 있을 확률분포는** 다음과 같이 계산합니다.

$$
\mathbf{p(t+1)}=\mathbf{M} \cdot \mathbf{p(t)}
$$

> 💡 만약에 유저가 웹 서핑을 계속하다가  $\mathbf{p(t+1)} = \mathbf{p(t)}$ 같은 상황이 되면 어떨까요?

$$
\mathbf{p(t+1)}=\mathbf{M} \cdot \mathbf{p(t)} = \mathbf{p(t)}
$$

이러한 상황에서는 더 이상 유저가 특정 페이지에 있을 확률이 변하지 않고 유지되는 경우가 되며, 이를 **stationary distribution of a random walk** 라고 합니다.

이러한 형태는 낮설지가 않은데, 앞서 rank vector $\mathbf{r}$가 매트릭스 $\mathbf{M}$과 flow equation을 구성할 때 이러한 꼴이었으며, 따라서 $\mathbf{r}$은 **stationary distribution of a random walk** 입니다.

## Eigenvector Formulation

이전 Lecture 2에서 잠시 배웠던 **eigenvector와 eignvalue**를 생각해보면 다음 수식을 떠올려볼 수 있습니다. 

$$
\lambda \mathbf{c} = \mathbf{A} \mathbf{c}
$$

여기에서 flow equation을 다시 위와 같은 꼴로 작성해보면, 
아래와 같이 eigenvalue가 1이고 eigenvector가 $\mathbf{r}$인 수식으로 해석될 수 있습니다.

$$
1 \cdot \mathbf{r}=\mathbf{M} \cdot \mathbf{r}
$$

따라서 $\mathbf{r}$은 매트릭스 $\mathbf{M}$의 **principle eigenvector**(eigenvalue 1)이며, 
임의의 벡터 $\mathbf{u}$에서 시작해서 계속 매트릭스 $\mathbf{M}$을 곱하여 극한 $\mathbf{M}(\mathbf{M}(...(\mathbf{M}(\mathbf{M}\mathbf{u}))))$으로 도달하게되는 **long-term distribution**이 됩니다. 이러한 방식으로 $\mathbf{r}$을 구하는 방법을 **Power iteration** 이라고 합니다.

## PageRank 정리

- 웹 구조에서 볼 수 있는 link들을 기반으로 node들의 importance를 측정할 수 있다.
- 랜덤하게 웹 서핑하는 유저 모델은 stochastic advacency matrix $\mathbf{M}$으로 나타낼 수 있다.
- PageRank 수식은 $\mathbf{r} = \mathbf{M}\mathbf{r}$ 이며, 
$\mathbf{r}$은 (1) 매트릭스 $\mathbf{M}$의 principle eigenvector, 
(2) stationary distribution of a random walk 2가지로 해석될 수 있다.

---

> Original Lecture Video : [CS224W: Machine Learning with Graphs 2021 Lecture 4.1 - PageRank](https://youtu.be/TU0ankRcHmo)


# 4.2 - PageRank, How to Solve?

이전의 강의에서 Powe iteration 방법으로 반복적인 매트릭스 곱 연산으로 $\mathbf{r}$을 구할 수 있음을 확인했습니다. 이 방법에 대해 조금 더 구체적으로 살펴보겠습니다.

## Power Iteration Method

power iteration은 2가지 표현식이 있는데 하나는 벡터의 요소 관점에서의 업데이트 식(왼쪽)과 다른 하나는 매트릭스 관점의 업데이트 식(오른쪽)으로 나타낼 수 있습니다.

![](https://i.imgur.com/EvHE2Zq.png?1)

**과정**을 살펴보면 다음과 같습니다.

1. 처음 초기화로 모든 노드의 importance score를 똑같은 값으로 만들어 줍니다.(반복적인 연산으로 수렴을 보장하므로 사실 어떤 값으로 초기화하든 상관없습니다.) 
$\boldsymbol{r}^{(0)}=[1 / N, \ldots ., 1 / N]^{T}$
2. 반복적인 연산을 하면서 $\mathbf{r}$ 값을 업데이트합니다.
$\boldsymbol{r}^{(t+1)}=\boldsymbol{M} \cdot \boldsymbol{r}^{(t)}$
3. 수렴조건 $\left\|\boldsymbol{r}^{(\boldsymbol{t}+\mathbf{1})}-\boldsymbol{r}^{(t)}\right\|_{1}<\varepsilon$ 을 만족할 때까지 2번 과정의 연산을 진행합니다.

예시 그래프에서의 power iteration 과정은 다음과 같습니다.

![](https://i.imgur.com/cLqhzSc.png?1)

![](https://i.imgur.com/ucqymgk.png?1)

## Three Questions

1. **Does this converge?** 반복적인 연산과정을 통해 값이 수렴하는가?
2. **Does it converge to what we want?** 수렴한 값이 우리가 원하는 값인가?
3. **Are results reasonable?** 연산 결과가 합당한가?(말이 되는가?)

*(어색한 한국어 번역보다 영어로된 질문에서 얻어가는 insight가 좋을 것 같습니다.)*

## Problems

PageRank에는 2가지의 문제가 있습니다.

1. **Dead Ends**

out-link를 가지지 않는 일부 페이지(노드)들에서 생기는 문제로 이런 페이지들에서 importance가 `leak out` 됩니다. leak out의 `세어나가다` 라는 뜻 그대로 importance flow의 흐름에서 값이 세어나가는 문제를 말합니다.

아래의 예시에서 페이지 b에서 나가는 out-link가 없다보니 importance update를 한 결과가 $r_a = 0, r_b=0$이 됨을 확인할 수 있습니다. 이는 앞서 page rank $\mathbf{r}$ vector의 정의에서 약속한 **모든 노드의 importance의 합이 1이 된다**는 `column stochastic` 수학적 전제에서 벗어난 결과 입니다. 

![](https://i.imgur.com/XGRYKJe.png?1)

1. **Spider traps**

특정 페이지의 모든 out-link들이 다른페이지로 나가지 않아 결국 spider trap 페이지가 모든 importance 값을 독차지하게 됩니다.

아래의 예시에서 a에서 walk를 시작하더라고 b로 이동한 후 b에서 빠져나올 수 없습니다. 이런 경우 importance update 결과 모든 importance를 페이지 b가 가지게 되어 $r_a = 0, r_b=1$이 됩니다. 이런 경우 페이지 a에 아무리 큰 웹 그래프가 연결되어 있다고 하더라도 이동할 수 없습니다. 사실 spider trap은 `column stochastic`을 만족하기 때문에 수학적으로 문제되진 않습니다. 하지만 `우리가 원하지 않는 값에 수렴하는 문제`로 볼 수 있습니다.

![](https://i.imgur.com/cKsXAsY.png?1)

## Solutions

위의 2가지 문제들 모두 **Teleports**로 해결할 수 있습니다.

![](https://i.imgur.com/26GjgNp.png?1)


1. **Dead Ends**를 Teleports로 해결하기

Dead Ends인 m 페이지에서 column stochastic을 만족하지 않고 모든 값이 0이 되지 않도록 자신을 포함한 그래프의 모든 노드들로 uniform random 하게 teleport 이동을 하도록 합니다. 이때 그래프의 노드가 총 3개이므로 m열의 행렬값을 $1/3$으로 채워 $\mathbf{M}$을 완성합니다.
    
![](https://i.imgur.com/bwVrukA.png?1)

1. **Spider Traps**를 Teleports로 해결하기

Spier Trap인 m 페이지에서 다른 노드로 빠져나갈 수 있도록 일정 확률 $1-\beta$만큼 random 페이지로 점핑(teleport)할 수 있도록 합니다. 즉, 확률 $\beta$만큼은 원래 그래프의 out-links 중에 골라서(random) 이동하고 나머지 확률($1-\beta$)로는 out-link와 상관없이 그래프의 모든 페이지들 중에 골라서 이동하여 거미줄, Spider trap에서 벗어나게 되는 것 입니다. 보통 $\beta$값으로는 0.8~0.9값을 사용하는 것이 일반적입니다.
    
![](https://i.imgur.com/wZUsNOu.png?1)
    

## The Google Matrix

PageRank에서 생길 수 있는 2가지 문제를 Teleport로 해결한다면 PageRank Equation은 다음과 같이 바꿀 수 있습니다. **첫번째 항**은 기존의 수식에 있던 부분으로 페이지 $i$의 out-link를 random하게 골라서 이동하는 것에다 확률 $\beta$값을 곱해 보통 0.8~0.9의 확률로 out-link를 통해 이동하게 합니다. 

**두번째 항**은 Teleport를로 out-link와 상관없이 그래프의 모든 페이지들중 하나로 랜덤하게 순간이동하는 것을 수식적으로 표현한 부분입니다. 그래프에 존재하는 모든 페이지의 수를 $N$이라고 할 때, 추가적으로 $1/N$의 확률로  페이지 $j$로 갈 수 있고 이는 앞서 확률 $\beta$를 제외한 나머지 확률, 약 0.2~0.1의 확률로 이동하는 것이므로 $1-\beta$를 곱해줍니다.

$$
r_{j}=\sum_{i \rightarrow j} \beta \frac{r_{i}}{d_{i}}+(1-\beta) \frac{1}{N}
$$

(단, 위의 수식은 $\mathbf{M}$에 dead ends가 없다고 가정하며, 실제로 모든 dead ends를 없애거나 dead ends인 부분들에는 random teleport를 확률1로 따르게 하여 계속 다른 노드로 이동하게 할 수 있습니다.)

**구글 매트릭스**는 이와 크게 다르지 않습니다. 단지 위의 PageRank equation을 **행렬식으로** 바꿔쓰면 구글 매트릭스가 됩니다. 각각의 항들이 의미하는 바는 위에서 설명된 것과 동일하며, 두번째 항의 $\left[\frac{1}{N}\right]_{N \times N}$는 행렬의 모든 원소가 $\frac{1}{N}$으로 채워진 $N \times N$차원의 행렬을 말합니다.

$$
G=\beta M+(1-\beta)\left[\frac{1}{N}\right]_{N \times N}
$$

## Random Teleports ($\beta=0.8$)

아래의 $\beta=0.8$일 때 Random Teleports 예시에서 검은색 선들은 teleports를 적용하지 않았을 때의 그래프의 directed links를 표현하며 초록색 선들은 0.2확률의 teleports가 추가된 부분을 나타냅니다. Power iteration을 통해 계산되면 페이지 $y, a, m$이 각각 $7/33, 5/33, 21/33$으로 수렴하는 것을 알 수 있고 **spider trap인 페이지 m**이 모든 importance를 흡수하지 않는 것을 확인할 수 있습니다.

![](https://i.imgur.com/NwNOgV6.png?1)

## Solving PageRank 정리

- PageRank $\mathbf{r} = \mathbf{G} \mathbf{r}$을 power iteration method로 풀 수 있다.
- PageRank에서 생길 수 있는 문제들인 **Dead Ends와 Spider Traps**를 **Random Uniform Teleportation**으로 해결할 수 있다.

---

> Original Lecture Video : [CS224W: Machine Learning with Graphs 2021 Lecture 4.2 - PageRank: How to Solve?](https://youtu.be/rK2ZBmQHVVs)

# 4.3 - Random Walk with Restarts

## Recommendation

추천 시스템에서 이분그래프(Bipartite graph)로 user와 item의 (구매)관계를 나타낸 **Bipartite User-Item Graph**는 다음 그림과 같습니다. 여기에서 **`특정 item Q를 구매한 user에게 어떤 item을 추천해주는 것이 좋을지`**를 고민한다면, 직관적으로 item Q가 item P와 비슷하게 user들과 관계를 가지고 있을 때 item P를 이 유저에게 추천하는 것이 좋을 것이라고 생각할 수 있습니다. 즉, **item Q와 item P가 얼마나 가까운 관계인지 판단하는 것**이 중요합니다. 

![](https://i.imgur.com/G3dopX8.png?1)

## Node proximity Measurements

노드 근접성(proximity) 측정에 대해 생각해보기 위해 아래의 3가지 케이스를 보겠습니다. `A-A’`은 `B-B’`보다 더 가까운 관계를 가지고 있다고 할 수 있습니다. 왜냐하면 `A-A’` path에서 user을 한번만 거치는데 반해, `B-B’`path에서는 B-user-item-user-B’ 로 path의 길이가 더 길기 때문입니다. 
`A-A’`와 `C-C’`를 비교해보면 `C-C’`이 더 가까운 관계를 가지고 있다고 판단할 수 있는데 그 이유는 `C-C’`이 `A-A’`보다 더 많은 공통의 이웃(Common Neighbors)를 가지고 있기 때문입니다. `C-C’`은 `A-A’`의 shortest path가 2개있는 것으로도 볼 수 있습니다.

![](https://i.imgur.com/H1VOUBR.png?1)

## Proximity on Graphs

이전에 PageRank를 다시 떠올려보면, (1) rank는 node의 “importance”를 정의하며 (2) 그래프의 모든 node들에 균일 분포로 teleport 이동을 할 수 있는 알고리즘이었습니다. 

여기에 좀 더 아이디어를 덧붙여서 **`Personalized PageRank`** 알고리즘을 생각해 볼 수 있습니다. 그래프의 모든 노드들에 대해 균일 분포로 teleport 이동을 하는 것이 아닌, **그래프 노드들의 부분집합(subset) $\mathbf{S}$의 노드들로만** teleport 이동을 하도록 할 수 있습니다. 모든 노드들로 랜덤하게 teleport하지 않고 **좀 더 연관성이 높은 노드들로 teleport할 수 있도록** 하는 것입니다. item Q와 item P가 더 연관성이 높다는 것(Node proxmity ↑)을 어떻게 알 수 있을까요? 이는 **Random Walks**로 확인해볼 수 있습니다.

## Random Walks

item Q가 우리가 알고싶은 item 노드들의 집합인 `QUERY_NODES`집합에 속해있다고 해봅시다. **Bipartite User-Item Graph 상에서** `QUERY_NODES` 집합에 속해 있는 어떤 노드(item Q)에서 시작하여 랜덤하게 움직이면서 과정을 기록합니다. 이 과정을 기록한다는 것은 item↔user 사이를 계속 랜덤하게 움직이면서 방문(visit)하게 된 item 노드에는 +1 count를 하는 것을 의미합니다. 이렇게 랜덤하게 움직이면서 이동을 결정할 때마다 일정 확률 `ALPHA` 만큼 재시작을 하게되는데, 재시작시에는 `QUERY_NODES`집합에 속해 있는 하나의 노드로 이동해서 다시 랜덤하게 움직이기 시작합니다. (아래 pseudo code 참고)

![](https://i.imgur.com/kG31TGo.png?1)

이렇게 계속 Random Walks를 하다보면 item 노드의 visit 수가 높을수록 query item Q와 높은 관계성을 가진것으로 판단할 수 있습니다.

![](https://i.imgur.com/U4WkIHE.png?1)
**Benefits**

이와 같은 Random Walks를 통한 시뮬레이션과 visit 수로 노드들간의 근접성(proximity)을 판단하는데 좋은 이유는 다음과 같은 사항들을 고려하여 similarity를 나타낼 수 있는 방법이기 때문입니다.

- Multiple connnections
- Multiple paths
- Direct and Indirect connections
- Degree of the node

## PageRank Varients 정리

PageRank와 이를 변형한 총 3가지 알고리즘들을 정리하면 다음과 같습니다.

| PageRank | Personalized PR | Random Walk w/ Restarts |
| --- | --- | --- |
| 모든 노드들에 같은 확률로 teleport 이동 | 특정 노드들로 특정 확률을 가지고 teleport 이동 | 항상 똑같은 1개의 노드로 이동 |

![](https://i.imgur.com/GJGKezA.png?1)

---

> Original Lecture Video : [CS224W: Machine Learning with Graphs 2021 Lecture 4.3 - Random Walk with Restarts](https://youtu.be/HbzQzUaJ_9I)

# 4.4 - Matrix Factorization and Node Embeddings

## Recall: Node Embeddings & Embedding matrix

이전 강의에서 배웠던 embedding matrix $\mathbf{Z}$에 대해 다시 떠올려봅시다. 이 매트릭스는 그래프의 각 노드들을 잠재변수 공간(embedding space)으로 encoding하는 행렬로 열의 차원은 embedding하는 크기, 행의 차원은 그래프에 있는 노드의 수가 됩니다. 이 매트릭스의 한 열은 특정 노드 $u$의 embedding vector $\mathbf{z}_u$를 나타내게 됩니다.

![](https://i.imgur.com/Tz0pFbz.png?1)

![](https://i.imgur.com/EdjhAkW.png?1)

이러한 Node embedding에서 objective는 그래프상에서 실제로 유사한 노드들의 simliarity가 embedding vector들의 내적(inner product)값도 높도록 만드는 것입니다.

<aside>
📌 Objective: Maximize $\mathbf{z}_{v}^{\mathrm{T}} \mathbf{z}_{u}$ for node pairs $(u, v)$ that are similar

</aside>

## Matrix Factorization


Embedding matrix를 [Matriz Factorization](https://en.wikipedia.org/wiki/Matrix_decomposition) 관점에서 다시 생각해봅시다. 그래프를 노드들간의 연결이 되어 있으면 1, 아니면 0으로 나타낸 인접행렬 $\mathbf{A}$을 embedding matrix $\mathbf{Z}$로 factorization 한다고 생각해볼 수 있습니다. 즉 $\mathbf{Z}^{\mathrm{T}}$와 $\mathbf{Z}$의 내적으로 인접행렬 $\mathbf{A}$를 만드는 것입니다.

$$
\mathbf{Z}^{\mathrm{T}} \mathbf{Z} = \mathbf{A}
$$

![](https://i.imgur.com/cYKWp52.png?1)

하지만 embedding matrix $\mathbf{Z}$의 행의 수, 즉 embedding dimension $d$는 노드의 수 $n$보다 작으므로 완벽한 factorization을 할 수 없고 대신에 이를 최적화 기법을 사용하여 근접(approzimate)시킬 수 있습니다. 이 최적화를 목적함수는 다음과 같습니다.

$$
\min _{\mathbf{Z}}\left\|A-\boldsymbol{Z}^{T} \boldsymbol{Z}\right\|_{2}
$$

결론은 edge connectivity로 정의된 node similarity을 나타내는 decoder($\mathbf{Z}$)의 내적은 $\mathbf{A}$의 matrix factorization과 동일하다는 것입니다.

## RandomWalk-based Similarity


DeepWalk와 node2vec 알고리즘에서는 random walks를 기반으로한 좀 더 복잡한 node similarity를 사용합니다. 2개의 알고리즘 모두에서 **matrix factorization**을 사용하고 있습니다.  DeepWalk에서 사용하는 node simliarity는 다음과 같이 정의됩니다. (node2vec은 이보다 조금 더 복잡합니다. 자세한 내용을 확인하고 싶으면 [Network Embedding as Matrix Factorization paper](https://arxiv.org/abs/1710.02971)를 참고)

![](https://i.imgur.com/Ny6RVJr.png?1)

## Limitations


Matrix factorization과 random walk로 node embedding을 할 경우 몇가지 제약(단점)이 있습니다.

1. **그래프에 새로운 노드가 생겼을 때 대응하지 못합니다.** training과정에서 보지 못한 노드가 생겼을 때 scratch부터 다시 계산해야 합니다.

![](https://i.imgur.com/YfxHzOz.png?1)

1. **구조적인 유사성을 파악하지 못합니다.** 아래의 그림에서 `1-2-3`과 `11-12-13`은 그래프에서 비슷한 구조를 가지고 있지만 각 노드마다 unique한 embedding 값으로 인해 구조적인 유사성을 파악하지 못합니다.

![](https://i.imgur.com/Zu8UBla.png?1)

1. **노드, 엣지, 그래프의 feature 정보를 활용할 수 없습니다.** DeepWalk나 node2vec에 쓰인 embedding은 노드에 있는 feature 정보를 활용할 수 없습니다. 이는 추후에 배울 `Deep Representation Learning`으로 해결할 수 있습니다.

![](https://i.imgur.com/31Jj1Pb.png)


## Algorithms 정리

- PageRank: 그래프에서 노드의 importance를 측정하는 알고리즘이며 인접행렬의 power iteration으로 계산할 수 있다.
    - 총 3가지 관점에서 해석할 수 있다. 
    - (1) Flow formulation 
    - (2) Random walk & Stationary distribution 
    - (3) Linear Algebra - eigenvector
- Personalized PageRank(PPR): PageRank에서 좀 더 발전시킨 알고리즘으로 random walk로 구한 특정 노드의 중요성을 더 고려하여 teleport를 하는 알고리즘
- Random walks 기반 Node Embeddings은 Matrix factorization으로 표현될 수 있다.

> 그래프를 행렬로 이해하는 것은 위의 알고리즘들을 이해하는데 매우 중요하다는 것을 알 수 있습니다.
> 

---

> Original Lecture Video : [CS224W: Machine Learning with Graphs 2021 Lecture 4.4 - Matrix Factorization and Node Embeddings](https://youtu.be/r12qJZZVtqc)