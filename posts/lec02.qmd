---
toc: true
title: "Lecture 2"
description: Traditional Methods for Machine Learning in Graphs
author: "JooHo Kim"
date: "2022-07-13"
categories: [LEC02]
---

# Lecture2 정리


2강에선 Graph를 이용한 Traditional ML 방법론들에 대해 설명한다.


### 💡Traditional ML Pipeline

***Traditional ML Pipeline***은 크게 2단계로 이루어져 있다.

1. Data Point, Node, Link, Graph(이하 **입력**)를 Feature Vector로 변환해 ML모델을 학습시킨다.
2. 새로운 **입력**이 들어오면 Feature Vector를 얻고 모델을 통해 예측한다.

---

### 💡 Lecture Object : Feature Design

---

`Goal` : **입력** Set이 주어졌을때 예측값을 만들어내는것 / 모델은 ML Model 사용

> Design Choice
> 
- Feature : $d$차원의 벡터
- 입력 : Nodes, Links, Sets of Nodes, Entire Graph
- Objective Function : 무슨 Task를 풀려고 하는가?

---

1. Traditional ML Pipeline은 수작업으로 만들어진(Hand-Designed) Feature를 사용한다.
2. Hand Designed Feature를 Graph의 세 레벨 (Node, Link, Graph)로 나누어 설명한다.
3. Undirected Graph를 중점으로 설명한다.

---

## 🔴 Traditional Feature-Based Method : Node


> **Goal : Network에서 Node의 구조와 위치를 특정할 수 있는 Feature를 만드는 것**


### 💡 1. Node Degree ($k_v$)

Node $v$ 의 Degree를 $k_v$ 라고 정의하자. 이때 $k_v$ 는 $v$ 가 갖고있는 Edge(Link)의 수와 같다.

![](https://i.imgur.com/bRvU9Xz.png)



### 💡 2. Node Centrality(**$c_v$**)

Node Degree는 단순히 이웃한 Node의 갯수를 세므로, 그것들의 중요도를 Capture할 수 없다. 

Node Centrality($c_v$)는 Graph에서 해당 Node( $v$)의 중요도를 포함시킨 개념이다. 

- **2-1. Engienvector centrality**
    - `Important` : $v$가 *Important* 이웃노드 $u$에 둘러싸여 있을 때 $v$는 *Important*하다고 한다.
    - `Formula`
        - $c_v = {1 \over \lambda} \sum\limits_{u\in N(v)}c_u$ ($\lambda$는 Normalization 상수) ⇒ 이렇게 하면 Recursive함
        - $\lambda c= Ac$ ($A$는 Adjacency Matrix)
            - 고유값과 고유벡터 형태로 재설정
            - $c$는 $A$의 고유벡터, $\lambda$는 고유값이며 $\lambda_{max}$는 항상 양수에 Unique함
- **2-2. Betweenness centrality**
    - `Important` : $v$가 다른 노드들을 연결하는 최단 경로에 있을때 *Important*하다고 한다.(경유)
    - `Formula`  : $c_v = \sum\limits_{s \neq v \neq t}{v를\ 포함하는 s와\ t사이 \ 최단\ 경로 \over s와\ t사이\ 최단\ 경로}$
        
        ![](https://i.imgur.com/xh0zsGH.png)
        
- **2-3. Closeness Centrality**
    - `Important` :  $v$가 다른 모든 노드에 대한 최단 경로의 길이가 짧을때 *Important*하다고 한다.
    - `Formula` : $c_v = 1 \div  \sum\limits_{u \neq v} (u와\ v사이의\ 최단경로의\ 길이)$
        
        ![](https://i.imgur.com/QA0e7Hi.png)
        

---


### 💡 3. Clustering Coefficient

***Clustering Coefficient***는 Node $v$의 이웃들이 얼마나 연결되어 있는지를 측정하는 개념이다.

$v$의 이웃간 연결된 경우의 수를 이웃 Node들이 서로 연결될 수 있는 전체 경우의 수로 나누어준다.

![](https://i.imgur.com/WSxpVTm.png)

![](https://i.imgur.com/g7wDAiK.png)

---

### 🤔 4. Graphlets***


---

`Observation` : Clustering Coefficient는 Ego-Network의 #(Triangle)을 센다)

- Ego-Network : Node가 주어졌을때 자기자신과 1차-이웃만 포함한 Network
- #(Triangle) : 3개의 노드가 연결되어 있는 것
- 이런 Triangle Counting을 다양한 구조에 대해 일반화 하는것 ⇒ Graphlets의 개념

![](https://i.imgur.com/oANu76X.png)

---

- Graphlet의 목적 : Node $u$의 이웃 구조를 기술하는 것
    - Graphlets : $u$의 이웃 구조를 기술하기 위한 작은 Subgraph(Template?)
        
        ![](https://i.imgur.com/Rih91Sq.png)
        

---

> ***Graphlet Degree Vector(GDV)*** : Node의 Graphelt-Based Feature
> 
- Degree of Graphlet : 특정 Node가 포함된 Graphlet의 갯수 벡터이다. 
어떻게 세는지는 아래의 예시를 통해 설명한다.

---

1. 아래와 같이 생긴 Graph $G$에서 Node $u$에 관심있다고 가정해 보자.
    
    ![](https://i.imgur.com/6wbUZqm.png)
    
2. Graph 구조를 보았을때, 최대 3개의 Node가 참여하는 Graphlet을 만들 수 있다.
    
    ![](https://i.imgur.com/3uIYSMl.png)
    
3. 각각의 Graphlet이 $G$에서 $u$를 포함한채로 몇번 나타나는지 세보자

    ![](https://i.imgur.com/3HqOtKv.png)

4. Node $u$의 GDV는 [2,1,0,2]가 된다.

> ***Graphlet Summary*** 

    2~5개의 Node가 참여하는 Graphlet의 갯수는 73개이다. 이를 73차원의 벡터로 표시할 수 있고, 각 Index는 특정한 Neighborhood Topology에 Signature이다. 이 벡터를 이용해 Node의 Local Network Topology를 잘 정제된 Feature로 만든게 GDV이며, 앞에서 소개한 방식보다 자세한 정보를 갖고있다.


### 💡 Node-Level Feature Summary


    Node Level Feature는 2가지 분류로 나눌수 있다

    1. Importance Based (Ex Task : 영향력있는 Node찾기(SNS의 셀럽찾기))
        1. Node Degree : 단순히 이웃의 숫자를 센다
        2. Node Centrality : Graph에서의 이웃 노드의 중요도를 모델링한다.
    2. Structure Based (Ex Task : Node의 역할 찾기(단백질 구조에서 특정 단백질의 기능찾기))
        1. Node Degree : 단순히 이웃의 숫자를 센다
        2. Clustering Coefficient : 이웃이 어떻게 연결되어있는지 측정한다.
        3. Graphlet Count Vector : 여러 Graphlet들이 출현하는 빈도를 센다.

---

## 🔗 Traditional Feature-Based Method : Link

---


### 💡 Link-Level Prediction Task

Link-Level Task는 존재하는 Link를 바탕으로 새로운 Link를 예측하는 것이다. Link를 예측하는 Task는 크게 2가지 Formulation이 있다.

> ***1. 랜덤하게 사라진 Link 찾기 :*** Static한 Graph에 적절하다.

> ***2. 시간이 흐름에 따라 생겨나는 Link 찾기 :*** SNS, Transaction같이 Dynamic한 Graph에 적절하다.
 

Link-Level Prediction의 자세한 방법은 다음과 같다.

1. 각 Node쌍 ($x,y)$에 score $c(x,y)$를 계산한다.
2. $c(x,y)$ 내림차순으로 Node 쌍을 정렬한다
3. Top $N$개의 Pair들을 새로운 Link로 예측한다.

---


### 💡 Link - Level Feature : Distance-Based Features

</aside>

> ***1. 노드간 최단 경로***
> 

두 Node간 최단경로의 거리를 사용한다. 이웃의 수나 강도에 대한 어떠한 정보도 캡쳐하지 않는다.

> ***2. Local Neighborhood Overlap***
> 

두 Node가 공유하는 이웃을 캡쳐한다.

- **Coomon Neighbors** : 단순히 교집합을 구한다
- **Jaccard’s Coefficient** : 교집합의 크기를 합집합으로 나눈다
- **Adamic-Adar Index** : (SNS에서 잘 동작한다고 하네요)
두 Node가 공유하는 이웃을 u라고 할 때 $\sum \limits_u {1\over log(k_u)}$

> ***3. Global Neighborhood Overlap***
> 

Local Neighborhood Overlap의 단점은 잠재적 이웃도 직접적인 공통 이웃이 없으면 0이 된다는 점이다.

![](https://i.imgur.com/xWSmHWZ.png)

- **Katz Index** : 주어진 Node 쌍을 잇는 모든 길이의 경로를 센다. Matrix를 이용해 깔끔하게 연산할수 있다.
    - $A_{uv}$는 직접 이웃일 때 1이고 아니면 0이다.
    - $P_{uv}^{(K)}$는 $K$길이의 $u,v$를 잇는 경로이다.
    - $P^{(K)}$ = $A^k$이다.
    - `Formula`
        - $S_{uv} = \sum \limits_{l=1}^\infty
        \beta^l A_{uv}^{l}$ ($l$ : Path의 길이, $\beta$: Discount Factor($0<\beta<1$)
        - $S_{uv} = \sum \limits_{l=1}^\infty \beta^lA^i=(I-\beta A)^{-1}-I$ (Closed-Form)

### 💡Link-Level Feature Summary


    Link Level Feature는 3가지 분류로 나눌수 있다

    1. Distance-Based :두 Node간 최단경로의 거리
    2. Local Neigborhood Overlap : 두 Node가 공유하는 이웃의 수
    3. Global Neighborhood Overlap : 두 Node를 잇는 모든 길이의 경로 가중합

---

## ⛓ Traditional Feature-Based Method : Graph

---

> **Goal : 전체 Graph 구조를 특정할 수 있는 Feature를 만드는 것**
> 
- 아이디어 : Graph로 Feature를 직접 만드는 대신 Kernel을 만들자.
    - Kernel $K(G,G') \in \R$ 은 두 Graph$(G)$ 사이의 유사도를 측정한다.
    - Kernel Matrix는 항상 양의 고유값을 갖고 대칭행렬 이어야한다.
    - Feature Representaiton $\phi(.)$이 존재한다.
    - 이 Kernel을 SVM등에 붙여서 사용한다.

---


### 💡 Kernel Method

- **Goal :** Graph Feature Vector $\phi(G)$를 설계한다
- **Idea :** Graph에 대해 Bow를 만든다.
    - *Bow* : NLP에서 모든 단어가 몇 번 나타나는지 세는 방법
    - *Naive Solution* : Node를 Word로 사용한다. 그러나 너무 Naive해서 써먹기 어렵다.
        
        ![](https://i.imgur.com/c7WNp0e.png)
        
    - *Node Degrees* : Node Degree를 Word로 사용한다.
        
        ![](https://i.imgur.com/nLC4jnv.png)
        
    - 이런식의 Bag-of-something 방식이 Graphlet Kernel과 WL Kernel에서도 사용된다.

---


### 💡 Grahplet Features


- **Idea** : Graph에 존재하는 서로 다른 Graphlet의 숫자를 세자
- **Note :** 이때의 Graphlet은 Node-Level과 조금 다른 정의를 갖고있다.
    - Isolated Node로 Graphlet의 일부로 허용한다.
    - Root Node가 없다.
    
    ![](https://i.imgur.com/RCZYqWd.png)
    
- Graph $G$와 Graphlet list $g_k = (g_1,g_2 ...g_{nk})$가 주어졌을 때
Graphlet Count Vector $f_G \in \R^{nk}$ 는 Graph에서 나타나는 각 Graphlet의 인스턴스 수로 정의된다.
    - $(f_G)_i = \#(g_i \in G)$ | (for $i = 1,2,...n_k)$
        
        ![](https://i.imgur.com/15aKkBh.png)
        


### 💡 Graphlet Kernel

- 2개의 Graph $G$ 와 $G'$가 주어지면, Graphlet Kernel은 $K(G,G') = {f_G}^Tf_{G'}$로 표현될 수 있다(내적)
- `Problem` : $G$ 와 $G'$가 크기(Scale)이 다르면 값이 크게 왜곡된다.
- `Solution`: $f_G$  대신 Sum으로 나눠준 $h_G$를 사용한다.  $h_G = {f_G \over Sum(f_G)}$
- `Limitation` : Graphlet을 세는 연산이 매우 Expensive하다 !
    - $n$ 크기의 Graph의 $k$ 크기의 Graphlet를 세려면 $n^k$번 연산해야 한다.

---


### 💡 Weisfeiler-Lehman(WL) Kernel

- **Goal :** 효율적인 Graph Feature Descriptor를 만드는 것
    - WL-Kernel은 강력하고 효율적이어서 인기가 많다.
- **Idea :** Node Degree를 이용해 반복적으로 Node Vocap을 풍부하게 만들어 나가는 것
    - One-Hop Neighborhood인 Node Degree 방식을 일반화 한 버전이다.
    - ***Color Refinement*** 알고리즘을 통해 이루어진다.
- **각 Step에서의 Time-Complexity가 Edge에 따라 Linear하게 증가한다.**


### 💡 Color Refinement

- `Given` : Graph $G$와 그것들의 Set of Nodes $V$
    
    1. Initial Color $c^{(0)}(v)$를 각 노드 $v$에 할당한다
    2. Iteratively하게 Node의 Color를 정제해 나간다.
    $c^{(k+1)}(v) = HASH(\{c^{(k)}(v), \{c^{(k)}(u) \}_{u\in N(v)})$
        - HASH는 다른 입력을 다른 Color로 매핑하는 연산이다.
    3. $K$ Step 동안의 정제가 끝나면 $c^{(K)}(v)$ 값을 Summary한다.
    
    ---
    
    - 비슷하지만 조금 다른 Graph 두개 ($G_1, G_2$)가 주어졌을때 Color Refienment 예시이다
    
        1. 동일한 Initial Color를 모든 Node에 할당한다.
    
            ![](https://i.imgur.com/MbNlENL.png)
    
        2. 이웃하는 색상에 대해 Aggregate한다.
        
            ![](https://i.imgur.com/qsIdHIv.png)
        
        3. Aggregate된 Color를 HASH한다.
        
            ![](https://i.imgur.com/ARtbUYJ.png)
        
        4. 이웃하는 색상에 대해 Aggregate한다.
        
            ![](https://i.imgur.com/54MxJxX.png)
        
        5. Aggregate된 Color를 HASH한다.
        
            ![](https://i.imgur.com/jzDMsuL.png)
        
        6. Color Refinement가 끝나면 WL Kernel이 각 Color가 등장했던 횟수를 세서 Summary한다.
        
            ![](https://i.imgur.com/l2oEW8J.png)
        
        7. Color Count Vector를 내적해 WL Kernel의 결과값을 구한다.
        
            ![](https://i.imgur.com/XXN2ldt.png)
        

---


### 💡 ***Graph-Level Feature Summary***

    Graph Level Feature는 Kernel을 이용한다.

    1. Graphlet Kernel :Bag-of-Graphlets, Computationally Expensive
    2. WL- Kernel :
        - Color-Refinement 알고리즘을 이용해 반복적으로 피팅
        - Bag-of-Colors
        - Computationally Efficient !
        - Closely related to Graph Neural Networks
