---
toc: true
title: "Lecture 3"
description: Node Embeddings
author: "YoojinKim"
date: "2022-07-13"
categories: [LEC03]
---

## 3.1 Node Embedding

### Graph Representation Learning

**representation learning ëª©ì **ì€ feature engineeringì„ í†µí•´ ì§ì ‘ nodeì˜ featureë¥¼ ì§€ì •í•˜ëŠ” ëŒ€ì‹ , **featureë¥¼ ìë™ìœ¼ë¡œ í•™ìŠµ**í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ê·¸ë¦¼ì—ì„œì™€ ê°™ì´, representation learningì„ ê±°ì³ node uë¥¼ dì°¨ì› vectorë¡œ í‘œí˜„í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ë•Œ, featureë¥¼ í‘œí˜„í•˜ëŠ” vectorë¥¼ **â€˜feature representationâ€™ í˜¹ì€ â€˜embeddingâ€™**ì´ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤. ì´ vectorëŠ” nodeì˜ íŠ¹ì§•ì„ ì˜ ë‹´ì•„ë‚´ì•¼ í•˜ë©°, ê·¸ë˜í”„ì˜ ì „ë°˜ì ì¸ êµ¬ì¡°ì˜ ì˜ë¯¸ë„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. 

![](../images/lec03/Untitled.png)

### Why Embedding?

**embedding spaceë‚´ì— í‘œí˜„ëœ nodeë“¤ì€ ìœ ì‚¬ì„±(similarity) ê°€ì§ˆìˆ˜ë¡ ë¹„ìŠ·í•œ embeddingì„ ê°€ì§„ë‹¤**ëŠ” ê²ƒì´ í•µì‹¬ì…ë‹ˆë‹¤. ì´ë ‡ê²Œ í‘œí˜„ëœ nodeë“¤ì€ í›„ì† ì‘ì—…ì¸ ì˜ˆì¸¡ task (node classification, link prediction, graph classification ë“±)ì— íš¨ê³¼ì ìœ¼ë¡œ í™œìš©ë©ë‹ˆë‹¤. 

### Example Node Embedding

DeepWalk ì˜ ì—°êµ¬ë¡œ, Zacharyâ€™s Karate Club networkë¥¼ 2ì°¨ì› vectorì˜ featureë¡œ í‘œí˜„í–ˆìŠµë‹ˆë‹¤. **ê°™ì€ ìƒ‰ìƒì„ ê°€ì§„ nodeë¼ë¦¬ ë¹„ìŠ·í•œ vector ê°’ì„ ê°–ëŠ” ê²ƒ**ì„ í™•ì¸í•  ìˆ˜ ìˆìœ¼ë©°, ì´ë¥¼ í†µí•´ embedding ì‘ì—…ì´ ê½¤ ì„±ê³µì ìœ¼ë¡œ ì´ë£¨ì–´ì¡Œë‹¤ê³  íŒë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

![](../images/lec03/Untitled 1.png)

### Node Embeddings: Encoder and Decoder

ìš°ì„  ê°„ë‹¨í•œ ê·¸ë˜í”„ì—ì„œ ì‹œì‘í•´ë´…ì‹œë‹¤!

featureëŠ” ì—†ê³ , ì—°ê²° ê´€ê³„ë§Œ(adjacency matrix)ë§Œ ì¡´ì¬í•˜ëŠ” undirected ê·¸ë˜í”„ê°€ ìˆë‹¤ê³  í•©ì‹œë‹¤. ì´ë•Œ embedding ëª©í‘œëŠ” **embeddingí›„ì˜ embedding spaceì—ì„œì˜ ê° nodeë¼ë¦¬ì˜ ìœ ì‚¬ì„±(similarity)ê³¼ ê·¸ë˜í”„ìƒì—ì„œì˜ ìœ ì‚¬ì„±**ì´ ë¹„ìŠ·í•´ì§€ëŠ” ê²ƒì…ë‹ˆë‹¤. **embedding spaceì—ì„œì˜ ìœ ì‚¬ì„±ì„ ì¸¡ì •í•  ë•ŒëŠ” vectorê°„ì˜ dot product ì—°ì‚°ì„ ìˆ˜í–‰**í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤. **dot product**ì€ $a \cdot b = |a||b|cos\theta$  ì´ë¯€ë¡œ, **ë‘ vectorê°„ì˜ ê°ë„ê°€ ì‘ì„ìˆ˜ë¡(=ê°€ê¹Œì›€, ë¹„ìŠ·í•¨), í° ê°’**ì„ ê°–ê²Œ ë©ë‹ˆë‹¤. ì ì´ì œ, embedding spaceì—ì„œì˜ ìœ ì‚¬ì„±ì€ ì¸¡ì •í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìœ¼ë¯€ë¡œ, **ê·¸ë˜í”„ìƒì—ì„œì˜ ìœ ì‚¬ì„±ì„ ì¸¡ì •í•  ìˆ˜ ìˆëŠ” similarity functionì„ ì •ì˜**í•´ì•¼ í•©ë‹ˆë‹¤. 

> **Goal :** $\red {similarity(u,v)} \;\approx\; \green{z^T_v z_u}$
> 

![](../images/lec03/Untitled 2.png)

### Learning Node Embeddings

**1) Encoderë¥¼ í†µí•´ nodeë¥¼ embeddingê°’ìœ¼ë¡œ ë³€í™˜**í•©ë‹ˆë‹¤.
     Encoder : $ENC(v) = z_v$,   $z_v : d$-dimensional embedding
     ì´ë•Œ, embedding ì°¨ì›ìœ¼ë¡œ ë³´í†µ 64~1000ì„ ì±„íƒí•©ë‹ˆë‹¤. 

**2)** **ê·¸ë˜í”„ ìƒì—ì„œì˜ ë…¸ë“œ ê°„ ìœ ì‚¬ì„±ì„ ì¸¡ì •í•  similarity functionì„ ì •ì˜**í•©ë‹ˆë‹¤. 

**3) DecoderëŠ” embeddingê°’ë“¤ ê°„ì˜ ìœ ì‚¬ì„±ì„ ì¸¡ì •**í•©ë‹ˆë‹¤. â†’ similarity score
     Decoder : $DEC(z^T_vz_u)$

**4) $\red {similarity(u,v)} \;\approx\; \green{z^T_v z_u}$ê°€ ë  ìˆ˜ ìˆë„ë¡, Encoderì˜ parameterë“¤ì„ ìµœì í™”** í•©ë‹ˆë‹¤. 

### Shallow Encoding

ê°€ì¥ ê°„ë‹¨í•œ encoderëŠ” ë‹¨ìˆœí•œ **embedding-lookup(â†’ì¡°íšŒ)**ì…ë‹ˆë‹¤. 

> $ENC(v) = z_v= Z \cdot v$ ,      $Z\in\R^{d*|v|}$, $v\in I^{|v|}$
> 

ì—¬ê¸°ì„œ $**Z$ëŠ” ëª¨ë“  nodeì˜ embeddingì„ í¬í•¨í•˜ëŠ” matrixë¡œ ê° columnì´ í•˜ë‚˜ì˜ node embeddingì„ ì˜ë¯¸**í•©ë‹ˆë‹¤. $v$ëŠ” indicator vectorë¡œ, í˜„ì¬ í‘œí˜„í•  nodeë§Œ 1ì´ê³  ë‚˜ë¨¸ì§€ëŠ” 0ì¸ vectorì…ë‹ˆë‹¤. **ì´ë•Œì˜ ëª©í‘œëŠ” matrix $Z$ë¥¼ ìµœì í™”**í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë‹¤ìŒ ë°©ë²•ì„ ì´ìš©í•œ ëŒ€í‘œì ì¸ ì•Œê³ ë¦¬ì¦˜ì€ **DeepWalkì™€ node2vec**ì´ ìˆìŠµë‹ˆë‹¤. 

![](../images/lec03/Untitled 3.png)

 

í•˜ì§€ë§Œ ì´ë•Œì˜ **ë¬¸ì œì ì€ ìµœì í™”í•´ì•¼ í•  embedding matrix $Z$ì˜ parameterìˆ˜ê°€ ë§¤ìš° ì»¤ì§ˆ ìˆ˜ ìˆë‹¤ëŠ” ì **ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 10ì–µ ê°œ nodeë¡œ êµ¬ì„±ëœ ê·¸ë˜í”„ì— ëŒ€í•´ì„œ ìµœì í™” í•´ì•¼ í•  parameterëŠ” (10ì–µ * embedding dimension)ê°œ ì…ë‹ˆë‹¤. ë”°ë¼ì„œ í•´ë‹¹ ë°©ë²•ì€ ë‹¨ìˆœ lookup ìœ¼ë¡œ ë§¤ìš° ê°„ë‹¨í•  ìˆ˜ ìˆì§€ë§Œ, í™•ì¥ ê°€ëŠ¥ì„±ì´ ì‘ìŠµë‹ˆë‹¤.(= low scalability)

(vs. deep encoder(GNNs)ëŠ” 6ì¥ì—ì„œë¶€í„° ë‹¤ë£° ì˜ˆì •ì…ë‹ˆë‹¤ğŸ™‚)

### Note on Node Embeddings

node embeddingì„ ì°¾ëŠ” ê²ƒì€ unsupervised/self-supervised ë°©ì‹ìœ¼ë¡œ êµ¬ë¶„ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ„ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ **node label, node feature ë“±ì„ ì´ìš©í•˜ì§€ ì•Šê³ , graph networkì˜ êµ¬ì¡°ë¥¼ ë³´ì¡´í•œ ì±„ node embeddingì„ ì°¾ê¸° ë•Œë¬¸**ì…ë‹ˆë‹¤. ë”°ë¼ì„œ, í•´ë‹¹ ë°©ë²•ì€ **task independentë¡œ, ì–´ë–¤ taskë“  ì ìš© ê°€ëŠ¥**í•©ë‹ˆë‹¤. 

---

## 3.2 Random Walk Approaches for Node embeddings

ë³¸ê²©ì ì¸ node embedding ë°©ë²• ì†Œê°œì— ì•ì„œ, **ìš©ì–´**ë¶€í„° ì‚´í´ë´…ì‹œë‹¤!

- **Vector $z_u$   :** node uì˜ **embedding â†’ ìš°ë¦¬ê°€ ì°¾ê³ ì í•˜ëŠ” ê²ƒ.**
- **Probability $P(v|z_u)$ :** node uì—ì„œ ì‹œì‘í•´ì„œ random walkë¡œ node vì— ë°©ë¬¸í•  í™•ë¥ 
- **Non-linear function :**

(1) softmax 

ì…ë ¥ë°›ì€ ê°’ì„ ì¶œë ¥ìœ¼ë¡œ 0~1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ëª¨ë‘ ì •ê·œí™”í•˜ë©° ì¶œë ¥ê°’ë“¤ì˜ ì´í•©ì€ í•­ìƒ 1ì´ ë˜ëŠ” íŠ¹ì„±ì„ ê°€ì§„ í•¨ìˆ˜ë¡œ, ë¶„ë¥˜í•˜ê³  ì‹¶ì€ class ìˆ˜ë§Œí¼ ì¶œë ¥ìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤. ê°€ì¥ í° ì¶œë ¥ê°’ì„ ë¶€ì—¬ë°›ì€ classê°€Â í™•ë¥ ì´ ê°€ì¥ ë†’ìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

(2) sigmoid function

ëª¨ë“  ì‹¤ìˆ˜ ì…ë ¥ ê°’ì„ 0ë³´ë‹¤ í¬ê³  1ë³´ë‹¤ ì‘ì€ ë¯¸ë¶„ ê°€ëŠ¥í•œ ìˆ˜ë¡œ ë³€í™˜í•˜ëŠ” íŠ¹ì§•ì„ ê°–ìŠµë‹ˆë‹¤.

- **Random Walk :**

node 4ì—ì„œ ì‹œì‘í•œë‹¤ë©´, **ì´ë™ ê°€ëŠ¥í•œ neighbor nodeì¤‘ randomìœ¼ë¡œ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì—¬ ì´ë™**í•©ë‹ˆë‹¤. ì´í›„, **ê³ ì •ëœ ìˆ˜ë§Œí¼ ë°˜ë³µì ìœ¼ë¡œ ì´ë™**í•˜ë©´ ë©ë‹ˆë‹¤. random walkë¥¼ ìˆ˜í–‰í•˜ë©´ ê·¸ë˜í”„ìƒì—ì„œ ì´ë™í•œ ì¼ë ¨ì˜ nodeë“¤ì´ ê²°ê³¼ë¡œ ë„ì¶œë©ë‹ˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì—ì„œëŠ” **{4,5,8,9,8,11}** ì…ë‹ˆë‹¤. 

![](../images/lec03/Untitled 4.png)

random walkì—ì„œì˜ $**z^T_v z_u$ëŠ” node uì™€ node vê°€ random walk ì¤‘ ë™ì‹œì— ë°©ë¬¸ ë  í™•ë¥ **ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. random walk ì „ëµ $R$ì´ ìˆë‹¤ê³  í•  ë•Œ, node uì—ì„œ ì‹œì‘í•˜ì—¬ ì „ëµ $R$ì— ë”°ë¼ random walkë¥¼ ìˆ˜í–‰í–ˆì„ ë•Œ, node vë¥¼ ë°©ë¬¸í•  í™•ë¥ , $P_R(v|u)$ ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ì´í›„, ë‘ vector ì‚¬ì´ì˜ ê°ë„ $\theta$ê°€ $P_R(v|u)$ì— ë¹„ë¡€í•˜ë„ë¡ embedding ì„ ìµœì í™”í•©ë‹ˆë‹¤.  

 

![](../images/lec03/Untitled 5.png)



### Why Random Walks?

**ì¥ì  1) Expressivity**

local(ë³¸ì¸)ë¿ë§Œ ì•„ë‹ˆë¼, higher-order neighborhood(ì—¬ëŸ¬ hop ë–¨ì–´ì§„ ì´ì›ƒ)ë“¤ì˜ ì •ë³´ë¥¼ í¬í•¨í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. 

**ì¥ì  2) Efficiency**

ëª¨ë“  nodeë¥¼ í•œêº¼ë²ˆì— ê³ ë ¤í•˜ì§€ ì•Šê³ , random walkë¥¼ í†µí•´ ë°©ë¬¸í•œ nodeë“¤ì˜ ìŒë§Œ ê³ ë ¤í•˜ì—¬ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Unsupervised Feature Learning

ë‹¤ì‹œ unsupervised feature learningì—ì„œì˜ ëª©ì ìœ¼ë¡œ ëŒì•„ì™€ ë³´ë©´, ìœ ì‚¬ì„±ì„ ë³´ì¡´í•œ ì±„ node embeddingì„ ì°¾ê³ ì í•©ë‹ˆë‹¤. **ì„¤ë“ë ¥ ìˆëŠ” node embeddingì´ ë˜ê¸° ìœ„í•´ì„œ ê°€ê¹Œìš´(nearby) ë…¸ë“œë¼ë¦¬ ë¹„ìŠ·í•œ embeddingì„ ê°–ë„ë¡ í•´ì•¼ í•©ë‹ˆë‹¤.**

ì ê·¸ëŸ¼ **â€˜ê°€ê¹Œìš´â€™ì€ ì–´ë–»ê²Œ ì •ì˜**í•  ìˆ˜ ìˆì„ê¹Œìš”?? ì—¬ê¸°ì„œ random walkê°€ ë“±ì¥í•©ë‹ˆë‹¤!! ë°”ë¡œ **â€˜random walkë¡œ ë°©ë¬¸í•˜ê²Œëœ ì´ì›ƒë“¤ì„ â€˜ê°€ê¹ë‹¤'ë¼ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.**

> $N_R(u)$  : random walk ì „ëµ $R$ì— ë”°ë¼ node uì—ì„œ ì¶œë°œí•˜ì—¬ ë°©ë¬¸í•˜ê²Œëœ ì´ì›ƒë“¤(neighborhood)
> 

 

### Feature Learning as Optimization

ê·¸ë˜í”„ $G = (V,E)$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ,

**node embedding function, $f: u \rightarrow \R^d : f(u) = z_u$ ì„ ìµœì í™”**í•´ì•¼í•©ë‹ˆë‹¤. 

ì´ë¥¼ ìœ„í•´ì„œ, **log-likelihood objectiveëŠ” $max_f \sum_{u\in V}\log P(N_R(u)|z_u)$** ìœ¼ë¡œ, í•´ë‹¹ ê°’ì´ ìµœëŒ€í™” ë˜ëŠ” í•¨ìˆ˜ $f$ë¥¼ ì°¾ì•„ì•¼í•©ë‹ˆë‹¤.

### Random Walk Optimization

**Step 1)** random walk strategy $R$ì— ë”°ë¼ì„œ ê° node uì—ì„œ ê³ ì •ëœ í¬ê¸°ì˜ ì§§ì€ random walkë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

**Step 2)** ê° node ë§ˆë‹¤ multiset ì´ì›ƒ ì§‘í•©, $N_R(u)$ì„ ëª¨ìë‹ˆë‹¤.
*multiset : random walkë™ì•ˆ íŠ¹ì • ë…¸ë“œë¥¼ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì¤‘ë³µëœ ì›ì†Œë¥¼ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

**Step 3)** node uê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ì´ì›ƒë“¤ $N_R(u)$ ì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡  embeddingì„ ìµœì í™”í•©ë‹ˆë‹¤. 

$max_f \sum_{u\in V}\log P(N_R(u)|z_u)$

ìœ„ ì‹ì€  ëª¨ë“  node u, ê·¸ë¦¬ê³  ê·¸ ê°ê°ì˜ ì´ì›ƒ ë…¸ë“œ vì— ëŒ€í•œ ì‹ìœ¼ë¡œ í’€ì–´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> $L =  \sum_{u\in V}\sum_{v\in N_R(u)} -\log(P(v|z_u))$,
> 

ë˜í•œ, **node uì— ê°€ê¹Œìš´ node vë¥¼ íŒë³„(ë¹„êµ)í•˜ê³ ì í•˜ëŠ” ì ì—ì„œ softmax**ë¥¼ ì‚¬ìš©í•˜ì—¬, $p(v|z_u)$ ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

> $p(v|z_u) = \frac{exp(z^T_uz_v)}{\sum_{n\in V}exp(z^T_u z_n)}$
> 

![](../images/lec03/Untitled 6-1681368211905-8.png)

ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•œ loss function $L$ ì´ ìµœì†Œí™”ë˜ë„ë¡ $z_u$ë¥¼ ì°¾ìŠµë‹ˆë‹¤!!

í•˜ì§€ë§Œ, ë‹¤ìŒ ì‹ì€ ë§¤ìš° ê³„ì‚°ì´ expensive í•©ë‹ˆë‹¤. ëª¨ë“  nodeì— ëŒ€í•œ ê³„ì‚°ì´ 2ë²ˆì´ë‚˜(ìœ„ ê·¸ë¦¼ì—ì„œ íŒŒë‘ìƒ‰, ë…¸ë‘ìƒ‰ ë¶€ë¶„ì— í•´ë‹¹) ì¤‘ì²©ë˜ê²Œ ì´ë£¨ì–´ì§€ê¸° ë•Œë¬¸ì—, complexityê°€ ë¬´ë ¤ $O(|V|^2)$ì…ë‹ˆë‹¤.  

ì´ë¥¼ ì‹¤ì œì—ì„œ í™œìš©í•˜ê¸° ìœ„í•´ì„ , **softmax í•­ì˜ ë¶„ëª¨ì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ì„ ê·¼ì‚¬**í•´ì•¼ í•©ë‹ˆë‹¤. 

â†’ ë°©ë²•ì€, **â€˜Negative Samplingâ€™**ì…ë‹ˆë‹¤!!

ëª¨ë“  ë…¸ë“œì— ëŒ€í•œ ê°’ì„ êµ¬í•´ normalizeí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, **kê°œì˜ random negative sampleì— ëŒ€í•´ ì •ê·œí™”ë¥¼ ì§„í–‰**í•©ë‹ˆë‹¤. ì´ë•Œ kê°œì˜ sampleì„ samplingí•  ë•ŒëŠ” degreeì— ê¸°ë°˜í•˜ì—¬ **bias sampling**($n_i \sim P_v$)ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. **higher degree(undirected graphì˜ ê²½ìš°, ì—°ê²°ëœ edge ìˆ˜)ë¥¼ ê°€ì§ˆìˆ˜ë¡ sample ë  í™•ë¥ ì´ ë†’ì•„**ì§‘ë‹ˆë‹¤. ìƒ˜í”Œì˜ ê°¯ìˆ˜ kê°€ ì»¤ì§ˆ ìˆ˜ë¡, ë” ì •êµí•œ ì˜ˆì¸¡ì´ ê°€ëŠ¥í•˜ì§€ë§Œ, ë™ì‹œì— biasê°€ ì»¤ì§€ëŠ” ë‹¨ì ì´ ì¡´ì¬í•©ë‹ˆë‹¤. ë³´í†µ, këŠ” 5~20ìœ¼ë¡œ ì§€ì •í•©ë‹ˆë‹¤. 

![](../images/lec03/Untitled 7.png)



**loss functionì„ ìµœì í™”í•  ë•Œ ë³´í†µ stochastic gradient descentë¥¼ ì‚¬**ìš©í•©ë‹ˆë‹¤. gradient descentë€, random pointì—ì„œ ì‹œì‘í•˜ì—¬ ë¯¸ë¶„ê°’ì„ ê³„ì‚°í•˜ê³  learning rateì— ë§ê²Œ ë°©í–¥ì„ ì¡°ê¸ˆì”© ë³€í™”í•©ë‹ˆë‹¤. ì´ ì‘ì—…ì„ ìˆ˜ë ´í•  ë•Œ ê¹Œì§€ ë°˜ë³µí•©ë‹ˆë‹¤. íŠ¹íˆ, stochastic gradient descentëŠ” ëª¨ë“  exampleì— ëŒ€í•´ì„œ í•´ë‹¹ ê³„ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ë§¤ë²ˆ ëœë¤ìœ¼ë¡œ ì„ íƒí•œ í•˜ë‚˜ì˜ exampleì— ëŒ€í•´ì„œë§Œ ë¯¸ë¶„ê°’ì„ ê³„ì‚°í•˜ê³  ì—…ë°ì´íŠ¸ í•´ë‚˜ê°‘ë‹ˆë‹¤. 

ì ì—¬ê¸°ê¹Œì§€ **Random Walk ê³¼ì •ì„ ìš”ì•½**í•´ë³´ìë©´,

**Step 1)** random walk strategy $R$ì— ë”°ë¼ì„œ ê° node uì—ì„œ ê³ ì •ëœ í¬ê¸°ì˜ ì§§ì€ random walkë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

**Step 2)** ê° node ë§ˆë‹¤ multiset ì´ì›ƒ ì§‘í•©, $N_R(u)$ì„ ëª¨ìë‹ˆë‹¤.

**Step 3)** loss funcion : $L =  \sum_{u\in V}\sum_{v\in N_R(u)} -\log(P(v|z_u))$ 
ë‹¤ìŒ loss functionì„ negative samplingì„ ì‚¬ìš©í•´ softmax ë¶€ë¶„ì„ ê·¼ì‚¬í•˜ê³ , stochastic gradient descentë¥¼ í†µí•´ ìµœì í™”ì—¬ embeddingì„ êµ¬í•©ë‹ˆë‹¤.

### How should we randomly walk?

ê·¸ë ‡ë‹¤ë©´ **íš¨ê³¼ì ì¸ random walk ì „ëµ $R$ì€ ë¬´ì—‡**ì¼ê¹Œìš”?? ê³ ì • í¬ê¸° ë§Œí¼ì˜ random walkì™€ ê°™ì€ ë‹¨ìˆœí•œ ë°©ë²•ì€ ìœ ì‚¬ì„±ì„ í‘œí˜„í•˜ê¸°ì— ë§¤ìš° í•œì •ì ì…ë‹ˆë‹¤. ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ë“±ì¥í•œ ë°©ë²•ì´ **node2vec** ì…ë‹ˆë‹¤. í‘œí˜„ë ¥ì´ ì¢‹ì€ (expressive)í•œ ì´ì›ƒë“¤ì„ ëª¨ìœ¼ê¸° ìœ„í•´ **â€˜biased 2nd order random walk $R$â€™** ì´ ì‚¬ìš©ë©ë‹ˆë‹¤. 

### node2vec : Biased Walks

í•µì‹¬ ì•„ì´ë””ì–´ëŠ” **local + global íƒìƒ‰ì˜ tradeoffë¥¼ ì˜ ê³ ë ¤í•˜ì—¬ biased walkë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒ**ì…ë‹ˆë‹¤. ì´ë•Œ **localì€ BFS(Breadth-First Search) ë„ˆë¹„ ìš°ì„  íƒìƒ‰ì„ í†µí•´, globalì€ DFS(Depth-First Search) ê¹Šì´ ìš°ì„  íƒìƒ‰**ì„ í†µí•´ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.   

![](../images/lec03/Untitled 8.png)

### Interpolating BFS and DFS

node u ì— ëŒ€í•œ $N_R(u)$ë¥¼ ëª¨ì„ ë•Œ, 2ê°€ì§€ hyper-parameterë¥¼ ì§€ì •í•˜ì—¬ BFSì™€ DFS ë°©ë²•ì„ ë™ì‹œì— ì ì ˆíˆ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

**1) return parameter $p$ :** ì´ì „ nodeë¡œ ëŒì•„ê°ˆ í™•ë¥ 

**2) in-out parameter $q$ :** BFS(inwards)ì™€ DFS(outwards)ê°„ì˜ ë¹„ìœ¨(ratio)

### Biased 2nd-order random walks

ë‹¤ìŒ ê·¸ë¦¼ì€ random walk ì¤‘ node $s_1$ì—ì„œ node $w$ë¡œ ì´ë™í•œ ìƒí™©ì…ë‹ˆë‹¤. 

ì´ì œ $w$ì—ì„œ ì·¨í•  ìˆ˜ ìˆëŠ” í–‰ë™ ìœ í˜•ì€ ì´ 3ê°€ì§€, 
<**(1) ì´ì „ ë…¸ë“œ $s_1$ìœ¼ë¡œ ëŒì•„ê°€ê¸°, (2) $s_1$ì—ì„œ ë™ì¼í•˜ê²Œ 1 hop ë–¨ì–´ì ¸ ìˆëŠ” $s_2$ë¡œ ì´ë™, (3)ë” ë©€ë¦¬ $s_3, s_4$ë¡œ ì´ë™** >ì´ ìˆìŠµë‹ˆë‹¤. ì•ì„œ ì •ì˜í–ˆë˜ parameterë“¤ì„ ì ìš©í•´ë³´ë©´, $1/p$ê°€ ë‹¤ì‹œ ëŒì•„ê°ˆ í™•ë¥ , $1/q$ ë¥¼ ë” ë©€ë¦¬ ì´ë™í•  í™•ë¥ ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´í›„, $w$ì—ì„œì˜ 4ê°€ì§€ ì„ íƒì§€($s_1, s_2, s_3,s_4$)ì— ëŒ€í•œ í™•ë¥ ì„ 1ë¡œ ë§ì¶”ì–´ ì •ê·œí™”í•©ë‹ˆë‹¤. BFSì™€ ê°™ì´ inwardsë¥¼ ëŒì•„ë‹¤ë‹ˆê²Œ í•˜ë ¤ë©´ pê°’ì„ ë‚®ì¶”ë©´ ë˜ê³ , DFSì™€ ê°™ì´ outwardsë¥¼ ëŒì•„ë‹¤ë‹ˆë ¤ë©´ qê°’ì„ ë‚®ì¶”ë©´ ë©ë‹ˆë‹¤. 

![](../images/lec03/Untitled 9.png)

### node2vec algorithm

**Step 1)** random walk í™•ë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

**Step 2)** ê° ë…¸ë“œ u ì—ì„œ ê¸¸ì´ $l$ ë§Œí¼ì˜ random walkë¥¼ $r$ë²ˆ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.

**Step 3)** Stochastic gradient descentë¥¼ ì‚¬ìš©í•˜ì—¬ node2vec objective functionì„ ìµœì í™”í•©ë‹ˆë‹¤.

ì‹œê°„ ë³µì¡ë„ëŠ” linear-timeì´ë©°, ê° stepì´ ë³‘ë ¬ì ìœ¼ë¡œ ìˆ˜í–‰ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ, í•´ë‹¹ ë°©ë²•ì˜ ë‹¨ì ì€ ëª¨ë“  nodeë“¤ì´ ê°ê°ì˜ node embeddingì„ í•™ìŠµí•´ì•¼ í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ ê·¸ë˜í”„ í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡, ë” ë§ì€ embeddingì„ í•™ìŠµí•´ì•¼ í•©ë‹ˆë‹¤. 

* ì¼ë°˜ì ìœ¼ë¡œ node2vecì€ node classificationì„ ì˜ ìˆ˜í–‰í•œë‹¤ê³  ì•Œë ¤ì ¸ìˆìœ¼ë©°, random walkëŠ” ì „ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ **ì–´ë–¤ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ê°€ëŠ” ë³¸ì¸ì˜ ì—°êµ¬ì— ì œì¼ ì˜ ë§ëŠ” ë°©ë²•ì„ ì±„íƒ**í•´ì•¼ í•©ë‹ˆë‹¤.

---

## 3.3 Embedding Entire Graphs

3ì¥ì—ì„œëŠ” node levelì—ì„œ ë²—ì–´ë‚˜ **ê·¸ë˜í”„ ì „ì²´ë¥¼ embedding** í•´ë´…ì‹œë‹¤. 

![](../images/lec03/Untitled 10.png)

**ë°©ë²• 1) node embeddingì˜ í•©** 

random walk í˜¹ì€ node2vecì—ì„œ êµ¬í•œ node embeddingì„ ì´ìš©í•˜ì—¬ ê³„ì‚°(í•©/í‰ê· ) í•©ë‹ˆë‹¤.

> $z_G = \sum_{v\in G }z_v$
> 

 * í•´ë‹¹ ë°©ë²•ì€ moleculesë¥¼ ë¶„ë¥˜í•˜ëŠ” ì—°êµ¬ì— ì„±ê³µì ìœ¼ë¡œ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. 

**ë°©ë²• 2) â€˜virtual nodeâ€™**

ê·¸ë˜í”„ì˜ **íŠ¹ì • ë¶€ë¶„ì„ ëŒ€í‘œí•˜ëŠ” â€˜virtual nodeâ€™ë¥¼ ì¶”ì¶œ**í•˜ê³ , ì—¬ê¸°ì— graph embedding ê¸°ìˆ ì„ ì ìš©í•©ë‹ˆë‹¤.  

![](../images/lec03/Untitled 11.png)

**ë°©ë²• 3) Anonymous Walk Embeddings**

ì²« ë²ˆì§¸ë¡œ **ë°©ë¬¸í•œ nodeë¶€í„° ìˆœì„œëŒ€ë¡œ indexë¥¼ ë¶€ì—¬**í•©ë‹ˆë‹¤. **random walk ë¡œ ì´ë™í•˜ë©´ì„œ index sequenceë¥¼ ê¸°ë¡**í•©ë‹ˆë‹¤. ë°©ë¬¸í•œ nodeê°€ ì‹¤ì œ ì–´ë–¤ nodeì¸ì§€ ìƒê´€ì—†ì´ ë°©ë¬¸í•œ ìˆœì„œì— ë”°ë¼ indexê°€ ì£¼ì–´ì§‘ë‹ˆë‹¤. 

![](../images/lec03/Untitled 12.png)

random walk ê¸¸ì´ê°€ 3ì¼ë•Œ, ê°€ëŠ¥í•œ anonymous walkì˜ ìˆ˜ëŠ” ì´ 5ê°œ ì…ë‹ˆë‹¤. 
 â†’ $w_1=111,w_2=112,w_3=121,w_4=122,w_5=123$

**ê°€ëŠ¥í•œ anonymous walkìˆ˜ëŠ” random walk ê¸¸ì´ì— ë”°ë¼ exponentialí•˜ê²Œ ì¦ê°€**í•©ë‹ˆë‹¤.  

![](../images/lec03/Untitled 13.png)

### Simple Use of Anonymous Walks

step ìˆ˜ $l$ ì„ ì •í•œ ë’¤, **ê°€ëŠ¥í•œ anonymous walkë¥¼ ê³„ì‚°**í•©ë‹ˆë‹¤. ì´í›„ **ê° walkì— ëŒ€í•œ probability distributionì„ ê³„ì‚°**í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´,  $l=3$ì¼ ë•Œ ê°€ëŠ¥í•œ walkëŠ” 5ê°œì˜€ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ê° 5ê°œ walkì— ëŒ€í•œ í™•ë¥ ì„ ê³„ì‚°í•˜ì—¬, í•´ë‹¹ ê·¸ë˜í”„ë¥¼ **5ì°¨ì›ì˜ vectorë¡œ í‘œí˜„**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

> $Z_G[i]$   =  probability of anonymous walk $w_i$
> 

### Sampling Anonymous Walks

ê·¸ë ‡ë‹¤ë©´ ì ë‹¹í•œ random walkëŠ” ëª‡ ê°œ ì¼ê¹Œìš”?
 $\delta$ì˜ í™•ë¥ ë³´ë‹¤ ë‚®ê²Œ $\epsilon$ ì´ìƒì˜ errorë¥¼ ê°–ê²Œ í•˜ê¸° ìœ„í•´ì„  ë‹¤ìŒ ì‹ì„ ë§Œì¡±í•´ì•¼ í•©ë‹ˆë‹¤. 

![](../images/lec03/Untitled 14.png)

### New idea : Learn Walk Embeddings

ìƒˆë¡œìš´ ì•„ì´ë””ì–´ëŠ” anonymous walk $w_i$ì˜ embedding $z_i$ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´, ê·¸ë˜í”„ embeddingì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.  $Z = \{z_i : i = 1 \dots \eta \},\;\eta$ : sampled anonymous walks ê°¯ìˆ˜

**anynomous random walkë¥¼ ìƒ˜í”Œë§**í•©ë‹ˆë‹¤. **ì´í›„ $\Delta$-size window ë‚´ì— ì–´ë–¤ walk ê°€ í•¨ê»˜ ë°œìƒí• ì§€ ì˜ˆì¸¡í•©ë‹ˆë‹¤.** ì˜ˆë¥¼ ë“¤ì–´, $\Delta$ê°€ 1ì¼ ë•Œ ê²½ìš°, **ì§ì „ê³¼ ì§í›„ì— ì–´ë–¤ walkê°€ ë°œìƒí•  ì§€ ì˜ˆì¸¡**í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ í’€ì–´ë³´ë©´ objectiveëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. graph embedding $Z_G$ì™€ time window ë‚´ì— walkë“¤($w_{t-\Delta}, \dots, w_{t+\Delta}$)ì´ ì£¼ì–´ì¡Œì„ ë•Œ $w_t$ì— ëŒ€í•œ log probabilityë¥¼ ìµœëŒ€í™”í•´ì•¼ í•©ë‹ˆë‹¤. ëª¨ë“  nodeê°€ ì¶œë°œì ì´ ë˜ì–´ì„œ ê°ê°ì˜ objectiveë¥¼ ê³„ì‚°í•˜ê³ , ì´ë¥¼ ëª¨ë‘ ë”í•©ë‹ˆë‹¤. 

![](../images/lec03/Untitled 15.png)

ì´ì œ **node u ì˜ ì´ì›ƒ $N_R(u)$ì€ random walk ì˜ setìœ¼ë¡œ í‘œí˜„**ë©ë‹ˆë‹¤. 

$N_R(u) = \{w_1^u, w_2^u,\dots, w_T^u\}$



![Untitled 16](../images/lec03/Untitled 16.png) 

ë‹¤ìŒê³¼ ê°™ì´ graph embedding $Z_G$ë¥¼ êµ¬í•œë’¤, inner product í˜¹ì€ neural networkë¥¼ ê±°ì³ graph classificationê³¼ ê°™ì€ ì˜ˆì¸¡ ì‘ì—…ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

### Summary of Graph embedding

ì§€ê¸ˆê¹Œì§€, **ì´ 3ê°€ì§€ ê·¸ë˜í”„ embedding ë°©ë²•**ì— ëŒ€í•´ ë‹¤ë£¨ì—ˆìŠµë‹ˆë‹¤. 

**ë°©ë²• 1)** deep walk í˜¹ì€ node2vecìœ¼ë¡œ êµ¬í•œ node embeddingì„ í•©í•˜ê±°ë‚˜ í‰ê· ë‚´ê¸°

**ë°©ë²• 2)** virtual nodeì™€ ê°™ì´ super-nodeêµ¬í•´ì„œ embeddingí•˜ê¸°

**ë°©ë²• 3)** Anonymous Walk Embedding

**3-1)** anonymous walk sampleì„ êµ¬í•œ ë’¤, ê°ê°ì˜ walkê°€ ëª‡ë²ˆ ë°œìƒí–ˆëŠ”ì§€ ë¹„ìœ¨ ê³„ì‚°

**3-2)** anonymous walkë¥¼ embeddingí•˜ê³  ì´ë¥¼ í•©ì³ graph embeddingí•˜ê¸°

*ì°¨í›„ì˜ ê°•ì˜ì—ì„œëŠ” Hierarchical Embeddingì— ëŒ€í•´ ë‹¤ë£¹ë‹ˆë‹¤(Lecture 8).

graphìƒì˜ nodeë“¤ì„ ê³„ì¸µì ìœ¼ë¡œ ë¬¶ì€ ë’¤, ì´ë“¤ì„ í•©/í‰ê·  ë‚´ì–´ graph embedding í•©ë‹ˆë‹¤. 

![Untitled 17](../images/lec03/Untitled 17.png)

### How to Use Embeddings

ì—´ì‹¬íˆ êµ¬í•œ embeddingì€ ì•„ë˜ì™€ ê°™ì´ í™œìš© ê°€ëŠ¥í•©ë‹ˆë‹¤ğŸ™‚ğŸ™‚

**1)** Clustering/ community detection

**2)** Node classification

**3)** Link Prediction

: 2ê°œì˜ embedding($z_i,z_j$)ì— ëŒ€í•´ concatenate, hadamard, sum/avg, distance ê³„ì‚°

**4)** Graph classification